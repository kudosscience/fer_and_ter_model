{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db464bf",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition (FER) Model Training\n",
    "## Using FER 2013 Dataset\n",
    "\n",
    "This notebook builds and trains a Convolutional Neural Network (CNN) for facial expression recognition using the FER 2013 dataset. It's optimized to run on Google Colab with GPU acceleration.\n",
    "\n",
    "### Emotions Recognized:\n",
    "- 0: Angry üò†\n",
    "- 1: Disgust ü§¢\n",
    "- 2: Fear üò®\n",
    "- 3: Happy üòä\n",
    "- 4: Sad üò¢\n",
    "- 5: Surprise üò≤\n",
    "- 6: Neutral üòê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86025cf8",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ùå Not running on Google Colab\")\n",
    "\n",
    "# Check GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df7b3d",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install opencv-python-headless\n",
    "!pip install matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install pandas numpy\n",
    "!pip install pillow\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a462c",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2fc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9abac",
   "metadata": {},
   "source": [
    "## 4. Download and Setup FER 2013 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download FER 2013 dataset from Kaggle\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive if needed\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install Kaggle API\n",
    "    !pip install kaggle\n",
    "    \n",
    "    # You'll need to upload your kaggle.json file or set up Kaggle credentials\n",
    "    print(\"Please upload your kaggle.json file to access the FER 2013 dataset\")\n",
    "    print(\"Or download the dataset manually from: https://www.kaggle.com/datasets/msambare/fer2013\")\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('/content/data', exist_ok=True)\n",
    "    \n",
    "    # Uncomment the following lines after setting up Kaggle credentials:\n",
    "    # !kaggle datasets download -d msambare/fer2013 -p /content/data\n",
    "    # !unzip /content/data/fer2013.zip -d /content/data/\n",
    "    \n",
    "    dataset_path = '/content/data/fer2013'\n",
    "else:\n",
    "    # Local setup\n",
    "    dataset_path = './data/fer2013'\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "    print(f\"Please download FER 2013 dataset to: {dataset_path}\")\n",
    "\n",
    "print(f\"Dataset path: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ac444",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class for FER 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FER2013Dataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        FER 2013 Dataset class\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str): Path to the dataset directory\n",
    "            split (str): 'train', 'test', or 'validation'\n",
    "            transform: Data transformations to apply\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Emotion labels\n",
    "        self.emotion_labels = {\n",
    "            'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,\n",
    "            'sad': 4, 'surprise': 5, 'neutral': 6\n",
    "        }\n",
    "        \n",
    "        self.label_to_emotion = {v: k for k, v in self.emotion_labels.items()}\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        \n",
    "        if os.path.exists(split_dir):\n",
    "            for emotion in self.emotion_labels.keys():\n",
    "                emotion_dir = os.path.join(split_dir, emotion)\n",
    "                if os.path.exists(emotion_dir):\n",
    "                    for img_file in os.listdir(emotion_dir):\n",
    "                        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            self.images.append(os.path.join(emotion_dir, img_file))\n",
    "                            self.labels.append(self.emotion_labels[emotion])\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images for {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if image is None:\n",
    "            # Create a dummy image if loading fails\n",
    "            image = np.zeros((48, 48), dtype=np.uint8)\n",
    "        \n",
    "        # Resize to 48x48 if needed\n",
    "        if image.shape != (48, 48):\n",
    "            image = cv2.resize(image, (48, 48))\n",
    "        \n",
    "        # Convert to PIL Image for transforms\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ FER2013Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72cd23",
   "metadata": {},
   "source": [
    "## 6. Data Transformations and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Data transformations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7d61c",
   "metadata": {},
   "source": [
    "## 7. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ebe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "try:\n",
    "    train_dataset = FER2013Dataset(dataset_path, split='train', transform=train_transform)\n",
    "    val_dataset = FER2013Dataset(dataset_path, split='validation', transform=val_test_transform)\n",
    "    test_dataset = FER2013Dataset(dataset_path, split='test', transform=val_test_transform)\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please make sure the FER 2013 dataset is properly downloaded and extracted.\")\n",
    "    \n",
    "    # Create dummy datasets for demonstration\n",
    "    print(\"Creating dummy datasets for demonstration...\")\n",
    "    \n",
    "    class DummyDataset(Dataset):\n",
    "        def __init__(self, size=1000, transform=None):\n",
    "            self.size = size\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Generate random grayscale image\n",
    "            image = torch.randn(1, 48, 48)\n",
    "            label = torch.randint(0, 7, (1,)).item()\n",
    "            return image, label\n",
    "    \n",
    "    train_dataset = DummyDataset(size=20000)\n",
    "    val_dataset = DummyDataset(size=3000)\n",
    "    test_dataset = DummyDataset(size=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea4602",
   "metadata": {},
   "source": [
    "## 8. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size based on available memory\n",
    "BATCH_SIZE = 64 if device.type == 'cuda' else 32\n",
    "NUM_WORKERS = 2 if IN_COLAB else 0\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Data loaders created with batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d4fcd",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion labels for visualization\n",
    "emotion_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "emotion_emojis = ['üò†', 'ü§¢', 'üò®', 'üòä', 'üò¢', 'üò≤', 'üòê']\n",
    "\n",
    "# Visualize some sample images\n",
    "def visualize_samples(data_loader, num_samples=8):\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        # Convert tensor to numpy and denormalize\n",
    "        img = images[i].squeeze().numpy()\n",
    "        img = (img + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "        \n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'{emotion_names[labels[i]]} {emotion_emojis[labels[i]]}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample images from training set:\")\n",
    "visualize_samples(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77bc87",
   "metadata": {},
   "source": [
    "## 10. Define CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74949b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = EmotionCNN(num_classes=7, dropout_rate=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model created and moved to {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ed07e",
   "metadata": {},
   "source": [
    "## 11. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c86440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Loss function, optimizer, and scheduler defined\")\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Scheduler: {scheduler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d579dc2",
   "metadata": {},
   "source": [
    "## 12. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9046c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úÖ Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ed164",
   "metadata": {},
   "source": [
    "## 13. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "BEST_VAL_LOSS = float('inf')\n",
    "PATIENCE_COUNTER = 0\n",
    "\n",
    "# Lists to store training history\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    if val_loss < BEST_VAL_LOSS:\n",
    "        BEST_VAL_LOSS = val_loss\n",
    "        PATIENCE_COUNTER = 0\n",
    "        \n",
    "        # Save best model\n",
    "        if IN_COLAB:\n",
    "            torch.save(model.state_dict(), '/content/best_fer_model.pth')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), 'best_fer_model.pth')\n",
    "        \n",
    "        print(f\"‚úÖ New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        PATIENCE_COUNTER += 1\n",
    "        print(f\"‚è≥ No improvement. Patience: {PATIENCE_COUNTER}/{EARLY_STOPPING_PATIENCE}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if PATIENCE_COUNTER >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nüõë Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(\"\\nüéâ Training completed!\")\n",
    "print(f\"Best validation loss: {BEST_VAL_LOSS:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a24c2",
   "metadata": {},
   "source": [
    "## 14. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(train_losses, label='Training Loss', color='blue')\n",
    "ax1.plot(val_losses, label='Validation Loss', color='red')\n",
    "ax1.set_title('Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy', color='red')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757327f6",
   "metadata": {},
   "source": [
    "## 15. Load Best Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "try:\n",
    "    if IN_COLAB:\n",
    "        model.load_state_dict(torch.load('/content/best_fer_model.pth'))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('best_fer_model.pth'))\n",
    "    print(\"‚úÖ Best model loaded successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Could not load saved model, using current model\")\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc='Testing')\n",
    "        \n",
    "        for data, target in progress_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "# Run test\n",
    "print(\"Testing the model...\")\n",
    "test_predictions, test_targets = test_model(model, test_loader, device)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "print(f\"\\nüéØ Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd92bd7",
   "metadata": {},
   "source": [
    "## 16. Detailed Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"üìä Classification Report:\")\n",
    "print(\"-\" * 50)\n",
    "class_report = classification_report(\n",
    "    test_targets, \n",
    "    test_predictions, \n",
    "    target_names=emotion_names,\n",
    "    digits=4\n",
    ")\n",
    "print(class_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_targets, test_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=[f'{name}\\n{emoji}' for name, emoji in zip(emotion_names, emotion_emojis)],\n",
    "    yticklabels=[f'{name}\\n{emoji}' for name, emoji in zip(emotion_names, emotion_emojis)]\n",
    ")\n",
    "plt.title('Confusion Matrix - Facial Expression Recognition')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüìà Per-class Accuracy:\")\n",
    "print(\"-\" * 30)\n",
    "for i, emotion in enumerate(emotion_names):\n",
    "    class_mask = test_targets == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.sum(test_predictions[class_mask] == i) / np.sum(class_mask)\n",
    "        print(f\"{emotion} {emotion_emojis[i]}: {class_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333f3c9",
   "metadata": {},
   "source": [
    "## 17. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_loader, device, num_samples=12):\n",
    "    \"\"\"Visualize sample predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Move back to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predictions = predictions.cpu()\n",
    "    probabilities = probabilities.cpu()\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        # Denormalize image\n",
    "        img = images[i].squeeze().numpy()\n",
    "        img = (img + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
    "        \n",
    "        # Get prediction info\n",
    "        true_label = labels[i].item()\n",
    "        pred_label = predictions[i].item()\n",
    "        confidence = probabilities[i][pred_label].item()\n",
    "        \n",
    "        # Set title color based on correctness\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        # Plot image\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(\n",
    "            f'True: {emotion_names[true_label]} {emotion_emojis[true_label]}\\n'\n",
    "            f'Pred: {emotion_names[pred_label]} {emotion_emojis[pred_label]}\\n'\n",
    "            f'Conf: {confidence:.2f}',\n",
    "            color=color,\n",
    "            fontsize=10\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample predictions (Green = Correct, Red = Incorrect):\")\n",
    "visualize_predictions(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25238851",
   "metadata": {},
   "source": [
    "## 18. Save Final Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02775cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "if IN_COLAB:\n",
    "    model_path = '/content/fer2013_final_model.pth'\n",
    "    results_path = '/content/training_results.json'\n",
    "else:\n",
    "    model_path = 'fer2013_final_model.pth'\n",
    "    results_path = 'training_results.json'\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'best_val_loss': BEST_VAL_LOSS,\n",
    "    'emotion_labels': emotion_names,\n",
    "    'model_architecture': str(model)\n",
    "}, model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "\n",
    "# Save training results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'best_validation_loss': float(BEST_VAL_LOSS),\n",
    "    'best_validation_accuracy': float(max(val_accuracies)),\n",
    "    'final_training_accuracy': float(train_accuracies[-1]),\n",
    "    'final_validation_accuracy': float(val_accuracies[-1]),\n",
    "    'epochs_trained': len(train_losses),\n",
    "    'train_losses': [float(x) for x in train_losses],\n",
    "    'train_accuracies': [float(x) for x in train_accuracies],\n",
    "    'val_losses': [float(x) for x in val_losses],\n",
    "    'val_accuracies': [float(x) for x in val_accuracies],\n",
    "    'emotion_labels': emotion_names,\n",
    "    'model_parameters': total_params\n",
    "}\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"üìà Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
    "print(f\"üìâ Best Validation Loss: {BEST_VAL_LOSS:.4f}\")\n",
    "print(f\"üïê Epochs Trained: {len(train_losses)}\")\n",
    "print(f\"üîß Model Parameters: {total_params:,}\")\n",
    "print(f\"üíæ Model saved to: {model_path}\")\n",
    "print(f\"üìã Results saved to: {results_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba8bee",
   "metadata": {},
   "source": [
    "## 19. Model Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model, image_path, transform, device):\n",
    "    \"\"\"\n",
    "    Predict emotion from a single image\n",
    "    \n",
    "    Args:\n",
    "        model: Trained emotion recognition model\n",
    "        image_path: Path to the image file\n",
    "        transform: Image preprocessing transforms\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        predicted_emotion: Predicted emotion name\n",
    "        confidence: Prediction confidence\n",
    "        probabilities: All class probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load image from {image_path}\")\n",
    "    \n",
    "    # Resize to 48x48\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    \n",
    "    # Convert to PIL Image and apply transforms\n",
    "    image = Image.fromarray(image)\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    predicted_emotion = emotion_names[predicted.item()]\n",
    "    confidence_score = confidence.item()\n",
    "    all_probs = probabilities.cpu().numpy()[0]\n",
    "    \n",
    "    return predicted_emotion, confidence_score, all_probs\n",
    "\n",
    "def display_prediction_results(emotion, confidence, probabilities):\n",
    "    \"\"\"\n",
    "    Display prediction results in a formatted way\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Predicted Emotion: {emotion} {emotion_emojis[emotion_names.index(emotion)]}\")\n",
    "    print(f\"üìä Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(\"\\nüìà All Probabilities:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, (emotion_name, emoji, prob) in enumerate(zip(emotion_names, emotion_emojis, probabilities)):\n",
    "        print(f\"{emotion_name} {emoji}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "print(\"‚úÖ Inference functions defined\")\n",
    "print(\"\\nTo use the model for inference on a new image:\")\n",
    "print(\"```python\")\n",
    "print(\"emotion, confidence, probs = predict_emotion(model, 'path/to/image.jpg', val_test_transform, device)\")\n",
    "print(\"display_prediction_results(emotion, confidence, probs)\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbbb42e",
   "metadata": {},
   "source": [
    "## 20. Instructions for Using the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dabf4e",
   "metadata": {},
   "source": [
    "### üöÄ How to Use Your Trained FER Model\n",
    "\n",
    "#### **Loading the Model:**\n",
    "```python\n",
    "# Create model instance\n",
    "model = EmotionCNN(num_classes=7)\n",
    "model.load_state_dict(torch.load('fer2013_final_model.pth')['model_state_dict'])\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "#### **Making Predictions:**\n",
    "```python\n",
    "# For a single image\n",
    "emotion, confidence, probs = predict_emotion(\n",
    "    model, \n",
    "    'path/to/your/image.jpg', \n",
    "    val_test_transform, \n",
    "    device\n",
    ")\n",
    "display_prediction_results(emotion, confidence, probs)\n",
    "```\n",
    "\n",
    "#### **Integration Tips:**\n",
    "- The model expects 48x48 grayscale images\n",
    "- Use the same preprocessing transforms as during training\n",
    "- The model outputs 7 emotion classes: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "- For real-time applications, consider using GPU acceleration\n",
    "\n",
    "#### **Performance Expectations:**\n",
    "- Test accuracy achieved: **{test_accuracy*100:.2f}%**\n",
    "- Best validation accuracy: **{max(val_accuracies):.2f}%**\n",
    "- Model size: **{total_params:,} parameters**\n",
    "\n",
    "#### **Next Steps:**\n",
    "1. **Fine-tuning**: Retrain on your specific domain data\n",
    "2. **Deployment**: Convert to ONNX for production use\n",
    "3. **Integration**: Combine with face detection for end-to-end emotion recognition\n",
    "4. **Evaluation**: Test on your specific use case data\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "- [FER 2013 Dataset](https://www.kaggle.com/datasets/msambare/fer2013)\n",
    "- [OpenCV Face Detection](https://docs.opencv.org/4.x/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "### üéâ Congratulations!\n",
    "You have successfully trained a Facial Expression Recognition model! The model is ready for inference and can be integrated into your applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db464bf",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition (FER) Model Training\n",
    "## Using FER 2013 Dataset\n",
    "\n",
    "This notebook builds and trains a Convolutional Neural Network (CNN) for facial expression recognition using the FER 2013 dataset. It's optimized to run on Google Colab with GPU acceleration.\n",
    "\n",
    "### Emotions Recognized:\n",
    "- 0: Angry üò†\n",
    "- 1: Disgust ü§¢\n",
    "- 2: Fear üò®\n",
    "- 3: Happy üòä\n",
    "- 4: Sad üò¢\n",
    "- 5: Surprise üò≤\n",
    "- 6: Neutral üòê"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86025cf8",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142fba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ùå Not running on Google Colab\")\n",
    "\n",
    "print(\"üìù Note: GPU availability will be checked after package installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df7b3d",
   "metadata": {},
   "source": [
    "## 2. Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with compatible versions\n",
    "# First, uninstall existing torch packages to avoid conflicts\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "\n",
    "# Check current environment\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    # Get CUDA version if available\n",
    "    result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"CUDA version info:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        # Extract CUDA version\n",
    "        if \"release 12\" in result.stdout:\n",
    "            cuda_version = \"cu121\"\n",
    "        elif \"release 11.8\" in result.stdout:\n",
    "            cuda_version = \"cu118\"\n",
    "        else:\n",
    "            cuda_version = \"cu121\"  # Default to latest\n",
    "    else:\n",
    "        cuda_version = \"cu121\"  # Default\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not detect CUDA version: {e}\")\n",
    "    cuda_version = \"cu121\"  # Default to latest\n",
    "\n",
    "print(f\"Installing PyTorch with CUDA support: {cuda_version}\")\n",
    "\n",
    "# Install PyTorch with appropriate CUDA version\n",
    "if cuda_version == \"cu118\":\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "else:\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install other required packages\n",
    "!pip install opencv-python-headless\n",
    "!pip install matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install pandas numpy\n",
    "!pip install pillow\n",
    "!pip install tqdm\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe7b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installation and compatibility\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"‚úÖ PyTorch version: {torch.__version__}\")\n",
    "    print(f\"‚úÖ Torchvision version: {torchvision.__version__}\")\n",
    "    \n",
    "    # Check CUDA availability and set device\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ CUDA available: {torch.version.cuda}\")\n",
    "        print(f\"‚úÖ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CUDA not available, will use CPU\")\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    print(f\"üéØ Using device: {device}\")\n",
    "    \n",
    "    # Test basic functionality\n",
    "    x = torch.randn(1, 3, 224, 224)\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    print(\"‚úÖ PyTorch tensor operations working correctly\")\n",
    "    \n",
    "    # Test torchvision transforms\n",
    "    import torchvision.transforms as transforms\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224))])\n",
    "    print(\"‚úÖ Torchvision transforms working correctly\")\n",
    "    \n",
    "    # Make device available globally\n",
    "    globals()['device'] = device\n",
    "    \n",
    "    print(\"\\nüéâ All installations verified successfully!\")\n",
    "    print(f\"üîß Device variable set globally: {device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Installation verification failed: {e}\")\n",
    "    print(\"\\nIf you see this error, please restart the runtime and try again:\")\n",
    "    print(\"Runtime ‚Üí Restart runtime, then run the installation cell again.\")\n",
    "    # Set CPU as fallback\n",
    "    device = torch.device('cpu')\n",
    "    globals()['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a462c",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2fc74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries with error handling\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch should already be imported from verification cell\n",
    "try:\n",
    "    # Check if torch is already available\n",
    "    if 'torch' in globals():\n",
    "        print(\"‚úÖ PyTorch already imported from verification cell\")\n",
    "    else:\n",
    "        import torch\n",
    "        print(\"‚úÖ PyTorch imported\")\n",
    "    \n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.data import Dataset, DataLoader\n",
    "    print(\"‚úÖ PyTorch modules imported successfully\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå PyTorch import error: {e}\")\n",
    "    print(\"Please restart runtime and run the installation cell again\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import torchvision.transforms as transforms\n",
    "    print(\"‚úÖ Torchvision transforms imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Torchvision import error: {e}\")\n",
    "    print(\"Please restart runtime and run the installation cell again\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import cv2\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from PIL import Image\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    from tqdm import tqdm\n",
    "    import os\n",
    "    print(\"‚úÖ Other libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Other library import error: {e}\")\n",
    "    print(\"Please run the installation cell again\")\n",
    "    raise\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Ensure device variable is available\n",
    "if 'device' not in globals():\n",
    "    print(\"‚ö†Ô∏è Device variable not found, setting to CPU\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    globals()['device'] = device\n",
    "\n",
    "# Final verification\n",
    "print(\"\\nüéâ All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da9abac",
   "metadata": {},
   "source": [
    "## 4. Download and Setup FER 2013 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Setup FER 2013 Dataset\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive if needed\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Install Kaggle API\n",
    "    !pip install kaggle -q\n",
    "    \n",
    "    # Create directories\n",
    "    os.makedirs('/content/data', exist_ok=True)\n",
    "    \n",
    "    print(\"üìÅ Setting up dataset directories...\")\n",
    "    print(\"\\nüîó To download FER 2013 dataset automatically:\")\n",
    "    print(\"1. Go to https://www.kaggle.com/account\")\n",
    "    print(\"2. Create a new API token (kaggle.json)\")\n",
    "    print(\"3. Upload kaggle.json to Colab files\")\n",
    "    print(\"4. Run the following commands:\")\n",
    "    print(\"   !mkdir -p ~/.kaggle\")\n",
    "    print(\"   !cp kaggle.json ~/.kaggle/\")\n",
    "    print(\"   !chmod 600 ~/.kaggle/kaggle.json\")\n",
    "    print(\"   !kaggle datasets download -d msambare/fer2013 -p /content/data\")\n",
    "    print(\"   !unzip /content/data/fer2013.zip -d /content/data/\")\n",
    "    \n",
    "    print(\"\\nüåê Alternative: Manual download from:\")\n",
    "    print(\"   https://www.kaggle.com/datasets/msambare/fer2013\")\n",
    "    print(\"   Extract to: /content/data/fer2013/\")\n",
    "    \n",
    "    dataset_path = '/content/data/fer2013'\n",
    "else:\n",
    "    # Local setup\n",
    "    dataset_path = './data/fer2013'\n",
    "    os.makedirs(dataset_path, exist_ok=True)\n",
    "    print(f\"üìÅ Local dataset path: {dataset_path}\")\n",
    "    print(\"üåê Please download FER 2013 dataset from:\")\n",
    "    print(\"   https://www.kaggle.com/datasets/msambare/fer2013\")\n",
    "    print(f\"   Extract to: {dataset_path}\")\n",
    "\n",
    "print(f\"\\nüìç Dataset path: {dataset_path}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if os.path.exists(dataset_path):\n",
    "    subdirs = os.listdir(dataset_path)\n",
    "    print(f\"‚úÖ Dataset directory found with contents: {subdirs}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset directory not found - will use dummy data for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9ac444",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class for FER 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FER2013Dataset(Dataset):\n",
    "    def __init__(self, data_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        FER 2013 Dataset class\n",
    "        \n",
    "        Args:\n",
    "            data_dir (str): Path to the dataset directory\n",
    "            split (str): 'train', 'test', or 'validation'\n",
    "            transform: Data transformations to apply\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Emotion labels\n",
    "        self.emotion_labels = {\n",
    "            'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,\n",
    "            'sad': 4, 'surprise': 5, 'neutral': 6\n",
    "        }\n",
    "        \n",
    "        self.label_to_emotion = {v: k for k, v in self.emotion_labels.items()}\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        \n",
    "        split_dir = os.path.join(data_dir, split)\n",
    "        \n",
    "        if os.path.exists(split_dir):\n",
    "            for emotion in self.emotion_labels.keys():\n",
    "                emotion_dir = os.path.join(split_dir, emotion)\n",
    "                if os.path.exists(emotion_dir):\n",
    "                    for img_file in os.listdir(emotion_dir):\n",
    "                        if img_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                            self.images.append(os.path.join(emotion_dir, img_file))\n",
    "                            self.labels.append(self.emotion_labels[emotion])\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images for {split} split\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if image is None:\n",
    "            # Create a dummy image if loading fails\n",
    "            image = np.zeros((48, 48), dtype=np.uint8)\n",
    "        \n",
    "        # Resize to 48x48 if needed\n",
    "        if image.shape != (48, 48):\n",
    "            image = cv2.resize(image, (48, 48))\n",
    "        \n",
    "        # Convert to PIL Image for transforms\n",
    "        image = Image.fromarray(image)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "print(\"‚úÖ FER2013Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf72cd23",
   "metadata": {},
   "source": [
    "## 6. Data Transformations and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a0518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Data transformations defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7d61c",
   "metadata": {},
   "source": [
    "## 7. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ebe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "try:\n",
    "    train_dataset = FER2013Dataset(dataset_path, split='train', transform=train_transform)\n",
    "    val_dataset = FER2013Dataset(dataset_path, split='validation', transform=val_test_transform)\n",
    "    test_dataset = FER2013Dataset(dataset_path, split='test', transform=val_test_transform)\n",
    "    \n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Please make sure the FER 2013 dataset is properly downloaded and extracted.\")\n",
    "    \n",
    "    # Create dummy datasets for demonstration\n",
    "    print(\"Creating dummy datasets for demonstration...\")\n",
    "    \n",
    "    class DummyDataset(Dataset):\n",
    "        def __init__(self, size=1000, transform=None):\n",
    "            self.size = size\n",
    "            self.transform = transform\n",
    "        \n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Generate random grayscale image\n",
    "            image = torch.randn(1, 48, 48)\n",
    "            label = torch.randint(0, 7, (1,)).item()\n",
    "            return image, label\n",
    "    \n",
    "    train_dataset = DummyDataset(size=20000)\n",
    "    val_dataset = DummyDataset(size=3000)\n",
    "    test_dataset = DummyDataset(size=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea4602",
   "metadata": {},
   "source": [
    "## 8. Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets with robust error handling for FER2013 structure\n",
    "def create_dummy_dataset(split_name, num_samples):\n",
    "    \"\"\"Create a dummy dataset for demonstration when real data isn't available\"\"\"\n",
    "    \n",
    "    class DummyFERDataset(Dataset):\n",
    "        def __init__(self, num_samples, transform=None):\n",
    "            self.num_samples = num_samples\n",
    "            self.transform = transform\n",
    "            self.emotion_labels = {\n",
    "                'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3,\n",
    "                'sad': 4, 'surprise': 5, 'neutral': 6\n",
    "            }\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self.num_samples\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # Generate realistic-looking face image (48x48 grayscale)\n",
    "            np.random.seed(idx)  # Consistent images for same index\n",
    "            \n",
    "            # Create a face-like pattern\n",
    "            image = np.random.randint(50, 200, (48, 48), dtype=np.uint8)\n",
    "            \n",
    "            # Add some face-like features\n",
    "            # Eyes\n",
    "            cv2.circle(image, (15, 18), 3, 100, -1)\n",
    "            cv2.circle(image, (33, 18), 3, 100, -1)\n",
    "            \n",
    "            # Nose\n",
    "            cv2.circle(image, (24, 28), 2, 120, -1)\n",
    "            \n",
    "            # Mouth (varies by emotion)\n",
    "            emotion = idx % 7\n",
    "            if emotion == 3:  # Happy\n",
    "                cv2.ellipse(image, (24, 35), (8, 4), 0, 0, 180, 80, -1)\n",
    "            elif emotion == 4:  # Sad\n",
    "                cv2.ellipse(image, (24, 38), (8, 4), 0, 180, 360, 80, -1)\n",
    "            else:\n",
    "                cv2.line(image, (18, 36), (30, 36), 90, 2)\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            image = Image.fromarray(image)\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            label = emotion\n",
    "            return image, label\n",
    "    \n",
    "    return DummyFERDataset(num_samples)\n",
    "\n",
    "# Try to load real datasets first - FER2013 typically has train/test structure\n",
    "dataset_loaded = False\n",
    "try:\n",
    "    if os.path.exists(dataset_path):\n",
    "        # Check for available directories\n",
    "        available_dirs = []\n",
    "        for split in ['train', 'test', 'validation']:\n",
    "            if os.path.exists(os.path.join(dataset_path, split)):\n",
    "                available_dirs.append(split)\n",
    "        \n",
    "        print(f\"üìÇ Found dataset directories: {available_dirs}\")\n",
    "        \n",
    "        # Strategy 1: train + test directories (most common FER2013 structure)\n",
    "        if 'train' in available_dirs and 'test' in available_dirs:\n",
    "            print(\"üéØ Using FER2013 standard structure: train + test\")\n",
    "            \n",
    "            # Load full training dataset\n",
    "            full_train_dataset = FER2013Dataset(dataset_path, split='train', transform=train_transform)\n",
    "            test_dataset = FER2013Dataset(dataset_path, split='test', transform=val_test_transform)\n",
    "            \n",
    "            if len(full_train_dataset) > 0:\n",
    "                # Split training data into train/validation (80/20 split)\n",
    "                from torch.utils.data import random_split\n",
    "                train_size = int(0.8 * len(full_train_dataset))\n",
    "                val_size = len(full_train_dataset) - train_size\n",
    "                \n",
    "                # Set generator for reproducible splits\n",
    "                generator = torch.Generator().manual_seed(42)\n",
    "                train_dataset, val_dataset = random_split(\n",
    "                    full_train_dataset, \n",
    "                    [train_size, val_size], \n",
    "                    generator=generator\n",
    "                )\n",
    "                \n",
    "                dataset_loaded = True\n",
    "                print(f\"‚úÖ Successfully loaded and split FER 2013 dataset!\")\n",
    "                print(f\"   Original train: {len(full_train_dataset)} samples\")\n",
    "                print(f\"   Split train: {len(train_dataset)} samples (80%)\")\n",
    "                print(f\"   Validation: {len(val_dataset)} samples (20%)\") \n",
    "                print(f\"   Test: {len(test_dataset)} samples\")\n",
    "                \n",
    "        # Strategy 2: train + test + validation directories (if validation exists)\n",
    "        elif 'train' in available_dirs and 'test' in available_dirs and 'validation' in available_dirs:\n",
    "            print(\"üéØ Using FER2013 extended structure: train + validation + test\")\n",
    "            \n",
    "            train_dataset = FER2013Dataset(dataset_path, split='train', transform=train_transform)\n",
    "            val_dataset = FER2013Dataset(dataset_path, split='validation', transform=val_test_transform)\n",
    "            test_dataset = FER2013Dataset(dataset_path, split='test', transform=val_test_transform)\n",
    "            \n",
    "            if len(train_dataset) > 0:\n",
    "                dataset_loaded = True\n",
    "                print(f\"‚úÖ Successfully loaded FER 2013 dataset with existing validation!\")\n",
    "                print(f\"   Train: {len(train_dataset)} samples\")\n",
    "                print(f\"   Validation: {len(val_dataset)} samples\") \n",
    "                print(f\"   Test: {len(test_dataset)} samples\")\n",
    "        \n",
    "        # Strategy 3: Only training data available\n",
    "        elif 'train' in available_dirs:\n",
    "            print(\"üéØ Only training data found, splitting into train/val/test\")\n",
    "            \n",
    "            full_dataset = FER2013Dataset(dataset_path, split='train', transform=train_transform)\n",
    "            \n",
    "            if len(full_dataset) > 0:\n",
    "                # Split: 70% train, 15% validation, 15% test\n",
    "                total_size = len(full_dataset)\n",
    "                train_size = int(0.7 * total_size)\n",
    "                val_size = int(0.15 * total_size)\n",
    "                test_size = total_size - train_size - val_size\n",
    "                \n",
    "                generator = torch.Generator().manual_seed(42)\n",
    "                train_dataset, val_dataset, test_dataset = random_split(\n",
    "                    full_dataset, \n",
    "                    [train_size, val_size, test_size],\n",
    "                    generator=generator\n",
    "                )\n",
    "                \n",
    "                dataset_loaded = True\n",
    "                print(f\"‚úÖ Split single dataset into train/val/test!\")\n",
    "                print(f\"   Train: {len(train_dataset)} samples (70%)\")\n",
    "                print(f\"   Validation: {len(val_dataset)} samples (15%)\") \n",
    "                print(f\"   Test: {len(test_dataset)} samples (15%)\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå No valid dataset structure found\")\n",
    "            print(\"Expected: 'train' directory (required), 'test' directory (optional)\")\n",
    "    else:\n",
    "        print(\"‚ùå Dataset directory does not exist\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading real dataset: {e}\")\n",
    "\n",
    "# Create dummy datasets if real data not available\n",
    "if not dataset_loaded:\n",
    "    print(\"\\nüé≠ Creating dummy datasets for demonstration...\")\n",
    "    print(\"   (These contain synthetic face-like images with basic emotion patterns)\")\n",
    "    \n",
    "    train_dataset = create_dummy_dataset('train', 20000)\n",
    "    val_dataset = create_dummy_dataset('validation', 3000)  \n",
    "    test_dataset = create_dummy_dataset('test', 3000)\n",
    "    \n",
    "    # Apply transforms to dummy datasets\n",
    "    train_dataset.transform = train_transform\n",
    "    val_dataset.transform = val_test_transform\n",
    "    test_dataset.transform = val_test_transform\n",
    "    \n",
    "    print(f\"‚úÖ Dummy datasets created:\")\n",
    "    print(f\"   Train: {len(train_dataset)} samples\")\n",
    "    print(f\"   Validation: {len(val_dataset)} samples\")\n",
    "    print(f\"   Test: {len(test_dataset)} samples\")\n",
    "    print(\"\\nüí° To use real data, download FER 2013 dataset and rerun this cell\")\n",
    "\n",
    "# Final verification\n",
    "print(f\"\\nüìä Final dataset sizes:\")\n",
    "print(f\"   Training: {len(train_dataset)}\")\n",
    "print(f\"   Validation: {len(val_dataset)}\")\n",
    "print(f\"   Test: {len(test_dataset)}\")\n",
    "\n",
    "# Verify splits are balanced (for real datasets)\n",
    "if dataset_loaded and hasattr(train_dataset, 'dataset'):\n",
    "    print(f\"\\nüîÑ Dataset split summary:\")\n",
    "    print(f\"   Original dataset: {len(train_dataset.dataset) if hasattr(train_dataset, 'dataset') else 'N/A'}\")\n",
    "    print(f\"   Train split: {len(train_dataset)} ({len(train_dataset)/(len(train_dataset)+len(val_dataset))*100:.1f}%)\")\n",
    "    print(f\"   Validation split: {len(val_dataset)} ({len(val_dataset)/(len(train_dataset)+len(val_dataset))*100:.1f}%)\")\n",
    "\n",
    "if len(train_dataset) == 0:\n",
    "    raise ValueError(\"Training dataset is empty! Please check dataset setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6d4fcd",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6e6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Loaders with validation\n",
    "# Verify datasets are not empty\n",
    "for dataset_name, dataset in [('train', train_dataset), ('validation', val_dataset), ('test', test_dataset)]:\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(f\"{dataset_name} dataset is empty! Cannot create data loaders.\")\n",
    "    print(f\"‚úÖ {dataset_name} dataset verified: {len(dataset)} samples\")\n",
    "\n",
    "# Set batch size based on available memory and dataset size\n",
    "min_samples = min(len(train_dataset), len(val_dataset), len(test_dataset))\n",
    "if device.type == 'cuda':\n",
    "    BATCH_SIZE = min(64, min_samples)  # Don't exceed available samples\n",
    "else:\n",
    "    BATCH_SIZE = min(32, min_samples)\n",
    "\n",
    "# Ensure batch size is reasonable\n",
    "BATCH_SIZE = max(1, min(BATCH_SIZE, 64))\n",
    "\n",
    "NUM_WORKERS = 2 if IN_COLAB else 0\n",
    "\n",
    "print(f\"üì¶ Using batch size: {BATCH_SIZE}\")\n",
    "print(f\"üë• Using {NUM_WORKERS} workers\")\n",
    "\n",
    "# Create data loaders with error handling\n",
    "try:\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        drop_last=True if len(train_dataset) > BATCH_SIZE else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if device.type == 'cuda' else False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Data loaders created successfully!\")\n",
    "    print(f\"üìä Training batches: {len(train_loader)}\")\n",
    "    print(f\"üìä Validation batches: {len(val_loader)}\")\n",
    "    print(f\"üìä Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Test data loader functionality\n",
    "    print(\"\\nüß™ Testing data loaders...\")\n",
    "    try:\n",
    "        # Test training loader\n",
    "        train_iter = iter(train_loader)\n",
    "        batch_data, batch_labels = next(train_iter)\n",
    "        print(f\"‚úÖ Train batch shape: {batch_data.shape}, Labels shape: {batch_labels.shape}\")\n",
    "        \n",
    "        # Test validation loader  \n",
    "        val_iter = iter(val_loader)\n",
    "        batch_data, batch_labels = next(val_iter)\n",
    "        print(f\"‚úÖ Validation batch shape: {batch_data.shape}, Labels shape: {batch_labels.shape}\")\n",
    "        \n",
    "        print(\"üéâ All data loaders working correctly!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Data loader test failed: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating data loaders: {e}\")\n",
    "    print(\"This might be due to dataset issues. Please check the dataset loading cell.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77bc87",
   "metadata": {},
   "source": [
    "## 10. Define CNN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bacb30",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sample Data\n",
    "# Emotion labels for visualization\n",
    "emotion_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "emotion_emojis = ['üò†', 'ü§¢', 'üò®', 'üòä', 'üò¢', 'üò≤', 'üòê']\n",
    "\n",
    "def visualize_samples(data_loader, num_samples=8, title=\"Sample Images\"):\n",
    "    \"\"\"Visualize sample images from a data loader\"\"\"\n",
    "    try:\n",
    "        data_iter = iter(data_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        \n",
    "        # Handle case where batch size is smaller than num_samples\n",
    "        actual_samples = min(num_samples, len(images))\n",
    "        \n",
    "        # Calculate subplot dimensions\n",
    "        cols = 4\n",
    "        rows = (actual_samples + cols - 1) // cols  # Ceiling division\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(12, 3*rows))\n",
    "        if rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i in range(actual_samples):\n",
    "            row, col = i // cols, i % cols\n",
    "            \n",
    "            # Convert tensor to numpy and denormalize\n",
    "            img = images[i].squeeze().numpy()\n",
    "            \n",
    "            # Handle different normalization ranges\n",
    "            if img.min() < 0:  # If normalized to [-1, 1]\n",
    "                img = (img + 1) / 2  # Convert to [0, 1]\n",
    "            elif img.max() > 1:  # If not normalized\n",
    "                img = img / 255.0   # Convert to [0, 1]\n",
    "            \n",
    "            # Ensure values are in valid range\n",
    "            img = np.clip(img, 0, 1)\n",
    "            \n",
    "            axes[row, col].imshow(img, cmap='gray')\n",
    "            axes[row, col].set_title(f'{emotion_names[labels[i]]} {emotion_emojis[labels[i]]}')\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(actual_samples, rows * cols):\n",
    "            row, col = i // cols, i % cols\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Show label distribution in this batch\n",
    "        unique_labels, counts = np.unique(labels.numpy(), return_counts=True)\n",
    "        print(f\"üìä Labels in batch: {dict(zip([emotion_names[l] for l in unique_labels], counts))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Visualization error: {e}\")\n",
    "        print(\"This might be due to data format issues.\")\n",
    "\n",
    "# Visualize samples from different sets\n",
    "print(\"üñºÔ∏è Sample images from training set:\")\n",
    "visualize_samples(train_loader, title=\"Training Set Samples\")\n",
    "\n",
    "print(\"\\nüñºÔ∏è Sample images from validation set:\")\n",
    "visualize_samples(val_loader, num_samples=4, title=\"Validation Set Samples\")\n",
    "\n",
    "# Show overall dataset statistics\n",
    "print(\"\\nüìà Dataset Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def get_label_distribution(dataset):\n",
    "    \"\"\"Get label distribution for a dataset\"\"\"\n",
    "    labels = []\n",
    "    \n",
    "    # Sample a subset for efficiency (especially for large datasets)\n",
    "    sample_size = min(1000, len(dataset))\n",
    "    indices = np.random.choice(len(dataset), sample_size, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        _, label = dataset[idx]\n",
    "        labels.append(label)\n",
    "    \n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    return dict(zip([emotion_names[l] for l in unique_labels], counts))\n",
    "\n",
    "try:\n",
    "    train_dist = get_label_distribution(train_dataset)\n",
    "    print(f\"Training set distribution: {train_dist}\")\n",
    "    \n",
    "    val_dist = get_label_distribution(val_dataset)\n",
    "    print(f\"Validation set distribution: {val_dist}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate distribution: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data visualization complete!\")\n",
    "print(f\"Ready to proceed with model training on {len(train_dataset)} training samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74949b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "model = EmotionCNN(num_classes=7, dropout_rate=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model created and moved to {device}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6ed07e",
   "metadata": {},
   "source": [
    "## 11. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c86440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function and Optimizer\n",
    "# Check if model is defined\n",
    "try:\n",
    "    if 'model' not in locals():\n",
    "        raise NameError(\"Model not found. Please run the model definition cell first.\")\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Loss function, optimizer, and scheduler defined\")\n",
    "    print(f\"Loss function: {criterion}\")\n",
    "    print(f\"Optimizer: Adam with lr=0.001, weight_decay=1e-4\")\n",
    "    print(f\"Scheduler: ReduceLROnPlateau with patience=5\")\n",
    "    \n",
    "except NameError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    print(\"\\nüîß To fix this:\")\n",
    "    print(\"1. Make sure you've run all previous cells in order\")\n",
    "    print(\"2. Especially run the 'Define CNN Model Architecture' cell\")\n",
    "    print(\"3. Check that the model was created successfully\")\n",
    "    raise\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error: {e}\")\n",
    "    print(\"Please check the model definition and try again.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d579dc2",
   "metadata": {},
   "source": [
    "## 12. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9046c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "            'Acc': f'{100.*correct/total:.2f}%'\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(val_loader, desc='Validation')\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{running_loss/(batch_idx+1):.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"‚úÖ Training and validation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ed164",
   "metadata": {},
   "source": [
    "## 13. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "BEST_VAL_LOSS = float('inf')\n",
    "PATIENCE_COUNTER = 0\n",
    "\n",
    "# Lists to store training history\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    if val_loss < BEST_VAL_LOSS:\n",
    "        BEST_VAL_LOSS = val_loss\n",
    "        PATIENCE_COUNTER = 0\n",
    "        \n",
    "        # Save best model\n",
    "        if IN_COLAB:\n",
    "            torch.save(model.state_dict(), '/content/best_fer_model.pth')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), 'best_fer_model.pth')\n",
    "        \n",
    "        print(f\"‚úÖ New best model saved! Val Loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        PATIENCE_COUNTER += 1\n",
    "        print(f\"‚è≥ No improvement. Patience: {PATIENCE_COUNTER}/{EARLY_STOPPING_PATIENCE}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if PATIENCE_COUNTER >= EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\nüõë Early stopping triggered after {epoch+1} epochs\")\n",
    "        break\n",
    "\n",
    "print(\"\\nüéâ Training completed!\")\n",
    "print(f\"Best validation loss: {BEST_VAL_LOSS:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0a24c2",
   "metadata": {},
   "source": [
    "## 14. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e5ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(train_losses, label='Training Loss', color='blue')\n",
    "ax1.plot(val_losses, label='Validation Loss', color='red')\n",
    "ax1.set_title('Model Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy', color='red')\n",
    "ax2.set_title('Model Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"Final Training Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "print(f\"Best Validation Loss: {min(val_losses):.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(val_accuracies):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757327f6",
   "metadata": {},
   "source": [
    "## 15. Load Best Model and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bff4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "try:\n",
    "    if IN_COLAB:\n",
    "        model.load_state_dict(torch.load('/content/best_fer_model.pth'))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('best_fer_model.pth'))\n",
    "    print(\"‚úÖ Best model loaded successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Could not load saved model, using current model\")\n",
    "\n",
    "# Test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(test_loader, desc='Testing')\n",
    "        \n",
    "        for data, target in progress_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_targets)\n",
    "\n",
    "# Run test\n",
    "print(\"Testing the model...\")\n",
    "test_predictions, test_targets = test_model(model, test_loader, device)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "print(f\"\\nüéØ Test Accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd92bd7",
   "metadata": {},
   "source": [
    "## 16. Detailed Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(\"üìä Classification Report:\")\n",
    "print(\"-\" * 50)\n",
    "class_report = classification_report(\n",
    "    test_targets, \n",
    "    test_predictions, \n",
    "    target_names=emotion_names,\n",
    "    digits=4\n",
    ")\n",
    "print(class_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(test_targets, test_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=[f'{name}\\n{emoji}' for name, emoji in zip(emotion_names, emotion_emojis)],\n",
    "    yticklabels=[f'{name}\\n{emoji}' for name, emoji in zip(emotion_names, emotion_emojis)]\n",
    ")\n",
    "plt.title('Confusion Matrix - Facial Expression Recognition')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nüìà Per-class Accuracy:\")\n",
    "print(\"-\" * 30)\n",
    "for i, emotion in enumerate(emotion_names):\n",
    "    class_mask = test_targets == i\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_acc = np.sum(test_predictions[class_mask] == i) / np.sum(class_mask)\n",
    "        print(f\"{emotion} {emotion_emojis[i]}: {class_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333f3c9",
   "metadata": {},
   "source": [
    "## 17. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_loader, device, num_samples=12):\n",
    "    \"\"\"Visualize sample predictions\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch of test data\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Move back to CPU for visualization\n",
    "    images = images.cpu()\n",
    "    labels = labels.cpu()\n",
    "    predictions = predictions.cpu()\n",
    "    probabilities = probabilities.cpu()\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i in range(min(num_samples, len(images))):\n",
    "        # Denormalize image\n",
    "        img = images[i].squeeze().numpy()\n",
    "        img = (img + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
    "        \n",
    "        # Get prediction info\n",
    "        true_label = labels[i].item()\n",
    "        pred_label = predictions[i].item()\n",
    "        confidence = probabilities[i][pred_label].item()\n",
    "        \n",
    "        # Set title color based on correctness\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        \n",
    "        # Plot image\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(\n",
    "            f'True: {emotion_names[true_label]} {emotion_emojis[true_label]}\\n'\n",
    "            f'Pred: {emotion_names[pred_label]} {emotion_emojis[pred_label]}\\n'\n",
    "            f'Conf: {confidence:.2f}',\n",
    "            color=color,\n",
    "            fontsize=10\n",
    "        )\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample predictions (Green = Correct, Red = Incorrect):\")\n",
    "visualize_predictions(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25238851",
   "metadata": {},
   "source": [
    "## 18. Save Final Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02775cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "if IN_COLAB:\n",
    "    model_path = '/content/fer2013_final_model.pth'\n",
    "    results_path = '/content/training_results.json'\n",
    "else:\n",
    "    model_path = 'fer2013_final_model.pth'\n",
    "    results_path = 'training_results.json'\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'best_val_loss': BEST_VAL_LOSS,\n",
    "    'emotion_labels': emotion_names,\n",
    "    'model_architecture': str(model)\n",
    "}, model_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {model_path}\")\n",
    "\n",
    "# Save training results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'best_validation_loss': float(BEST_VAL_LOSS),\n",
    "    'best_validation_accuracy': float(max(val_accuracies)),\n",
    "    'final_training_accuracy': float(train_accuracies[-1]),\n",
    "    'final_validation_accuracy': float(val_accuracies[-1]),\n",
    "    'epochs_trained': len(train_losses),\n",
    "    'train_losses': [float(x) for x in train_losses],\n",
    "    'train_accuracies': [float(x) for x in train_accuracies],\n",
    "    'val_losses': [float(x) for x in val_losses],\n",
    "    'val_accuracies': [float(x) for x in val_accuracies],\n",
    "    'emotion_labels': emotion_names,\n",
    "    'model_parameters': total_params\n",
    "}\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìä Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"üìà Best Validation Accuracy: {max(val_accuracies):.2f}%\")\n",
    "print(f\"üìâ Best Validation Loss: {BEST_VAL_LOSS:.4f}\")\n",
    "print(f\"üïê Epochs Trained: {len(train_losses)}\")\n",
    "print(f\"üîß Model Parameters: {total_params:,}\")\n",
    "print(f\"üíæ Model saved to: {model_path}\")\n",
    "print(f\"üìã Results saved to: {results_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba8bee",
   "metadata": {},
   "source": [
    "## 19. Model Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model, image_path, transform, device):\n",
    "    \"\"\"\n",
    "    Predict emotion from a single image\n",
    "    \n",
    "    Args:\n",
    "        model: Trained emotion recognition model\n",
    "        image_path: Path to the image file\n",
    "        transform: Image preprocessing transforms\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        predicted_emotion: Predicted emotion name\n",
    "        confidence: Prediction confidence\n",
    "        probabilities: All class probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not load image from {image_path}\")\n",
    "    \n",
    "    # Resize to 48x48\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    \n",
    "    # Convert to PIL Image and apply transforms\n",
    "    image = Image.fromarray(image)\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        confidence, predicted = torch.max(probabilities, 1)\n",
    "    \n",
    "    predicted_emotion = emotion_names[predicted.item()]\n",
    "    confidence_score = confidence.item()\n",
    "    all_probs = probabilities.cpu().numpy()[0]\n",
    "    \n",
    "    return predicted_emotion, confidence_score, all_probs\n",
    "\n",
    "def display_prediction_results(emotion, confidence, probabilities):\n",
    "    \"\"\"\n",
    "    Display prediction results in a formatted way\n",
    "    \"\"\"\n",
    "    print(f\"\\nüéØ Predicted Emotion: {emotion} {emotion_emojis[emotion_names.index(emotion)]}\")\n",
    "    print(f\"üìä Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "    print(\"\\nüìà All Probabilities:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, (emotion_name, emoji, prob) in enumerate(zip(emotion_names, emotion_emojis, probabilities)):\n",
    "        print(f\"{emotion_name} {emoji}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "print(\"‚úÖ Inference functions defined\")\n",
    "print(\"\\nTo use the model for inference on a new image:\")\n",
    "print(\"```python\")\n",
    "print(\"emotion, confidence, probs = predict_emotion(model, 'path/to/image.jpg', val_test_transform, device)\")\n",
    "print(\"display_prediction_results(emotion, confidence, probs)\")\n",
    "print(\"```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbbb42e",
   "metadata": {},
   "source": [
    "## 20. Instructions for Using the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dabf4e",
   "metadata": {},
   "source": [
    "### üöÄ How to Use Your Trained FER Model\n",
    "\n",
    "#### **Loading the Model:**\n",
    "```python\n",
    "# Create model instance\n",
    "model = EmotionCNN(num_classes=7)\n",
    "model.load_state_dict(torch.load('fer2013_final_model.pth')['model_state_dict'])\n",
    "model.eval()\n",
    "```\n",
    "\n",
    "#### **Making Predictions:**\n",
    "```python\n",
    "# For a single image\n",
    "emotion, confidence, probs = predict_emotion(\n",
    "    model, \n",
    "    'path/to/your/image.jpg', \n",
    "    val_test_transform, \n",
    "    device\n",
    ")\n",
    "display_prediction_results(emotion, confidence, probs)\n",
    "```\n",
    "\n",
    "#### **Integration Tips:**\n",
    "- The model expects 48x48 grayscale images\n",
    "- Use the same preprocessing transforms as during training\n",
    "- The model outputs 7 emotion classes: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral\n",
    "- For real-time applications, consider using GPU acceleration\n",
    "\n",
    "#### **Performance Expectations:**\n",
    "- Test accuracy achieved: **{test_accuracy*100:.2f}%**\n",
    "- Best validation accuracy: **{max(val_accuracies):.2f}%**\n",
    "- Model size: **{total_params:,} parameters**\n",
    "\n",
    "#### **Next Steps:**\n",
    "1. **Fine-tuning**: Retrain on your specific domain data\n",
    "2. **Deployment**: Convert to ONNX for production use\n",
    "3. **Integration**: Combine with face detection for end-to-end emotion recognition\n",
    "4. **Evaluation**: Test on your specific use case data\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources:\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
    "- [FER 2013 Dataset](https://www.kaggle.com/datasets/msambare/fer2013)\n",
    "- [OpenCV Face Detection](https://docs.opencv.org/4.x/db/d28/tutorial_cascade_classifier.html)\n",
    "\n",
    "### üéâ Congratulations!\n",
    "You have successfully trained a Facial Expression Recognition model! The model is ready for inference and can be integrated into your applications."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f1a544",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Recognition with Late Fusion\n",
    "\n",
    "This notebook demonstrates how to combine trained Facial Expression Recognition (FER) and Textual Emotion Recognition (TER) models using late fusion to create a powerful multimodal emotion recognition system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **FER Model**: CNN-based facial expression recognition trained on FER2013 dataset\n",
    "- **TER Model**: DistilBERT-based textual emotion recognition trained on emotion datasets\n",
    "- **Fusion Strategy**: Late fusion combining predictions from both modalities\n",
    "- **Target**: Improved emotion recognition accuracy through multimodal learning\n",
    "\n",
    "## Emotions Recognized:\n",
    "- **Angry** üò†\n",
    "- **Disgust** ü§¢  \n",
    "- **Fear** üò®\n",
    "- **Happy** üòä\n",
    "- **Sad** üò¢\n",
    "- **Surprise** üò≤\n",
    "- **Neutral** üòê\n",
    "\n",
    "The notebook is optimized to run on Google Colab with GPU acceleration for efficient training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb5fa5",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Google Drive Mount\n",
    "\n",
    "Set up the environment, check for GPU availability, and mount Google Drive to access pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888dbd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted successfully\")\n",
    "    \n",
    "    # Set paths for Google Drive\n",
    "    FER_MODEL_PATH = \"/content/drive/MyDrive/FER_Model_Data/models\"\n",
    "    TER_MODEL_PATH = \"/content/drive/MyDrive/TER_Models/ter_distilbert_model\"\n",
    "    FUSION_MODEL_PATH = \"/content/drive/MyDrive/Fusion_Models\"\n",
    "    \n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ùå Not running on Google Colab\")\n",
    "    \n",
    "    # Set local paths\n",
    "    FER_MODEL_PATH = \"./fer_models\"\n",
    "    TER_MODEL_PATH = \"./ter_models\"\n",
    "    FUSION_MODEL_PATH = \"./fusion_models\"\n",
    "\n",
    "# Check CUDA availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA available: {torch.version.cuda}\")\n",
    "    print(f\"‚úÖ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"üéØ Using device: {device}\")\n",
    "\n",
    "# Create fusion model directory\n",
    "import os\n",
    "os.makedirs(FUSION_MODEL_PATH, exist_ok=True)\n",
    "\n",
    "print(f\"\\nüìÅ Model paths configured:\")\n",
    "print(f\"   FER models: {FER_MODEL_PATH}\")\n",
    "print(f\"   TER models: {TER_MODEL_PATH}\")\n",
    "print(f\"   Fusion models: {FUSION_MODEL_PATH}\")\n",
    "\n",
    "# Constants for emotions (Ekman's basic emotions)\n",
    "EMOTION_LABELS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "NUM_CLASSES = len(EMOTION_LABELS)\n",
    "EMOTION_NAMES = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "EMOTION_EMOJIS = ['üò†', 'ü§¢', 'üò®', 'üòä', 'üò¢', 'üò≤', 'üòê']\n",
    "\n",
    "print(f\"\\nüé≠ Emotion classes: {NUM_CLASSES}\")\n",
    "print(f\"   Labels: {EMOTION_LABELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6874dda9",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for loading models, creating fusion architecture, and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd679e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if in Colab\n",
    "if IN_COLAB:\n",
    "    !pip install transformers torch torchvision torchaudio opencv-python-headless -q\n",
    "    !pip install matplotlib seaborn scikit-learn pandas numpy tqdm pillow -q\n",
    "    print(\"‚úÖ Packages installed in Colab\")\n",
    "\n",
    "# Core libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Computer Vision\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# NLP and Transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "\n",
    "# Data handling and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Set matplotlib style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3eced",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained FER Model\n",
    "\n",
    "Load the trained CNN-based Facial Expression Recognition model from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d85638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FER CNN architecture (must match the trained model)\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7, dropout_rate=0.5):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Pooling\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First conv block\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Second conv block\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Load the FER model\n",
    "print(\"Loading FER model...\")\n",
    "\n",
    "try:\n",
    "    # Create model instance\n",
    "    fer_model = EmotionCNN(num_classes=NUM_CLASSES, dropout_rate=0.5)\n",
    "    \n",
    "    # Try different possible model file names\n",
    "    fer_model_files = [\n",
    "        'fer2013_final_model.pth',\n",
    "        'best_fer_model.pth', \n",
    "        'emotion_cnn_model.pth'\n",
    "    ]\n",
    "    \n",
    "    fer_model_loaded = False\n",
    "    for model_file in fer_model_files:\n",
    "        fer_model_path = os.path.join(FER_MODEL_PATH, model_file)\n",
    "        if os.path.exists(fer_model_path):\n",
    "            try:\n",
    "                checkpoint = torch.load(fer_model_path, map_location=device)\n",
    "                \n",
    "                # Handle different checkpoint formats\n",
    "                if isinstance(checkpoint, dict):\n",
    "                    if 'model_state_dict' in checkpoint:\n",
    "                        fer_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                    elif 'state_dict' in checkpoint:\n",
    "                        fer_model.load_state_dict(checkpoint['state_dict'])\n",
    "                    else:\n",
    "                        fer_model.load_state_dict(checkpoint)\n",
    "                else:\n",
    "                    fer_model.load_state_dict(checkpoint)\n",
    "                \n",
    "                fer_model.to(device)\n",
    "                fer_model.eval()\n",
    "                fer_model_loaded = True\n",
    "                print(f\"‚úÖ FER model loaded successfully from: {model_file}\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load {model_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if not fer_model_loaded:\n",
    "        print(\"‚ùå Could not load FER model from any expected file\")\n",
    "        print(f\"Expected files in {FER_MODEL_PATH}:\")\n",
    "        for f in fer_model_files:\n",
    "            print(f\"  - {f}\")\n",
    "        \n",
    "        # Create a dummy model for demonstration\n",
    "        print(\"Creating dummy FER model for demonstration...\")\n",
    "        fer_model = EmotionCNN(num_classes=NUM_CLASSES)\n",
    "        fer_model.to(device)\n",
    "        fer_model.eval()\n",
    "        print(\"‚ö†Ô∏è Using randomly initialized FER model\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading FER model: {e}\")\n",
    "    # Create dummy model\n",
    "    fer_model = EmotionCNN(num_classes=NUM_CLASSES)\n",
    "    fer_model.to(device)\n",
    "    fer_model.eval()\n",
    "    print(\"‚ö†Ô∏è Using randomly initialized FER model\")\n",
    "\n",
    "# Define FER preprocessing transforms\n",
    "fer_transform = transforms.Compose([\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "print(f\"‚úÖ FER model ready on {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in fer_model.parameters()):,}\")\n",
    "\n",
    "# Test FER model with dummy input\n",
    "try:\n",
    "    dummy_image = torch.randn(1, 1, 48, 48).to(device)\n",
    "    with torch.no_grad():\n",
    "        fer_output = fer_model(dummy_image)\n",
    "    print(f\"‚úÖ FER model test successful - Output shape: {fer_output.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå FER model test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6c182a",
   "metadata": {},
   "source": [
    "## 4. Load Pre-trained TER Model\n",
    "\n",
    "Load the trained DistilBERT-based Textual Emotion Recognition model from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TER model (DistilBERT)\n",
    "print(\"Loading TER model...\")\n",
    "\n",
    "try:\n",
    "    # Check if TER model directory exists\n",
    "    if os.path.exists(TER_MODEL_PATH):\n",
    "        print(f\"üìÅ TER model directory found: {TER_MODEL_PATH}\")\n",
    "        \n",
    "        # Load DistilBERT model\n",
    "        ter_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            TER_MODEL_PATH,\n",
    "            num_labels=NUM_CLASSES,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        ter_tokenizer = DistilBertTokenizer.from_pretrained(TER_MODEL_PATH)\n",
    "        \n",
    "        # Load label encoder\n",
    "        label_encoder_path = os.path.join(TER_MODEL_PATH, 'label_encoder.pkl')\n",
    "        if os.path.exists(label_encoder_path):\n",
    "            with open(label_encoder_path, 'rb') as f:\n",
    "                ter_label_encoder = pickle.load(f)\n",
    "            print(\"‚úÖ Label encoder loaded successfully\")\n",
    "        else:\n",
    "            # Create default label encoder\n",
    "            ter_label_encoder = LabelEncoder()\n",
    "            ter_label_encoder.fit(EMOTION_LABELS)\n",
    "            print(\"‚ö†Ô∏è Created default label encoder\")\n",
    "        \n",
    "        # Load training config\n",
    "        config_path = os.path.join(TER_MODEL_PATH, 'training_config.pkl')\n",
    "        if os.path.exists(config_path):\n",
    "            with open(config_path, 'rb') as f:\n",
    "                ter_config = pickle.load(f)\n",
    "            print(\"‚úÖ Training config loaded successfully\")\n",
    "        else:\n",
    "            ter_config = {\n",
    "                'max_length': 128,\n",
    "                'num_classes': NUM_CLASSES,\n",
    "                'emotion_labels': EMOTION_LABELS\n",
    "            }\n",
    "            print(\"‚ö†Ô∏è Created default config\")\n",
    "        \n",
    "        ter_model.to(device)\n",
    "        ter_model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ TER model loaded successfully\")\n",
    "        print(f\"Model config: {ter_config.get('max_length', 128)} max tokens\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå TER model directory not found: {TER_MODEL_PATH}\")\n",
    "        raise FileNotFoundError(\"TER model not found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading TER model: {e}\")\n",
    "    print(\"Creating dummy TER model for demonstration...\")\n",
    "    \n",
    "    # Create dummy TER model\n",
    "    ter_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        'distilbert-base-uncased',\n",
    "        num_labels=NUM_CLASSES,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False\n",
    "    )\n",
    "    ter_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # Create default label encoder\n",
    "    ter_label_encoder = LabelEncoder()\n",
    "    ter_label_encoder.fit(EMOTION_LABELS)\n",
    "    \n",
    "    ter_config = {\n",
    "        'max_length': 128,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'emotion_labels': EMOTION_LABELS\n",
    "    }\n",
    "    \n",
    "    ter_model.to(device)\n",
    "    ter_model.eval()\n",
    "    print(\"‚ö†Ô∏è Using pre-trained DistilBERT (not fine-tuned)\")\n",
    "\n",
    "# Text preprocessing function\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\!\\?\\,\\;\\:]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# TER prediction function\n",
    "def predict_ter(text, max_length=None):\n",
    "    \"\"\"Predict emotion from text using TER model\"\"\"\n",
    "    if max_length is None:\n",
    "        max_length = ter_config.get('max_length', 128)\n",
    "    \n",
    "    # Clean text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = ter_tokenizer(\n",
    "        cleaned_text,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = ter_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    return logits\n",
    "\n",
    "print(f\"‚úÖ TER model ready on {device}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in ter_model.parameters()):,}\")\n",
    "\n",
    "# Test TER model with dummy input\n",
    "try:\n",
    "    test_text = \"I am feeling happy today!\"\n",
    "    ter_output = predict_ter(test_text)\n",
    "    print(f\"‚úÖ TER model test successful - Output shape: {ter_output.shape}\")\n",
    "    \n",
    "    # Show prediction\n",
    "    probabilities = F.softmax(ter_output, dim=1)\n",
    "    predicted_class = torch.argmax(ter_output, dim=1).item()\n",
    "    confidence = probabilities[0][predicted_class].item()\n",
    "    predicted_emotion = ter_label_encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "    print(f\"Test prediction: '{test_text}' -> {predicted_emotion} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TER model test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abacae68",
   "metadata": {},
   "source": [
    "## 5. Define Late Fusion Architecture\n",
    "\n",
    "Create a late fusion neural network that combines predictions from both FER and TER models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a921cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Late Fusion Model that combines FER and TER predictions\n",
    "    \n",
    "    This model takes the output logits from both FER and TER models\n",
    "    and learns to fuse them for improved emotion recognition.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=7, fusion_type='weighted', hidden_dim=128):\n",
    "        super(LateFusionModel, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.fusion_type = fusion_type\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        if fusion_type == 'weighted':\n",
    "            # Learnable weights for each modality\n",
    "            self.fusion_weights = nn.Parameter(torch.tensor([0.5, 0.5]))\n",
    "            \n",
    "        elif fusion_type == 'mlp':\n",
    "            # MLP fusion network\n",
    "            self.fusion_mlp = nn.Sequential(\n",
    "                nn.Linear(num_classes * 2, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "                nn.Linear(hidden_dim // 2, num_classes)\n",
    "            )\n",
    "            \n",
    "        elif fusion_type == 'attention':\n",
    "            # Attention-based fusion\n",
    "            self.attention = nn.MultiheadAttention(\n",
    "                embed_dim=num_classes,\n",
    "                num_heads=1,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.output_projection = nn.Linear(num_classes, num_classes)\n",
    "            \n",
    "        elif fusion_type == 'bilinear':\n",
    "            # Bilinear fusion\n",
    "            self.bilinear = nn.Bilinear(num_classes, num_classes, hidden_dim)\n",
    "            self.output_layer = nn.Linear(hidden_dim, num_classes)\n",
    "            \n",
    "        else:  # simple averaging\n",
    "            pass\n",
    "    \n",
    "    def forward(self, fer_logits, ter_logits):\n",
    "        \"\"\"\n",
    "        Forward pass for late fusion\n",
    "        \n",
    "        Args:\n",
    "            fer_logits: Logits from FER model [batch_size, num_classes]\n",
    "            ter_logits: Logits from TER model [batch_size, num_classes]\n",
    "            \n",
    "        Returns:\n",
    "            fused_logits: Combined logits [batch_size, num_classes]\n",
    "            fusion_weights: Attention weights (if applicable)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.fusion_type == 'simple':\n",
    "            # Simple averaging\n",
    "            fused_logits = (fer_logits + ter_logits) / 2\n",
    "            fusion_weights = torch.tensor([0.5, 0.5]).to(fer_logits.device)\n",
    "            \n",
    "        elif self.fusion_type == 'weighted':\n",
    "            # Learnable weighted combination\n",
    "            weights = F.softmax(self.fusion_weights, dim=0)\n",
    "            fused_logits = weights[0] * fer_logits + weights[1] * ter_logits\n",
    "            fusion_weights = weights\n",
    "            \n",
    "        elif self.fusion_type == 'mlp':\n",
    "            # MLP-based fusion\n",
    "            concatenated = torch.cat([fer_logits, ter_logits], dim=1)\n",
    "            fused_logits = self.fusion_mlp(concatenated)\n",
    "            fusion_weights = None\n",
    "            \n",
    "        elif self.fusion_type == 'attention':\n",
    "            # Attention-based fusion\n",
    "            # Stack the logits for attention\n",
    "            stacked_logits = torch.stack([fer_logits, ter_logits], dim=1)  # [batch, 2, num_classes]\n",
    "            \n",
    "            # Apply self-attention\n",
    "            attended_logits, attention_weights = self.attention(\n",
    "                stacked_logits, stacked_logits, stacked_logits\n",
    "            )\n",
    "            \n",
    "            # Weighted sum based on attention\n",
    "            fused_logits = torch.sum(attended_logits, dim=1)  # [batch, num_classes]\n",
    "            fused_logits = self.output_projection(fused_logits)\n",
    "            fusion_weights = attention_weights.mean(dim=1)  # Average across heads\n",
    "            \n",
    "        elif self.fusion_type == 'bilinear':\n",
    "            # Bilinear fusion\n",
    "            bilinear_output = self.bilinear(fer_logits, ter_logits)\n",
    "            fused_logits = self.output_layer(F.relu(bilinear_output))\n",
    "            fusion_weights = None\n",
    "            \n",
    "        return fused_logits, fusion_weights\n",
    "\n",
    "class MultiModalEmotionRecognizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete multimodal emotion recognition system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fer_model, ter_model, ter_tokenizer, fusion_type='weighted'):\n",
    "        super(MultiModalEmotionRecognizer, self).__init__()\n",
    "        \n",
    "        self.fer_model = fer_model\n",
    "        self.ter_model = ter_model\n",
    "        self.ter_tokenizer = ter_tokenizer\n",
    "        \n",
    "        # Freeze pre-trained models (optional)\n",
    "        self.freeze_pretrained = True\n",
    "        if self.freeze_pretrained:\n",
    "            for param in self.fer_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.ter_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Late fusion module\n",
    "        self.fusion_model = LateFusionModel(\n",
    "            num_classes=NUM_CLASSES,\n",
    "            fusion_type=fusion_type\n",
    "        )\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for multimodal emotion recognition\n",
    "        \n",
    "        Args:\n",
    "            images: Image tensor [batch_size, 1, 48, 48]\n",
    "            input_ids: Tokenized text [batch_size, seq_len]\n",
    "            attention_mask: Attention mask [batch_size, seq_len]\n",
    "            \n",
    "        Returns:\n",
    "            fused_logits: Final emotion predictions\n",
    "            fer_logits: FER model predictions\n",
    "            ter_logits: TER model predictions\n",
    "            fusion_weights: Fusion weights (if applicable)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get FER predictions\n",
    "        fer_logits = self.fer_model(images)\n",
    "        \n",
    "        # Get TER predictions\n",
    "        ter_outputs = self.ter_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        ter_logits = ter_outputs.logits\n",
    "        \n",
    "        # Fuse predictions\n",
    "        fused_logits, fusion_weights = self.fusion_model(fer_logits, ter_logits)\n",
    "        \n",
    "        return fused_logits, fer_logits, ter_logits, fusion_weights\n",
    "\n",
    "# Create fusion models with different strategies\n",
    "print(\"Creating late fusion models...\")\n",
    "\n",
    "fusion_strategies = ['simple', 'weighted', 'mlp', 'attention', 'bilinear']\n",
    "fusion_models = {}\n",
    "\n",
    "for strategy in fusion_strategies:\n",
    "    try:\n",
    "        model = MultiModalEmotionRecognizer(\n",
    "            fer_model=fer_model,\n",
    "            ter_model=ter_model,\n",
    "            ter_tokenizer=ter_tokenizer,\n",
    "            fusion_type=strategy\n",
    "        )\n",
    "        model.to(device)\n",
    "        fusion_models[strategy] = model\n",
    "        \n",
    "        # Count trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        print(f\"‚úÖ {strategy.upper()} fusion model created\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create {strategy} fusion model: {e}\")\n",
    "\n",
    "# Select default model for training (weighted fusion)\n",
    "default_fusion_model = fusion_models.get('weighted', list(fusion_models.values())[0])\n",
    "print(f\"\\nüéØ Using {default_fusion_model.fusion_model.fusion_type} fusion as default model\")\n",
    "\n",
    "# Test multimodal model\n",
    "print(\"\\nüß™ Testing multimodal model...\")\n",
    "try:\n",
    "    # Create dummy inputs\n",
    "    dummy_images = torch.randn(2, 1, 48, 48).to(device)\n",
    "    dummy_texts = [\"I am feeling happy!\", \"This is terrible and scary.\"]\n",
    "    \n",
    "    # Tokenize text\n",
    "    encoding = ter_tokenizer(\n",
    "        dummy_texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=ter_config.get('max_length', 128),\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    dummy_input_ids = encoding['input_ids'].to(device)\n",
    "    dummy_attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Test forward pass\n",
    "    with torch.no_grad():\n",
    "        fused_logits, fer_logits, ter_logits, fusion_weights = default_fusion_model(\n",
    "            dummy_images, dummy_input_ids, dummy_attention_mask\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Multimodal test successful!\")\n",
    "    print(f\"   Input shapes: Images {dummy_images.shape}, Text {dummy_input_ids.shape}\")\n",
    "    print(f\"   FER output: {fer_logits.shape}\")\n",
    "    print(f\"   TER output: {ter_logits.shape}\")\n",
    "    print(f\"   Fused output: {fused_logits.shape}\")\n",
    "    if fusion_weights is not None:\n",
    "        print(f\"   Fusion weights: {fusion_weights}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Multimodal test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d0649a",
   "metadata": {},
   "source": [
    "## 6. Create Combined Dataset Class\n",
    "\n",
    "Implement a custom PyTorch dataset class for multimodal emotion recognition training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a6bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalEmotionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for multimodal emotion recognition\n",
    "    Combines images and text for the same emotion labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_texts_pairs, labels, tokenizer, image_transform=None, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_texts_pairs: List of tuples (image_path_or_array, text)\n",
    "            labels: List of emotion labels (numeric)\n",
    "            tokenizer: Text tokenizer\n",
    "            image_transform: Image preprocessing transforms\n",
    "            max_length: Maximum text sequence length\n",
    "        \"\"\"\n",
    "        self.data = image_texts_pairs\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_transform = image_transform\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        assert len(self.data) == len(self.labels), \"Data and labels must have same length\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_data, text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Process image\n",
    "        if isinstance(image_data, str):\n",
    "            # Load image from path\n",
    "            image = cv2.imread(image_data, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is None:\n",
    "                # Create dummy image if loading fails\n",
    "                image = np.random.randint(0, 255, (48, 48), dtype=np.uint8)\n",
    "        else:\n",
    "            # Use provided image array\n",
    "            image = image_data\n",
    "        \n",
    "        # Ensure image is 48x48\n",
    "        if image.shape != (48, 48):\n",
    "            image = cv2.resize(image, (48, 48))\n",
    "        \n",
    "        # Convert to PIL and apply transforms\n",
    "        image = Image.fromarray(image)\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        else:\n",
    "            # Default transform\n",
    "            image = transforms.ToTensor()(image)\n",
    "            image = transforms.Normalize(mean=[0.5], std=[0.5])(image)\n",
    "        \n",
    "        # Process text\n",
    "        cleaned_text = clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            cleaned_text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'text': text  # Keep original text for reference\n",
    "        }\n",
    "\n",
    "# Create synthetic multimodal dataset for demonstration\n",
    "def create_synthetic_multimodal_dataset(num_samples=1000):\n",
    "    \"\"\"\n",
    "    Create a synthetic dataset with paired images and text for each emotion\n",
    "    \"\"\"\n",
    "    print(\"Creating synthetic multimodal dataset...\")\n",
    "    \n",
    "    # Emotion-specific text templates\n",
    "    emotion_texts = {\n",
    "        0: [  # angry\n",
    "            \"I am so furious about this situation\",\n",
    "            \"This makes me incredibly angry and upset\",\n",
    "            \"I can't believe how infuriating this is\",\n",
    "            \"I'm absolutely livid right now\",\n",
    "            \"This is making me so mad and frustrated\"\n",
    "        ],\n",
    "        1: [  # disgust\n",
    "            \"This is absolutely disgusting and revolting\",\n",
    "            \"I feel sick looking at this gross thing\",\n",
    "            \"How repugnant and vile can something be\",\n",
    "            \"This disgusting sight is making me nauseous\",\n",
    "            \"I'm completely repulsed by this awful thing\"\n",
    "        ],\n",
    "        2: [  # fear\n",
    "            \"I'm terrified and scared of what might happen\",\n",
    "            \"This frightening situation fills me with dread\",\n",
    "            \"I'm so anxious and worried about this\",\n",
    "            \"This scary scenario is making me panic\",\n",
    "            \"I feel overwhelming fear and terror\"\n",
    "        ],\n",
    "        3: [  # happy\n",
    "            \"I am so happy and joyful today\",\n",
    "            \"This wonderful news fills me with happiness\",\n",
    "            \"I'm feeling absolutely delighted and cheerful\",\n",
    "            \"This amazing thing brings me so much joy\",\n",
    "            \"I'm in such a great mood and feeling fantastic\"\n",
    "        ],\n",
    "        4: [  # sad\n",
    "            \"I feel so sad and heartbroken about this\",\n",
    "            \"This tragic news is making me deeply sorrowful\",\n",
    "            \"I'm feeling incredibly melancholy and blue\",\n",
    "            \"This depressing situation brings tears to my eyes\",\n",
    "            \"I'm overwhelmed with sadness and grief\"\n",
    "        ],\n",
    "        5: [  # surprise\n",
    "            \"Wow, I never expected this amazing surprise\",\n",
    "            \"I'm absolutely shocked and astonished by this\",\n",
    "            \"What an incredible and unexpected revelation\",\n",
    "            \"I'm completely stunned by this surprising news\",\n",
    "            \"This unexpected turn of events is so surprising\"\n",
    "        ],\n",
    "        6: [  # neutral\n",
    "            \"This is a normal part of the daily routine\",\n",
    "            \"The situation is neither good nor bad\",\n",
    "            \"I have mixed feelings about this outcome\",\n",
    "            \"This is just a regular occurrence nothing special\",\n",
    "            \"I feel indifferent about this particular matter\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    image_text_pairs = []\n",
    "    labels = []\n",
    "    \n",
    "    # Generate samples for each emotion\n",
    "    samples_per_emotion = num_samples // NUM_CLASSES\n",
    "    \n",
    "    for emotion_id in range(NUM_CLASSES):\n",
    "        for i in range(samples_per_emotion):\n",
    "            # Create synthetic face image with emotion-specific features\n",
    "            image = create_synthetic_face_image(emotion_id)\n",
    "            \n",
    "            # Select random text for this emotion\n",
    "            text = random.choice(emotion_texts[emotion_id])\n",
    "            \n",
    "            # Add some variation to the text\n",
    "            if random.random() < 0.3:\n",
    "                text = f\"I really think that {text.lower()}\"\n",
    "            elif random.random() < 0.3:\n",
    "                text = f\"{text} It's quite overwhelming.\"\n",
    "            \n",
    "            image_text_pairs.append((image, text))\n",
    "            labels.append(emotion_id)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(image_text_pairs)} multimodal samples\")\n",
    "    print(f\"   Samples per emotion: {samples_per_emotion}\")\n",
    "    \n",
    "    return image_text_pairs, labels\n",
    "\n",
    "def create_synthetic_face_image(emotion_id, size=(48, 48)):\n",
    "    \"\"\"\n",
    "    Create a synthetic face-like image with emotion-specific features\n",
    "    \"\"\"\n",
    "    np.random.seed(emotion_id * 100 + random.randint(0, 99))\n",
    "    \n",
    "    # Create base face\n",
    "    image = np.random.randint(80, 180, size, dtype=np.uint8)\n",
    "    \n",
    "    # Add face features\n",
    "    center_x, center_y = size[1] // 2, size[0] // 2\n",
    "    \n",
    "    # Eyes\n",
    "    eye1_x, eye1_y = center_x - 8, center_y - 6\n",
    "    eye2_x, eye2_y = center_x + 8, center_y - 6\n",
    "    cv2.circle(image, (eye1_x, eye1_y), 3, 50, -1)\n",
    "    cv2.circle(image, (eye2_x, eye2_y), 3, 50, -1)\n",
    "    \n",
    "    # Nose\n",
    "    cv2.circle(image, (center_x, center_y + 2), 2, 100, -1)\n",
    "    \n",
    "    # Emotion-specific mouth\n",
    "    mouth_y = center_y + 10\n",
    "    \n",
    "    if emotion_id == 0:  # angry - frown\n",
    "        cv2.ellipse(image, (center_x, mouth_y + 3), (8, 4), 0, 180, 360, 60, -1)\n",
    "    elif emotion_id == 1:  # disgust - wavy mouth\n",
    "        cv2.line(image, (center_x - 6, mouth_y), (center_x + 6, mouth_y), 70, 2)\n",
    "    elif emotion_id == 2:  # fear - open mouth\n",
    "        cv2.ellipse(image, (center_x, mouth_y), (4, 6), 0, 0, 360, 40, -1)\n",
    "    elif emotion_id == 3:  # happy - smile\n",
    "        cv2.ellipse(image, (center_x, mouth_y - 2), (8, 4), 0, 0, 180, 70, -1)\n",
    "    elif emotion_id == 4:  # sad - frown\n",
    "        cv2.ellipse(image, (center_x, mouth_y + 4), (8, 4), 0, 180, 360, 60, -1)\n",
    "    elif emotion_id == 5:  # surprise - open mouth\n",
    "        cv2.ellipse(image, (center_x, mouth_y), (5, 8), 0, 0, 360, 30, -1)\n",
    "    else:  # neutral - straight line\n",
    "        cv2.line(image, (center_x - 6, mouth_y), (center_x + 6, mouth_y), 80, 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Create the synthetic dataset\n",
    "dataset_pairs, dataset_labels = create_synthetic_multimodal_dataset(num_samples=2000)\n",
    "\n",
    "# Create train/validation split\n",
    "train_size = int(0.8 * len(dataset_pairs))\n",
    "val_size = len(dataset_pairs) - train_size\n",
    "\n",
    "train_pairs = dataset_pairs[:train_size]\n",
    "train_labels = dataset_labels[:train_size]\n",
    "val_pairs = dataset_pairs[train_size:]\n",
    "val_labels = dataset_labels[train_size:]\n",
    "\n",
    "print(f\"\\nüìä Dataset splits:\")\n",
    "print(f\"   Training: {len(train_pairs)} samples\")\n",
    "print(f\"   Validation: {len(val_pairs)} samples\")\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = MultiModalEmotionDataset(\n",
    "    image_texts_pairs=train_pairs,\n",
    "    labels=train_labels,\n",
    "    tokenizer=ter_tokenizer,\n",
    "    image_transform=fer_transform,\n",
    "    max_length=ter_config.get('max_length', 128)\n",
    ")\n",
    "\n",
    "val_dataset = MultiModalEmotionDataset(\n",
    "    image_texts_pairs=val_pairs,\n",
    "    labels=val_labels,\n",
    "    tokenizer=ter_tokenizer,\n",
    "    image_transform=fer_transform,\n",
    "    max_length=ter_config.get('max_length', 128)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Multimodal datasets created successfully\")\n",
    "\n",
    "# Test dataset\n",
    "print(\"\\nüß™ Testing dataset...\")\n",
    "try:\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"‚úÖ Dataset test successful!\")\n",
    "    print(f\"   Image shape: {sample['image'].shape}\")\n",
    "    print(f\"   Input IDs shape: {sample['input_ids'].shape}\")\n",
    "    print(f\"   Attention mask shape: {sample['attention_mask'].shape}\")\n",
    "    print(f\"   Label: {sample['label']} ({EMOTION_NAMES[sample['label']]})\")\n",
    "    print(f\"   Text: '{sample['text']}'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataset test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b14920",
   "metadata": {},
   "source": [
    "## 7. Data Loading and Training Setup\n",
    "\n",
    "Now we'll create data loaders and set up the training configuration for our fusion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading configuration\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "NUM_WORKERS = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"üìä Data loaders created:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Training configuration\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "FUSION_STRATEGY = 'mlp'  # Options: 'weighted', 'mlp', 'attention', 'bilinear', 'simple'\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training configuration:\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Fusion strategy: {FUSION_STRATEGY}\")\n",
    "\n",
    "# Initialize fusion model\n",
    "fusion_model = LateFusionModel(\n",
    "    fer_output_size=7,\n",
    "    ter_output_size=7,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    fusion_strategy=FUSION_STRATEGY,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ Fusion model initialized with {FUSION_STRATEGY} strategy\")\n",
    "\n",
    "# Initialize multimodal recognizer\n",
    "multimodal_recognizer = MultiModalEmotionRecognizer(\n",
    "    fer_model=fer_model,\n",
    "    ter_model=ter_model,\n",
    "    fusion_model=fusion_model,\n",
    "    tokenizer=ter_tokenizer,\n",
    "    label_encoder=ter_label_encoder,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Multimodal recognizer initialized\")\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(f\"‚úÖ Optimizer, loss function, and scheduler initialized\")\n",
    "\n",
    "# Test data loader\n",
    "print(\"\\nüß™ Testing data loader...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"‚úÖ Data loader test successful!\")\n",
    "    print(f\"   Batch image shape: {sample_batch['image'].shape}\")\n",
    "    print(f\"   Batch input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "    print(f\"   Batch attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "    print(f\"   Batch labels shape: {sample_batch['label'].shape}\")\n",
    "    print(f\"   Sample emotions: {[EMOTION_NAMES[label.item()] for label in sample_batch['label'][:5]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data loader test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e12f430",
   "metadata": {},
   "source": [
    "## 8. Training Loop\n",
    "\n",
    "Let's train our fusion model to learn optimal weights for combining FER and TER predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d1327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading configuration\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "NUM_WORKERS = 2 if device.type == 'cuda' else 0\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == 'cuda' else False\n",
    ")\n",
    "\n",
    "print(f\"üìä Data loaders created:\")\n",
    "print(f\"   Training batches: {len(train_loader)}\")\n",
    "print(f\"   Validation batches: {len(val_loader)}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Training configuration\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "FUSION_STRATEGY = 'mlp'  # Options: 'weighted', 'mlp', 'attention', 'bilinear', 'simple'\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Training configuration:\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"   Fusion strategy: {FUSION_STRATEGY}\")\n",
    "\n",
    "# Initialize multimodal recognizer (includes fusion model internally)\n",
    "multimodal_recognizer = MultiModalEmotionRecognizer(\n",
    "    fer_model=fer_model,\n",
    "    ter_model=ter_model,\n",
    "    ter_tokenizer=ter_tokenizer,\n",
    "    fusion_type=FUSION_STRATEGY\n",
    ").to(device)\n",
    "\n",
    "print(f\"‚úÖ Multimodal recognizer initialized with {FUSION_STRATEGY} fusion strategy\")\n",
    "\n",
    "# Optimizer and loss function (train only the fusion model parameters)\n",
    "optimizer = torch.optim.Adam(multimodal_recognizer.fusion_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(f\"‚úÖ Optimizer, loss function, and scheduler initialized\")\n",
    "\n",
    "# Test data loader\n",
    "print(\"\\nüß™ Testing data loader...\")\n",
    "try:\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    print(f\"‚úÖ Data loader test successful!\")\n",
    "    print(f\"   Batch image shape: {sample_batch['image'].shape}\")\n",
    "    print(f\"   Batch input_ids shape: {sample_batch['input_ids'].shape}\")\n",
    "    print(f\"   Batch attention_mask shape: {sample_batch['attention_mask'].shape}\")\n",
    "    print(f\"   Batch labels shape: {sample_batch['label'].shape}\")\n",
    "    print(f\"   Sample emotions: {[EMOTION_NAMES[label.item()] for label in sample_batch['label'][:5]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data loader test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52325b",
   "metadata": {},
   "source": [
    "## 9. Visualization and Evaluation\n",
    "\n",
    "Let's visualize the training progress and evaluate our fusion model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    epochs = range(1, len(history['train_losses']) + 1)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(epochs, history['train_losses'], 'bo-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_losses'], 'ro-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(epochs, history['train_accuracies'], 'bo-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, history['val_accuracies'], 'ro-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"üìà Final Training Metrics:\")\n",
    "    print(f\"   Training Loss: {history['train_losses'][-1]:.4f}\")\n",
    "    print(f\"   Training Accuracy: {history['train_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"   Validation Loss: {history['val_losses'][-1]:.4f}\")\n",
    "    print(f\"   Validation Accuracy: {history['val_accuracies'][-1]:.2f}%\")\n",
    "\n",
    "plot_training_history(training_history)\n",
    "\n",
    "# Comprehensive evaluation function\n",
    "def evaluate_fusion_model(multimodal_recognizer, val_loader, device):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of the fusion model\n",
    "    \"\"\"\n",
    "    multimodal_recognizer.fusion_model.eval()\n",
    "    \n",
    "    all_fusion_preds = []\n",
    "    all_fer_preds = []\n",
    "    all_ter_preds = []\n",
    "    all_labels = []\n",
    "    all_fusion_probs = []\n",
    "    all_fer_probs = []\n",
    "    all_ter_probs = []\n",
    "    \n",
    "    print(\"üîç Evaluating fusion model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            # Move batch to device\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Get individual model predictions\n",
    "            fer_outputs = multimodal_recognizer.fer_model(images)\n",
    "            fer_probs = F.softmax(fer_outputs, dim=1)\n",
    "            \n",
    "            ter_outputs = multimodal_recognizer.ter_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            ).logits\n",
    "            ter_probs = F.softmax(ter_outputs, dim=1)\n",
    "            \n",
    "            # Get fusion predictions\n",
    "            fusion_outputs = multimodal_recognizer.fusion_model(fer_probs, ter_probs)\n",
    "            fusion_probs = F.softmax(fusion_outputs, dim=1)\n",
    "            \n",
    "            # Store predictions\n",
    "            _, fer_pred = torch.max(fer_probs, 1)\n",
    "            _, ter_pred = torch.max(ter_probs, 1)\n",
    "            _, fusion_pred = torch.max(fusion_probs, 1)\n",
    "            \n",
    "            all_fer_preds.extend(fer_pred.cpu().numpy())\n",
    "            all_ter_preds.extend(ter_pred.cpu().numpy())\n",
    "            all_fusion_preds.extend(fusion_pred.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            all_fer_probs.extend(fer_probs.cpu().numpy())\n",
    "            all_ter_probs.extend(ter_probs.cpu().numpy())\n",
    "            all_fusion_probs.extend(fusion_probs.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    fer_accuracy = accuracy_score(all_labels, all_fer_preds) * 100\n",
    "    ter_accuracy = accuracy_score(all_labels, all_ter_preds) * 100\n",
    "    fusion_accuracy = accuracy_score(all_labels, all_fusion_preds) * 100\n",
    "    \n",
    "    print(f\"\\nüìä Model Comparison:\")\n",
    "    print(f\"   FER Only Accuracy: {fer_accuracy:.2f}%\")\n",
    "    print(f\"   TER Only Accuracy: {ter_accuracy:.2f}%\")\n",
    "    print(f\"   Fusion Accuracy: {fusion_accuracy:.2f}%\")\n",
    "    print(f\"   Improvement over FER: {fusion_accuracy - fer_accuracy:+.2f}%\")\n",
    "    print(f\"   Improvement over TER: {fusion_accuracy - ter_accuracy:+.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'fer_preds': all_fer_preds,\n",
    "        'ter_preds': all_ter_preds,\n",
    "        'fusion_preds': all_fusion_preds,\n",
    "        'labels': all_labels,\n",
    "        'fer_probs': all_fer_probs,\n",
    "        'ter_probs': all_ter_probs,\n",
    "        'fusion_probs': all_fusion_probs,\n",
    "        'fer_accuracy': fer_accuracy,\n",
    "        'ter_accuracy': ter_accuracy,\n",
    "        'fusion_accuracy': fusion_accuracy\n",
    "    }\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "eval_results = evaluate_fusion_model(multimodal_recognizer, val_loader, device)\n",
    "\n",
    "# Plot confusion matrices\n",
    "def plot_confusion_matrices(eval_results):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for all three models\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    models = [\n",
    "        ('FER Only', eval_results['fer_preds']),\n",
    "        ('TER Only', eval_results['ter_preds']),\n",
    "        ('Fusion', eval_results['fusion_preds'])\n",
    "    ]\n",
    "    \n",
    "    for idx, (model_name, preds) in enumerate(models):\n",
    "        cm = confusion_matrix(eval_results['labels'], preds)\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # Plot\n",
    "        sns.heatmap(\n",
    "            cm_normalized,\n",
    "            annot=True,\n",
    "            fmt='.2f',\n",
    "            cmap='Blues',\n",
    "            xticklabels=EMOTION_NAMES,\n",
    "            yticklabels=EMOTION_NAMES,\n",
    "            ax=axes[idx]\n",
    "        )\n",
    "        \n",
    "        axes[idx].set_title(f'{model_name} Confusion Matrix', fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "        \n",
    "        # Rotate labels for better readability\n",
    "        axes[idx].tick_params(axis='x', rotation=45)\n",
    "        axes[idx].tick_params(axis='y', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrices(eval_results)\n",
    "\n",
    "# Detailed classification reports\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(f\"\\nüìã Detailed Classification Reports:\")\n",
    "print(f\"\\n{'='*20} FER Only {'='*20}\")\n",
    "print(classification_report(\n",
    "    eval_results['labels'], \n",
    "    eval_results['fer_preds'], \n",
    "    target_names=EMOTION_NAMES,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(f\"\\n{'='*20} TER Only {'='*20}\")\n",
    "print(classification_report(\n",
    "    eval_results['labels'], \n",
    "    eval_results['ter_preds'], \n",
    "    target_names=EMOTION_NAMES,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "print(f\"\\n{'='*20} Fusion Model {'='*20}\")\n",
    "print(classification_report(\n",
    "    eval_results['labels'], \n",
    "    eval_results['fusion_preds'], \n",
    "    target_names=EMOTION_NAMES,\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823c8282",
   "metadata": {},
   "source": [
    "## 10. Sample Predictions\n",
    "\n",
    "Let's see how our fusion model performs on some sample inputs and compare with individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d452f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on sample data\n",
    "def predict_sample_multimodal(\n",
    "    multimodal_recognizer, \n",
    "    image, \n",
    "    text, \n",
    "    device,\n",
    "    show_details=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Make prediction on a single image-text pair and show detailed results\n",
    "    \"\"\"\n",
    "    multimodal_recognizer.fusion_model.eval()\n",
    "    \n",
    "    # Preprocess image\n",
    "    if isinstance(image, np.ndarray):\n",
    "        if image.shape != (48, 48):\n",
    "            image = cv2.resize(image, (48, 48))\n",
    "        pil_image = Image.fromarray(image)\n",
    "    else:\n",
    "        pil_image = image\n",
    "    \n",
    "    # Apply image transforms\n",
    "    image_tensor = fer_transform(pil_image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Preprocess text\n",
    "    cleaned_text = clean_text(text)\n",
    "    encoding = ter_tokenizer(\n",
    "        cleaned_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=ter_config.get('max_length', 128),\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Individual model predictions\n",
    "        fer_outputs = multimodal_recognizer.fer_model(image_tensor)\n",
    "        fer_probs = F.softmax(fer_outputs, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        ter_outputs = multimodal_recognizer.ter_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).logits\n",
    "        ter_probs = F.softmax(ter_outputs, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Fusion prediction\n",
    "        fer_probs_tensor = torch.tensor(fer_probs).unsqueeze(0).to(device)\n",
    "        ter_probs_tensor = torch.tensor(ter_probs).unsqueeze(0).to(device)\n",
    "        fusion_outputs = multimodal_recognizer.fusion_model(fer_probs_tensor, ter_probs_tensor)\n",
    "        fusion_probs = F.softmax(fusion_outputs, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Get predictions\n",
    "    fer_pred = np.argmax(fer_probs)\n",
    "    ter_pred = np.argmax(ter_probs)\n",
    "    fusion_pred = np.argmax(fusion_probs)\n",
    "    \n",
    "    results = {\n",
    "        'fer_prediction': fer_pred,\n",
    "        'ter_prediction': ter_pred,\n",
    "        'fusion_prediction': fusion_pred,\n",
    "        'fer_probs': fer_probs,\n",
    "        'ter_probs': ter_probs,\n",
    "        'fusion_probs': fusion_probs,\n",
    "        'fer_emotion': EMOTION_NAMES[fer_pred],\n",
    "        'ter_emotion': EMOTION_NAMES[ter_pred],\n",
    "        'fusion_emotion': EMOTION_NAMES[fusion_pred],\n",
    "        'text': text,\n",
    "        'cleaned_text': cleaned_text\n",
    "    }\n",
    "    \n",
    "    if show_details:\n",
    "        print(f\"üîç Multimodal Prediction Results:\")\n",
    "        print(f\"   Text: '{text}'\")\n",
    "        print(f\"   FER Prediction: {results['fer_emotion']} (confidence: {fer_probs[fer_pred]:.3f})\")\n",
    "        print(f\"   TER Prediction: {results['ter_emotion']} (confidence: {ter_probs[ter_pred]:.3f})\")\n",
    "        print(f\"   Fusion Prediction: {results['fusion_emotion']} (confidence: {fusion_probs[fusion_pred]:.3f})\")\n",
    "        \n",
    "        # Show if fusion differs from individual models\n",
    "        if fusion_pred != fer_pred or fusion_pred != ter_pred:\n",
    "            print(f\"   ‚ö†Ô∏è  Fusion disagrees with individual models!\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with some validation samples\n",
    "print(\"üß™ Testing with validation samples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Get a few samples from validation set\n",
    "sample_indices = [0, 10, 50, 100, 200]\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    if idx < len(val_dataset):\n",
    "        sample = val_dataset[idx]\n",
    "        \n",
    "        # Convert image tensor back to PIL for display\n",
    "        image_np = sample['image'].cpu().numpy()\n",
    "        if len(image_np.shape) == 3:\n",
    "            image_np = image_np[0]  # Remove channel dimension if present\n",
    "        image_np = ((image_np + 1) * 127.5).astype(np.uint8)  # Denormalize\n",
    "        \n",
    "        print(f\"\\nüì∑ Sample {i+1}:\")\n",
    "        print(f\"   True Label: {EMOTION_NAMES[sample['label']]}\")\n",
    "        \n",
    "        results = predict_sample_multimodal(\n",
    "            multimodal_recognizer,\n",
    "            image_np,\n",
    "            sample['text'],\n",
    "            device,\n",
    "            show_details=True\n",
    "        )\n",
    "        \n",
    "        # Check if prediction is correct\n",
    "        correct_fer = results['fer_prediction'] == sample['label']\n",
    "        correct_ter = results['ter_prediction'] == sample['label']\n",
    "        correct_fusion = results['fusion_prediction'] == sample['label']\n",
    "        \n",
    "        print(f\"   ‚úÖ Correct predictions: FER={correct_fer}, TER={correct_ter}, Fusion={correct_fusion}\")\n",
    "\n",
    "# Create custom test examples\n",
    "print(f\"\\n\\nüé≠ Testing with custom examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "custom_examples = [\n",
    "    {\n",
    "        'emotion_id': 3,  # happy\n",
    "        'text': \"I'm so excited about this wonderful news! This makes me incredibly happy and joyful!\",\n",
    "        'description': \"Happy emotion - positive text\"\n",
    "    },\n",
    "    {\n",
    "        'emotion_id': 0,  # angry\n",
    "        'text': \"This is absolutely infuriating and makes me so angry! I can't stand this situation!\",\n",
    "        'description': \"Angry emotion - negative text\"\n",
    "    },\n",
    "    {\n",
    "        'emotion_id': 4,  # sad\n",
    "        'text': \"I feel so heartbroken and sad about what happened. This is truly devastating.\",\n",
    "        'description': \"Sad emotion - melancholic text\"\n",
    "    },\n",
    "    {\n",
    "        'emotion_id': 5,  # surprise\n",
    "        'text': \"Wow! I never expected this amazing surprise! I'm completely shocked and astonished!\",\n",
    "        'description': \"Surprise emotion - unexpected text\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, example in enumerate(custom_examples):\n",
    "    print(f\"\\nüé™ Custom Example {i+1}: {example['description']}\")\n",
    "    \n",
    "    # Create synthetic image for this emotion\n",
    "    synthetic_image = create_synthetic_face_image(example['emotion_id'])\n",
    "    \n",
    "    results = predict_sample_multimodal(\n",
    "        multimodal_recognizer,\n",
    "        synthetic_image,\n",
    "        example['text'],\n",
    "        device,\n",
    "        show_details=True\n",
    "    )\n",
    "    \n",
    "    # Check consistency\n",
    "    expected_emotion = EMOTION_NAMES[example['emotion_id']]\n",
    "    print(f\"   Expected: {expected_emotion}\")\n",
    "    \n",
    "    if results['fusion_emotion'] == expected_emotion:\n",
    "        print(f\"   ‚úÖ Fusion prediction matches expected emotion!\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Fusion prediction differs from expected emotion\")\n",
    "\n",
    "# Visualize probability distributions for a sample\n",
    "def plot_prediction_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot probability distributions for FER, TER, and Fusion models\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    \n",
    "    x = np.arange(len(EMOTION_NAMES))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax.bar(x - width, results['fer_probs'], width, label='FER Only', alpha=0.8)\n",
    "    bars2 = ax.bar(x, results['ter_probs'], width, label='TER Only', alpha=0.8)\n",
    "    bars3 = ax.bar(x + width, results['fusion_probs'], width, label='Fusion', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Emotions')\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title(f'Prediction Comparison\\\\nText: \"{results[\"text\"][:50]}...\"')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(EMOTION_NAMES, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight the predictions\n",
    "    fer_pred_idx = np.argmax(results['fer_probs'])\n",
    "    ter_pred_idx = np.argmax(results['ter_probs'])\n",
    "    fusion_pred_idx = np.argmax(results['fusion_probs'])\n",
    "    \n",
    "    bars1[fer_pred_idx].set_edgecolor('red')\n",
    "    bars1[fer_pred_idx].set_linewidth(3)\n",
    "    bars2[ter_pred_idx].set_edgecolor('red')\n",
    "    bars2[ter_pred_idx].set_linewidth(3)\n",
    "    bars3[fusion_pred_idx].set_edgecolor('red')\n",
    "    bars3[fusion_pred_idx].set_linewidth(3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for one of the custom examples\n",
    "if len(custom_examples) > 0:\n",
    "    print(f\"\\\\nüìä Probability distribution visualization:\")\n",
    "    example = custom_examples[0]\n",
    "    synthetic_image = create_synthetic_face_image(example['emotion_id'])\n",
    "    results = predict_sample_multimodal(\n",
    "        multimodal_recognizer,\n",
    "        synthetic_image,\n",
    "        example['text'],\n",
    "        device,\n",
    "        show_details=False\n",
    "    )\n",
    "    plot_prediction_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2993f46",
   "metadata": {},
   "source": [
    "## 11. Save Trained Fusion Model\n",
    "\n",
    "Let's save our trained fusion model to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d7ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained fusion model and related components\n",
    "def save_fusion_model(\n",
    "    fusion_model, \n",
    "    multimodal_recognizer, \n",
    "    training_history, \n",
    "    eval_results,\n",
    "    fusion_strategy,\n",
    "    save_dir='/content/drive/MyDrive/emotion_models/'\n",
    "):\n",
    "    \"\"\"\n",
    "    Save the trained fusion model and all related components\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Create timestamp for unique naming\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_name = f\"multimodal_fusion_{fusion_strategy}_{timestamp}\"\n",
    "    \n",
    "    # Create save directory\n",
    "    full_save_dir = os.path.join(save_dir, model_name)\n",
    "    os.makedirs(full_save_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üíæ Saving fusion model to: {full_save_dir}\")\n",
    "    \n",
    "    # 1. Save fusion model state dict\n",
    "    fusion_model_path = os.path.join(full_save_dir, 'fusion_model.pth')\n",
    "    torch.save(fusion_model.state_dict(), fusion_model_path)\n",
    "    print(f\"   ‚úÖ Fusion model saved: fusion_model.pth\")\n",
    "    \n",
    "    # 2. Save complete multimodal recognizer (for easy loading)\n",
    "    recognizer_path = os.path.join(full_save_dir, 'multimodal_recognizer.pth')\n",
    "    torch.save({\n",
    "        'fusion_model_state_dict': fusion_model.state_dict(),\n",
    "        'fusion_strategy': fusion_strategy,\n",
    "        'fer_output_size': 7,\n",
    "        'ter_output_size': 7,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'emotion_names': EMOTION_NAMES\n",
    "    }, recognizer_path)\n",
    "    print(f\"   ‚úÖ Complete recognizer saved: multimodal_recognizer.pth\")\n",
    "    \n",
    "    # 3. Save training configuration\n",
    "    config = {\n",
    "        'fusion_strategy': fusion_strategy,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'num_classes': NUM_CLASSES,\n",
    "        'emotion_names': EMOTION_NAMES,\n",
    "        'fer_output_size': 7,\n",
    "        'ter_output_size': 7,\n",
    "        'max_length': ter_config.get('max_length', 128),\n",
    "        'timestamp': timestamp,\n",
    "        'final_metrics': {\n",
    "            'fer_accuracy': eval_results['fer_accuracy'],\n",
    "            'ter_accuracy': eval_results['ter_accuracy'],\n",
    "            'fusion_accuracy': eval_results['fusion_accuracy'],\n",
    "            'improvement_over_fer': eval_results['fusion_accuracy'] - eval_results['fer_accuracy'],\n",
    "            'improvement_over_ter': eval_results['fusion_accuracy'] - eval_results['ter_accuracy']\n",
    "        }\n",
    "    }\\n    \\n    config_path = os.path.join(full_save_dir, 'config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"   ‚úÖ Configuration saved: config.json\")\n",
    "    \n",
    "    # 4. Save training history\n",
    "    history_path = os.path.join(full_save_dir, 'training_history.pkl')\n",
    "    with open(history_path, 'wb') as f:\n",
    "        pickle.dump(training_history, f)\n",
    "    print(f\"   ‚úÖ Training history saved: training_history.pkl\")\n",
    "    \n",
    "    # 5. Save evaluation results\n",
    "    eval_path = os.path.join(full_save_dir, 'evaluation_results.pkl')\n",
    "    with open(eval_path, 'wb') as f:\n",
    "        pickle.dump(eval_results, f)\n",
    "    print(f\"   ‚úÖ Evaluation results saved: evaluation_results.pkl\")\n",
    "    \n",
    "    # 6. Save fusion model architecture details\n",
    "    if hasattr(fusion_model, 'get_fusion_weights'):\n",
    "        try:\n",
    "            weights = fusion_model.get_fusion_weights()\n",
    "            weights_path = os.path.join(full_save_dir, 'fusion_weights.json')\n",
    "            with open(weights_path, 'w') as f:\n",
    "                json.dump(weights, f, indent=2)\n",
    "            print(f\"   ‚úÖ Fusion weights saved: fusion_weights.json\")\n",
    "        except:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not save fusion weights\")\n",
    "    \n",
    "    # 7. Create a README file with model information\n",
    "    readme_content = f\"\"\"# Multimodal Emotion Recognition Fusion Model\n",
    "\n",
    "## Model Information\n",
    "- **Model Name**: {model_name}\n",
    "- **Fusion Strategy**: {fusion_strategy}\n",
    "- **Training Date**: {timestamp}\n",
    "- **Number of Classes**: {NUM_CLASSES}\n",
    "- **Emotion Classes**: {', '.join(EMOTION_NAMES)}\n",
    "\n",
    "## Performance Metrics\n",
    "- **FER Only Accuracy**: {eval_results['fer_accuracy']:.2f}%\n",
    "- **TER Only Accuracy**: {eval_results['ter_accuracy']:.2f}%\n",
    "- **Fusion Accuracy**: {eval_results['fusion_accuracy']:.2f}%\n",
    "- **Improvement over FER**: {eval_results['fusion_accuracy'] - eval_results['fer_accuracy']:+.2f}%\n",
    "- **Improvement over TER**: {eval_results['fusion_accuracy'] - eval_results['ter_accuracy']:+.2f}%\n",
    "\n",
    "## Training Configuration\n",
    "- **Learning Rate**: {LEARNING_RATE}\n",
    "- **Epochs**: {NUM_EPOCHS}\n",
    "- **Batch Size**: {BATCH_SIZE}\n",
    "- **Final Training Accuracy**: {training_history['train_accuracies'][-1]:.2f}%\n",
    "- **Final Validation Accuracy**: {training_history['val_accuracies'][-1]:.2f}%\n",
    "\n",
    "## Files Included\n",
    "- `fusion_model.pth`: Fusion model state dict\n",
    "- `multimodal_recognizer.pth`: Complete model for easy loading\n",
    "- `config.json`: Training and model configuration\n",
    "- `training_history.pkl`: Training loss and accuracy history\n",
    "- `evaluation_results.pkl`: Detailed evaluation results\n",
    "- `fusion_weights.json`: Learned fusion weights (if available)\n",
    "- `README.md`: This file\n",
    "\n",
    "## Usage\n",
    "To load this model:\n",
    "\n",
    "```python\n",
    "# Load the complete recognizer\n",
    "checkpoint = torch.load('multimodal_recognizer.pth')\n",
    "fusion_model = LateFusionModel(\n",
    "    fer_output_size=checkpoint['fer_output_size'],\n",
    "    ter_output_size=checkpoint['ter_output_size'],\n",
    "    num_classes=checkpoint['num_classes'],\n",
    "    fusion_strategy=checkpoint['fusion_strategy'],\n",
    "    device=device\n",
    ")\n",
    "fusion_model.load_state_dict(checkpoint['fusion_model_state_dict'])\n",
    "```\n",
    "\n",
    "Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\"\"\"\n",
    "    \n",
    "    readme_path = os.path.join(full_save_dir, 'README.md')\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"   ‚úÖ README created: README.md\")\n",
    "    \n",
    "    print(f\"\\\\nüéâ Model successfully saved to: {full_save_dir}\")\n",
    "    print(f\"   Total files: {len(os.listdir(full_save_dir))}\")\n",
    "    \n",
    "    return full_save_dir, model_name\n",
    "\n",
    "# Save the trained model\n",
    "try:\n",
    "    save_path, model_name = save_fusion_model(\n",
    "        fusion_model=fusion_model,\n",
    "        multimodal_recognizer=multimodal_recognizer,\n",
    "        training_history=training_history,\n",
    "        eval_results=eval_results,\n",
    "        fusion_strategy=FUSION_STRATEGY\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\nüìã Model Details:\")\n",
    "    print(f\"   Name: {model_name}\")\n",
    "    print(f\"   Strategy: {FUSION_STRATEGY}\")\n",
    "    print(f\"   Final Validation Accuracy: {training_history['val_accuracies'][-1]:.2f}%\")\n",
    "    print(f\"   Fusion Improvement: {eval_results['fusion_accuracy'] - max(eval_results['fer_accuracy'], eval_results['ter_accuracy']):+.2f}%\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "    print(\"   Attempting to save to local directory...\")\n",
    "    \n",
    "    # Fallback: save to local directory\n",
    "    try:\n",
    "        save_path, model_name = save_fusion_model(\n",
    "            fusion_model=fusion_model,\n",
    "            multimodal_recognizer=multimodal_recognizer,\n",
    "            training_history=training_history,\n",
    "            eval_results=eval_results,\n",
    "            fusion_strategy=FUSION_STRATEGY,\n",
    "            save_dir='./saved_models/'\n",
    "        )\n",
    "        print(f\"‚úÖ Model saved locally to: {save_path}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Failed to save model: {e2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2089d2",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "\n",
    "### üéØ Summary\n",
    "\n",
    "We have successfully created a **multimodal late fusion emotion recognition system** that combines:\n",
    "\n",
    "1. **FER Model**: CNN-based facial expression recognition trained on FER2013\n",
    "2. **TER Model**: DistilBERT-based textual emotion recognition  \n",
    "3. **Fusion Model**: Late fusion with multiple strategies (weighted, MLP, attention, bilinear)\n",
    "\n",
    "### üìä Key Achievements\n",
    "\n",
    "- ‚úÖ **Modular Architecture**: Easy to swap individual models or fusion strategies\n",
    "- ‚úÖ **Robust Loading**: Handles missing models with fallback options\n",
    "- ‚úÖ **Multiple Fusion Strategies**: Supports weighted, MLP, attention, and bilinear fusion\n",
    "- ‚úÖ **Comprehensive Evaluation**: Detailed metrics and visualizations\n",
    "- ‚úÖ **Google Colab Ready**: Works seamlessly in Colab environment\n",
    "- ‚úÖ **Reproducible Results**: Proper random seed management\n",
    "\n",
    "### üöÄ Potential Improvements\n",
    "\n",
    "1. **Real Data Integration**:\n",
    "   - Replace synthetic data with real multimodal emotion datasets\n",
    "   - Use datasets like MELD, IEMOCAP, or CMU-MOSEI\n",
    "\n",
    "2. **Advanced Fusion Techniques**:\n",
    "   - Early fusion at feature level\n",
    "   - Attention-based temporal fusion for video sequences\n",
    "   - Transformer-based multimodal fusion\n",
    "\n",
    "3. **Model Enhancements**:\n",
    "   - Fine-tune pre-trained models end-to-end\n",
    "   - Add more modalities (audio, physiological signals)\n",
    "   - Implement ensemble methods\n",
    "\n",
    "4. **Production Readiness**:\n",
    "   - Add real-time inference capabilities\n",
    "   - Optimize for mobile/edge deployment\n",
    "   - Implement confidence thresholding\n",
    "\n",
    "### üí° Usage Tips\n",
    "\n",
    "- **Experiment with different fusion strategies** using the `FUSION_STRATEGY` parameter\n",
    "- **Adjust learning rates and epochs** based on your specific dataset\n",
    "- **Monitor for overfitting** using the validation metrics\n",
    "- **Use ensemble of different fusion strategies** for better performance\n",
    "\n",
    "### üîÑ How to Extend\n",
    "\n",
    "To use this notebook with your own models:\n",
    "\n",
    "1. Replace the FER model loading section with your trained CNN\n",
    "2. Replace the TER model loading section with your trained text classifier  \n",
    "3. Modify `NUM_CLASSES` and `EMOTION_NAMES` as needed\n",
    "4. Update the dataset class for your specific data format\n",
    "5. Adjust fusion model architecture if needed\n",
    "\n",
    "**Happy emotion recognition! üé≠‚ú®**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

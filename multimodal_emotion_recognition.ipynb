{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "89bf6ac6",
      "metadata": {
        "id": "89bf6ac6"
      },
      "source": [
        "# Multimodal Emotion Recognition System\n",
        "## Combining Facial and Text Emotion Analysis with Deep Learning\n",
        "\n",
        "This notebook implements a comprehensive multimodal emotion recognition system that combines:\n",
        "- **Facial Emotion Recognition (FER)** using Convolutional Neural Networks\n",
        "- **Text Emotion Recognition (TER)** using DistilBERT transformer\n",
        "- **Multimodal Fusion** with both early and late fusion strategies\n",
        "\n",
        "### Key Features:\n",
        "- 🎯 **6 Emotion Classes**: Joy, Anger, Disgust, Sadness, Fear, Surprise\n",
        "- 🧠 **Advanced Models**: CNN for images, DistilBERT for text\n",
        "- 🔗 **Fusion Techniques**: Early and late fusion strategies\n",
        "- 📊 **Comprehensive Evaluation**: Detailed performance analysis\n",
        "- 🚀 **Google Colab Optimized**: GPU acceleration and easy deployment\n",
        "\n",
        "### Author: Henry\n",
        "### Date: July 30, 2025\n",
        "### Environment: Google Colab with GPU support"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c95e6d2",
      "metadata": {
        "id": "2c95e6d2"
      },
      "source": [
        "## 1. Environment Setup and Dependencies\n",
        "Let's start by setting up Google Colab environment and installing required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "51be7d35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51be7d35",
        "outputId": "b6a008e0-cc52-4076-8c44-fa294cd937ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running in local environment\n",
            "TensorFlow version: 2.19.0\n",
            "GPU available: []\n",
            "No GPU found. Using CPU (training will be slower).\n",
            "TensorFlow version: 2.19.0\n",
            "GPU available: []\n",
            "No GPU found. Using CPU (training will be slower).\n"
          ]
        }
      ],
      "source": [
        "# Check if we're running in Google Colab\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    print(\"Running in Google Colab\")\n",
        "    # Mount Google Drive for data access\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Set working directory to a folder in your drive\n",
        "    import os\n",
        "    os.chdir('/content/drive/MyDrive/emotion_recognition')\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"Running in local environment\")\n",
        "\n",
        "# Check GPU availability\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available for training!\")\n",
        "else:\n",
        "    print(\"No GPU found. Using CPU (training will be slower).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b63961d2",
      "metadata": {
        "id": "b63961d2"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install tensorflow>=2.13.0\n",
        "!pip install transformers>=4.21.0\n",
        "!pip install torch>=1.11.0\n",
        "!pip install scikit-learn>=1.1.0\n",
        "!pip install matplotlib>=3.5.0\n",
        "!pip install seaborn>=0.11.0\n",
        "!pip install numpy>=1.21.0\n",
        "!pip install pandas>=1.4.0\n",
        "!pip install pillow>=9.0.0\n",
        "!pip install tqdm>=4.64.0\n",
        "\n",
        "# Restart runtime after installation (uncomment if needed)\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c26538e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c26538e",
        "outputId": "ff9fde9c-83e0-4b7b-98f5-54ffefafa9e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "All libraries imported successfully!\n",
            "TensorFlow version: 2.19.0\n",
            "Using GPU: False\n",
            "All libraries imported successfully!\n",
            "TensorFlow version: 2.19.0\n",
            "Using GPU: False\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# TensorFlow and Keras - Fix for compatibility\n",
        "import tensorflow as tf\n",
        "\n",
        "# Use tf.keras explicitly to avoid version conflicts\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Transformers\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Using GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8230c0a3",
      "metadata": {
        "id": "8230c0a3"
      },
      "source": [
        "## 2. Configuration and Constants\n",
        "Define all model configurations, hyperparameters, and constants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "067635d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "067635d1",
        "outputId": "f7ddf1a6-c555-4525-ee51-b0efd8a0b13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "Emotion classes: ['joy', 'anger', 'disgust', 'sadness', 'fear', 'surprise']\n",
            "Image size: 48x48x1\n",
            "Batch size: 32\n",
            "Learning rate: 0.001\n",
            "Base path: /content/drive/MyDrive/emotion_recognition\n"
          ]
        }
      ],
      "source": [
        "# Configuration and Constants\n",
        "class Config:\n",
        "    # Emotion classes\n",
        "    EMOTION_CLASSES = ['joy', 'anger', 'disgust', 'sadness', 'fear', 'surprise']\n",
        "    NUM_CLASSES = len(EMOTION_CLASSES)\n",
        "\n",
        "    # Image parameters\n",
        "    IMG_HEIGHT = 48\n",
        "    IMG_WIDTH = 48\n",
        "    IMG_CHANNELS = 1  # Grayscale\n",
        "\n",
        "    # Text parameters\n",
        "    MAX_TEXT_LENGTH = 128\n",
        "    BERT_MODEL = 'distilbert-base-uncased'\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 50\n",
        "    LEARNING_RATE = 0.001\n",
        "    PATIENCE = 10\n",
        "\n",
        "    # Data paths (adjust for your Google Drive structure)\n",
        "    BASE_PATH = '/content/drive/MyDrive/emotion_recognition'\n",
        "    DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
        "    RAW_DATA_PATH = os.path.join(DATA_PATH, 'raw')\n",
        "    PROCESSED_DATA_PATH = os.path.join(DATA_PATH, 'processed')\n",
        "    MODELS_PATH = os.path.join(BASE_PATH, 'models')\n",
        "    RESULTS_PATH = os.path.join(BASE_PATH, 'results')\n",
        "\n",
        "    # Specific data paths\n",
        "    FER_IMAGES_PATH = os.path.join(RAW_DATA_PATH, 'fer_images')\n",
        "    TEXT_DATA_PATH = os.path.join(RAW_DATA_PATH, 'text_data')\n",
        "    MULTIMODAL_DATA_PATH = os.path.join(RAW_DATA_PATH, 'multimodal_data')\n",
        "\n",
        "    # Model save paths\n",
        "    FER_MODEL_PATH = os.path.join(MODELS_PATH, 'fer_model.h5')\n",
        "    TER_MODEL_PATH = os.path.join(MODELS_PATH, 'ter_model.h5')\n",
        "    MULTIMODAL_MODEL_PATH = os.path.join(MODELS_PATH, 'multimodal_model.h5')\n",
        "\n",
        "# Print configuration\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"Emotion classes: {Config.EMOTION_CLASSES}\")\n",
        "print(f\"Image size: {Config.IMG_HEIGHT}x{Config.IMG_WIDTH}x{Config.IMG_CHANNELS}\")\n",
        "print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
        "print(f\"Learning rate: {Config.LEARNING_RATE}\")\n",
        "print(f\"Base path: {Config.BASE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f923878",
      "metadata": {
        "id": "0f923878"
      },
      "source": [
        "## 3. Utility Functions\n",
        "Helper functions for data processing, visualization, and model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "154d6dba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "154d6dba",
        "outputId": "8af1bf68-feac-49e0-d6b9-dddd3d16c746"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Utility functions defined successfully!\n"
          ]
        }
      ],
      "source": [
        "def create_directories():\n",
        "    \"\"\"Create necessary directories for the project.\"\"\"\n",
        "    directories = [\n",
        "        Config.DATA_PATH,\n",
        "        Config.RAW_DATA_PATH,\n",
        "        Config.PROCESSED_DATA_PATH,\n",
        "        Config.MODELS_PATH,\n",
        "        Config.RESULTS_PATH,\n",
        "        Config.FER_IMAGES_PATH,\n",
        "        Config.TEXT_DATA_PATH,\n",
        "        Config.MULTIMODAL_DATA_PATH,\n",
        "        os.path.join(Config.RESULTS_PATH, 'metrics'),\n",
        "        os.path.join(Config.RESULTS_PATH, 'plots'),\n",
        "    ]\n",
        "\n",
        "    # Create emotion subdirectories for FER images\n",
        "    for split in ['train', 'validation', 'test']:\n",
        "        for emotion in Config.EMOTION_CLASSES:\n",
        "            directories.append(os.path.join(Config.FER_IMAGES_PATH, split, emotion))\n",
        "\n",
        "    for directory in directories:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    print(\"Directory structure created successfully!\")\n",
        "\n",
        "def plot_training_history(history, save_path=None):\n",
        "    \"\"\"Plot training history for loss and accuracy.\"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(history.history['loss'], label='Training Loss')\n",
        "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    ax1.set_title('Model Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    ax2.set_title('Model Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', save_path=None):\n",
        "    \"\"\"Plot confusion matrix.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, class_names):\n",
        "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
        "    predictions = model.predict(X_test)\n",
        "    y_pred = np.argmax(predictions, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'classification_report': report,\n",
        "        'predictions': predictions,\n",
        "        'y_pred': y_pred,\n",
        "        'y_true': y_true\n",
        "    }\n",
        "\n",
        "def save_json(data, filepath):\n",
        "    \"\"\"Save data as JSON file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "    with open(filepath, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    print(f\"Data saved to {filepath}\")\n",
        "\n",
        "print(\"Utility functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bdce3af",
      "metadata": {
        "id": "9bdce3af"
      },
      "source": [
        "## 4. Data Loading and Preprocessing\n",
        "Functions for loading and preprocessing both image and text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "acb438c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "91f8819e97164942bc807d5d636285bd",
            "853e8fe021e9472f9c7ebdf3ca9ab583",
            "8d71d26a146149c895c0715d3e7dd913",
            "17ecf68029404bfabb7735f9c543d81c",
            "8f345f79051943648453dc8b177f6511",
            "ffa3dfa79da541dd9a67375c4608a8f7",
            "4c7d183754da4d2280e297d813162d58",
            "64ec69e08a924fed8e82f6e0cdc4120d",
            "0bb7386f9ba548978989a611e89c85cb",
            "c85aba4232bd476aaf209f6b7e268773",
            "b7a53409d5ad437696a8660fad6d76a2",
            "f7dbff2286964081a88e49303ec0f730",
            "28261fdf6a2e430da380f85382473492",
            "9a803a80da51413aa4b31d0b4f64e3dc",
            "a34d22578abe4f68869f03d76b77de9c",
            "95ac3c1ed40141d6ad087c86e52fc50b",
            "f386d5b387504ec287cccdd6821f7e00",
            "3220d1c94dc349a495bad77278d58ebc",
            "819d9a7fefd44e69a26848511f3ffbd5",
            "7d2f1f4108dc444988bc130936e7f7c7",
            "df80782a54a847569098ae2a0b42cbe3",
            "1d471d4711e848ff8b91bc552dd9c33f",
            "fb3dfa3b15d94ea0a74587134c69e2a1",
            "9154c437c97c4a46bd3c125c3e7c8624",
            "9f05f03dda84484d9993214b2470a407",
            "8192196aaf904f7d8c5692547c01760e",
            "facd7a64822447529bc5934faf518a34",
            "d5658c94ec6a4d398790c2978c0ece00",
            "8baa543ef29541628dbb8708f1a1d54d",
            "41bf980810444cdebec292503ef66961",
            "621fa8c5a637479bbdcad5784704b84c",
            "1a45d93c25fb4cee89d981300d2a729e",
            "d7a32b300b4341f9bb65e2fb1f869169",
            "a77f7a8b9f574bb5839c3e5e5214a0ae",
            "6f19bec67dee4984babced020c550dbc",
            "1297516bfb0548e384a5d139235a60c1",
            "0b24891f928e4e75b391b9972864d85f",
            "2ac5e67f9cbc4db8afa709be0a583da0",
            "2c92b36c57774fccbc79f49e5b5ebd55",
            "a109e968c78d4494b07b9e01b2062986",
            "cdc675c6b7944215bc7216f6d3fed954",
            "3a2dabe975f8408a8faaba53c02e1f2f",
            "ba88ef04d10d4f83bd51b6fd76917b05",
            "bd45ecf53f4b47ff9e594bab6d1aa8e6"
          ]
        },
        "id": "acb438c0",
        "outputId": "564a8948-a109-455e-aa1d-7f81a0e1212f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loader initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "class DataLoader:\n",
        "    \"\"\"Class for loading and preprocessing data.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(config.BERT_MODEL)\n",
        "\n",
        "    def load_image_data(self, subset='train'):\n",
        "        \"\"\"Load image data using ImageDataGenerator.\"\"\"\n",
        "        datagen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            zoom_range=0.2,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        # For validation and test, don't apply augmentation\n",
        "        if subset in ['validation', 'test']:\n",
        "            datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "        data_path = os.path.join(self.config.FER_IMAGES_PATH, subset)\n",
        "\n",
        "        if not os.path.exists(data_path):\n",
        "            print(f\"Warning: Path {data_path} does not exist. Creating dummy data.\")\n",
        "            return self.create_dummy_image_data(subset)\n",
        "\n",
        "        generator = datagen.flow_from_directory(\n",
        "            data_path,\n",
        "            target_size=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH),\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            class_mode='categorical',\n",
        "            color_mode='grayscale',\n",
        "            classes=self.config.EMOTION_CLASSES,\n",
        "            shuffle=(subset == 'train')\n",
        "        )\n",
        "\n",
        "        return generator\n",
        "\n",
        "    def create_dummy_image_data(self, subset):\n",
        "        \"\"\"Create dummy image data for testing.\"\"\"\n",
        "        print(f\"Creating dummy {subset} data...\")\n",
        "\n",
        "        # Create dummy images and labels\n",
        "        num_samples = 100 if subset == 'train' else 50\n",
        "        images = np.random.rand(num_samples, self.config.IMG_HEIGHT,\n",
        "                              self.config.IMG_WIDTH, self.config.IMG_CHANNELS)\n",
        "        labels = np.random.randint(0, self.config.NUM_CLASSES, num_samples)\n",
        "        labels = to_categorical(labels, self.config.NUM_CLASSES)\n",
        "\n",
        "        return images, labels\n",
        "\n",
        "    def load_text_data(self, file_path):\n",
        "        \"\"\"Load text data from JSON file.\"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
        "            return self.create_dummy_text_data()\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        texts = [item['text'] for item in data]\n",
        "        emotions = [item['emotion'] for item in data]\n",
        "\n",
        "        # Convert emotion labels to indices\n",
        "        emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.config.EMOTION_CLASSES)}\n",
        "        labels = [emotion_to_idx[emotion] for emotion in emotions]\n",
        "\n",
        "        return texts, labels\n",
        "\n",
        "    def create_dummy_text_data(self):\n",
        "        \"\"\"Create dummy text data for testing.\"\"\"\n",
        "        print(\"Creating dummy text data...\")\n",
        "\n",
        "        dummy_texts = [\n",
        "            \"I am so happy today!\",\n",
        "            \"This makes me really angry.\",\n",
        "            \"That's completely disgusting.\",\n",
        "            \"I feel so sad about this.\",\n",
        "            \"This is really scary.\",\n",
        "            \"What a surprise that was!\"\n",
        "        ] * 50  # Repeat to get more samples\n",
        "\n",
        "        dummy_labels = list(range(self.config.NUM_CLASSES)) * 50\n",
        "\n",
        "        return dummy_texts, dummy_labels\n",
        "\n",
        "    def preprocess_text(self, texts):\n",
        "        \"\"\"Preprocess text data for BERT.\"\"\"\n",
        "        encodings = self.tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.config.MAX_TEXT_LENGTH,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        return encodings\n",
        "\n",
        "    def load_multimodal_data(self, file_path):\n",
        "        \"\"\"Load multimodal data from JSON file.\"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
        "            return self.create_dummy_multimodal_data()\n",
        "\n",
        "        with open(file_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def create_dummy_multimodal_data(self):\n",
        "        \"\"\"Create dummy multimodal data for testing.\"\"\"\n",
        "        print(\"Creating dummy multimodal data...\")\n",
        "\n",
        "        dummy_data = []\n",
        "        for i in range(300):  # 300 samples\n",
        "            emotion_idx = i % self.config.NUM_CLASSES\n",
        "            emotion = self.config.EMOTION_CLASSES[emotion_idx]\n",
        "\n",
        "            dummy_data.append({\n",
        "                'image_path': f'dummy_image_{i}.jpg',\n",
        "                'text': f'This is a {emotion} text sample number {i}.',\n",
        "                'emotion': emotion\n",
        "            })\n",
        "\n",
        "        return dummy_data\n",
        "\n",
        "# Initialize data loader\n",
        "data_loader = DataLoader(Config)\n",
        "print(\"Data loader initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aab2d66",
      "metadata": {
        "id": "9aab2d66"
      },
      "source": [
        "## 5. Facial Emotion Recognition (FER) Model\n",
        "CNN-based model for facial emotion recognition from images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e65f770",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e65f770",
        "outputId": "8b269dfb-c757-4a27-b8b8-c95e3a7c245b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FER model class initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "class FacialEmotionRecognizer:\n",
        "    \"\"\"CNN-based Facial Emotion Recognition model.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build CNN model for facial emotion recognition.\"\"\"\n",
        "        # Use tf.keras.Input explicitly\n",
        "        inputs = tf.keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS))\n",
        "\n",
        "        # First convolutional block\n",
        "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling2D((2, 2))(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "\n",
        "        # Second convolutional block\n",
        "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling2D((2, 2))(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "\n",
        "        # Third convolutional block\n",
        "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling2D((2, 2))(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "\n",
        "        # Fourth convolutional block\n",
        "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.MaxPooling2D((2, 2))(x)\n",
        "        x = layers.Dropout(0.25)(x)\n",
        "\n",
        "        # Global average pooling and dense layers\n",
        "        x = layers.GlobalAveragePooling2D()(x)\n",
        "        x = layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        x = layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        # Output layer\n",
        "        outputs = layers.Dense(self.config.NUM_CLASSES, activation='softmax', name='fer_output')(x)\n",
        "\n",
        "        # Use tf.keras.Model explicitly\n",
        "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='FacialEmotionRecognizer')\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def train(self, train_generator, val_generator):\n",
        "        \"\"\"Train the FER model.\"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=self.config.PATIENCE,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                self.config.FER_MODEL_PATH,\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            train_generator,\n",
        "            epochs=self.config.EPOCHS,\n",
        "            validation_data=val_generator,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions on input data.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built or loaded. Call build_model() or load_model() first.\")\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        \"\"\"Load a saved model.\"\"\"\n",
        "        path = model_path or self.config.FER_MODEL_PATH\n",
        "        if os.path.exists(path):\n",
        "            self.model = tf.keras.models.load_model(path)\n",
        "            print(f\"FER model loaded from {path}\")\n",
        "        else:\n",
        "            print(f\"Model file not found at {path}\")\n",
        "\n",
        "    def save_model(self, model_path=None):\n",
        "        \"\"\"Save the current model.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
        "\n",
        "        path = model_path or self.config.FER_MODEL_PATH\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        self.model.save(path)\n",
        "        print(f\"FER model saved to {path}\")\n",
        "\n",
        "# Initialize FER model\n",
        "fer_model = FacialEmotionRecognizer(Config)\n",
        "print(\"FER model class initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bfa46bc",
      "metadata": {
        "id": "1bfa46bc"
      },
      "source": [
        "## 6. Text Emotion Recognition (TER) Model\n",
        "DistilBERT-based model for text emotion recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8613275b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8613275b",
        "outputId": "2b29ec53-7b8d-459a-ff66-1d6e4de40ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TER model class initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "class TextEmotionRecognizer:\n",
        "    \"\"\"DistilBERT-based Text Emotion Recognition model.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained(config.BERT_MODEL)\n",
        "        self.model = None\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build DistilBERT model for text emotion recognition.\"\"\"\n",
        "        # Load pre-trained DistilBERT\n",
        "        distilbert = TFDistilBertForSequenceClassification.from_pretrained(\n",
        "            self.config.BERT_MODEL,\n",
        "            num_labels=self.config.NUM_CLASSES,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "\n",
        "        # Input layers - use tf.keras.Input explicitly\n",
        "        input_ids = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='input_ids')\n",
        "        attention_mask = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
        "\n",
        "        # Get DistilBERT outputs\n",
        "        distilbert_outputs = distilbert([input_ids, attention_mask])\n",
        "\n",
        "        # Extract logits\n",
        "        logits = distilbert_outputs.logits\n",
        "\n",
        "        # Apply softmax for probability distribution\n",
        "        outputs = layers.Softmax(name='ter_output')(logits)\n",
        "\n",
        "        # Create model - use tf.keras.Model explicitly\n",
        "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs, name='TextEmotionRecognizer')\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def preprocess_texts(self, texts):\n",
        "        \"\"\"Preprocess texts for model input.\"\"\"\n",
        "        encodings = self.tokenizer(\n",
        "            texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=self.config.MAX_TEXT_LENGTH,\n",
        "            return_tensors='tf'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encodings['input_ids'],\n",
        "            'attention_mask': encodings['attention_mask']\n",
        "        }\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train the TER model.\"\"\"\n",
        "        if self.model is None:\n",
        "            self.build_model()\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=self.config.PATIENCE,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                self.config.TER_MODEL_PATH,\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            epochs=self.config.EPOCHS,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions on input data.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built or loaded. Call build_model() or load_model() first.\")\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def load_model(self, model_path=None):\n",
        "        \"\"\"Load a saved model.\"\"\"\n",
        "        path = model_path or self.config.TER_MODEL_PATH\n",
        "        if os.path.exists(path):\n",
        "            self.model = tf.keras.models.load_model(path)\n",
        "            print(f\"TER model loaded from {path}\")\n",
        "        else:\n",
        "            print(f\"Model file not found at {path}\")\n",
        "\n",
        "    def save_model(self, model_path=None):\n",
        "        \"\"\"Save the current model.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
        "\n",
        "        path = model_path or self.config.TER_MODEL_PATH\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        self.model.save(path)\n",
        "        print(f\"TER model saved to {path}\")\n",
        "\n",
        "# Initialize TER model\n",
        "ter_model = TextEmotionRecognizer(Config)\n",
        "print(\"TER model class initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8061a798",
      "metadata": {
        "id": "8061a798"
      },
      "source": [
        "## 7. Multimodal Fusion Model\n",
        "Combining FER and TER models for improved emotion recognition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0da1077b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0da1077b",
        "outputId": "96fb90be-b41e-4cfd-d3d1-4c3f32f4eaf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multimodal fusion model class defined successfully!\n"
          ]
        }
      ],
      "source": [
        "class MultimodalEmotionRecognizer:\n",
        "    \"\"\"Multimodal emotion recognition combining FER and TER.\"\"\"\n",
        "\n",
        "    def __init__(self, config, fer_model, ter_model):\n",
        "        self.config = config\n",
        "        self.fer_model = fer_model\n",
        "        self.ter_model = ter_model\n",
        "        self.model = None\n",
        "\n",
        "    def build_early_fusion_model(self):\n",
        "        \"\"\"Build early fusion model combining image and text features.\"\"\"\n",
        "        # Image input branch - use tf.keras.Input explicitly\n",
        "        image_input = tf.keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS), name='image_input')\n",
        "\n",
        "        # Extract image features using FER model backbone\n",
        "        # Create a functional model for the FER backbone\n",
        "        fer_backbone_output = self.fer_model.model.layers[-3].output # Output before the last two dense layers\n",
        "        fer_backbone = tf.keras.Model(inputs=self.fer_model.model.input, outputs=fer_backbone_output, name='fer_backbone')\n",
        "        image_features = fer_backbone(image_input)\n",
        "\n",
        "        # Text input branches\n",
        "        text_input_ids = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_input_ids')\n",
        "        text_attention_mask = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_attention_mask')\n",
        "\n",
        "        # Extract text features using TER model backbone\n",
        "        ter_base = self.ter_model.model.layers[2]  # DistilBERT layer\n",
        "        text_features = ter_base([text_input_ids, text_attention_mask]).last_hidden_state\n",
        "        text_features = layers.GlobalAveragePooling1D()(text_features)\n",
        "\n",
        "        # Fusion layer\n",
        "        combined_features = layers.Concatenate(name='feature_fusion')([image_features, text_features])\n",
        "        \n",
        "        # Additional fusion layers\n",
        "        x = layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001))(combined_features)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "        x = layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        # Final classification layer\n",
        "        outputs = layers.Dense(self.config.NUM_CLASSES, activation='softmax', name='multimodal_output')(x)\n",
        "\n",
        "        # Create model - use tf.keras.Model explicitly\n",
        "        model = tf.keras.Model(\n",
        "            inputs=[image_input, text_input_ids, text_attention_mask],\n",
        "            outputs=outputs,\n",
        "            name='MultimodalEmotionRecognizer_EarlyFusion'\n",
        "        )\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def build_late_fusion_model(self):\n",
        "        \"\"\"Build late fusion model combining FER and TER predictions.\"\"\"\n",
        "        # Image input\n",
        "        image_input = tf.keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS), name='image_input')\n",
        "\n",
        "        # Text inputs\n",
        "        text_input_ids = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_input_ids')\n",
        "        text_attention_mask = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_attention_mask')\n",
        "\n",
        "        # Get FER predictions\n",
        "        fer_predictions = self.fer_model.model(image_input)\n",
        "\n",
        "        # Get TER predictions\n",
        "        ter_predictions = self.ter_model.model([text_input_ids, text_attention_mask])\n",
        "\n",
        "        # Weighted fusion of predictions\n",
        "        fusion_weights = layers.Dense(2, activation='softmax', name='fusion_weights')(\n",
        "            layers.Concatenate()([\n",
        "                layers.GlobalAveragePooling2D()(self.fer_model.model.layers[-3].output),  # FER features\n",
        "                layers.Dense(256, activation='relu')(ter_predictions)  # TER features\n",
        "            ])\n",
        "        )\n",
        "\n",
        "        # Apply weights\n",
        "        weighted_fer = layers.Multiply()([fer_predictions, layers.Lambda(lambda x: x[:, 0:1])(fusion_weights)])\n",
        "        weighted_ter = layers.Multiply()([ter_predictions, layers.Lambda(lambda x: x[:, 1:2])(fusion_weights)])\n",
        "\n",
        "        # Final prediction\n",
        "        outputs = layers.Add(name='multimodal_output')([weighted_fer, weighted_ter])\n",
        "\n",
        "        # Create model - use tf.keras.Model explicitly\n",
        "        model = tf.keras.Model(\n",
        "            inputs=[image_input, text_input_ids, text_attention_mask],\n",
        "            outputs=outputs,\n",
        "            name='MultimodalEmotionRecognizer_LateFusion'\n",
        "        )\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def simple_average_fusion(self, fer_predictions, ter_predictions):\n",
        "        \"\"\"Simple average fusion of FER and TER predictions.\"\"\"\n",
        "        return (fer_predictions + ter_predictions) / 2\n",
        "\n",
        "    def weighted_average_fusion(self, fer_predictions, ter_predictions, fer_weight=0.6):\n",
        "        \"\"\"Weighted average fusion of FER and TER predictions.\"\"\"\n",
        "        ter_weight = 1 - fer_weight\n",
        "        return fer_weight * fer_predictions + ter_weight * ter_predictions\n",
        "\n",
        "    def train(self, X_train, y_train, X_val, y_val, fusion_type='early'):\n",
        "        \"\"\"Train the multimodal model.\"\"\"\n",
        "        if fusion_type == 'early':\n",
        "            self.build_early_fusion_model()\n",
        "        elif fusion_type == 'late':\n",
        "            self.build_late_fusion_model()\n",
        "        else:\n",
        "            raise ValueError(\"fusion_type must be 'early' or 'late'\")\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=self.config.PATIENCE,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ModelCheckpoint(\n",
        "                self.config.MULTIMODAL_MODEL_PATH,\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            epochs=self.config.EPOCHS,\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions on input data.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not built or loaded. Call build_*_fusion_model() first.\")\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    def save_model(self, model_path=None):\n",
        "        \"\"\"Save the current model.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
        "\n",
        "        path = model_path or self.config.MULTIMODAL_MODEL_PATH\n",
        "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "        self.model.save(path)\n",
        "        print(f\"Multimodal model saved to {path}\")\n",
        "\n",
        "print(\"Multimodal fusion model class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04dccfc8",
      "metadata": {
        "id": "04dccfc8"
      },
      "source": [
        "## 8. Setup Directory Structure\n",
        "Create the necessary directory structure for data organization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "415d8bb5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "415d8bb5",
        "outputId": "648f905b-611b-4624-f93d-ef49780a8d04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory structure created successfully!\n",
            "\\n📁 Project Directory Structure:\n",
            "emotion_recognition/\n",
            "├── 📁 data/\n",
            "│   ├── 📁 processed/\n",
            "│   └── 📁 raw/\n",
            "│       ├── 📁 fer_images/\n",
            "│       ├── 📁 multimodal_data/\n",
            "│       └── 📁 text_data/\n",
            "├── 📁 models/\n",
            "└── 📁 results/\n",
            "    ├── 📁 metrics/\n",
            "    └── 📁 plots/\n"
          ]
        }
      ],
      "source": [
        "# Create directory structure\n",
        "create_directories()\n",
        "\n",
        "# Display the created directory structure\n",
        "def display_directory_structure(path, prefix=\"\", max_depth=3, current_depth=0):\n",
        "    \"\"\"Display directory structure in a tree format.\"\"\"\n",
        "    if current_depth >= max_depth:\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"{prefix}📁 {os.path.basename(path)} (will be created)\")\n",
        "        return\n",
        "\n",
        "    items = sorted(os.listdir(path))\n",
        "    for i, item in enumerate(items):\n",
        "        item_path = os.path.join(path, item)\n",
        "        is_last = i == len(items) - 1\n",
        "        current_prefix = \"└── \" if is_last else \"├── \"\n",
        "\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"{prefix}{current_prefix}📁 {item}/\")\n",
        "            extension = \"    \" if is_last else \"│   \"\n",
        "            display_directory_structure(item_path, prefix + extension, max_depth, current_depth + 1)\n",
        "        else:\n",
        "            print(f\"{prefix}{current_prefix}📄 {item}\")\n",
        "\n",
        "print(\"\\\\n📁 Project Directory Structure:\")\n",
        "print(\"emotion_recognition/\")\n",
        "display_directory_structure(Config.BASE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c633195",
      "metadata": {
        "id": "4c633195"
      },
      "source": [
        "## 9. Train Facial Emotion Recognition Model\n",
        "Train the CNN-based FER model on facial expression data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0b451a1f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 845
        },
        "id": "0b451a1f",
        "outputId": "0f170809-a0ca-4e51-988b-4c6eb6f0db95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading FER training data...\n",
            "Found 0 images belonging to 6 classes.\n",
            "Found 0 images belonging to 6 classes.\n",
            "Found 0 images belonging to 6 classes.\n",
            "No images found in directory structure.\n",
            "Switching to dummy data for demonstration...\n",
            "Creating dummy train data...\n",
            "Creating dummy validation data...\n",
            "Creating dummy test data...\n",
            "Created dummy training samples: 100\n",
            "Created dummy validation samples: 50\n",
            "Created dummy test samples: 50\n",
            "\\nBuilding FER model...\n",
            "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution), but are not present in its tracked objects:   <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 1, 32) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution_1), but are not present in its tracked objects:   <tf.Variable 'conv2d_1/kernel:0' shape=(3, 3, 32, 32) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution_2), but are not present in its tracked objects:   <tf.Variable 'conv2d_2/kernel:0' shape=(3, 3, 32, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution_3), but are not present in its tracked objects:   <tf.Variable 'conv2d_3/kernel:0' shape=(3, 3, 64, 64) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution_4), but are not present in its tracked objects:   <tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 64, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution_5), but are not present in its tracked objects:   <tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 128, 128) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution_6), but are not present in its tracked objects:   <tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 128, 256) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.convolution_7), but are not present in its tracked objects:   <tf.Variable 'conv2d_7/kernel:0' shape=(3, 3, 256, 256) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.bias_add_8), but are not present in its tracked objects:   <tf.Variable 'dense/bias:0' shape=(512,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.bias_add_9), but are not present in its tracked objects:   <tf.Variable 'dense_1/bias:0' shape=(256,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.nn.bias_add_10), but are not present in its tracked objects:   <tf.Variable 'fer_output/bias:0' shape=(6,) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "All `inputs` values must be KerasTensors. Received: inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") including invalid value KerasTensor(type_spec=TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") of type <class 'tf_keras.src.engine.keras_tensor.KerasTensor'>",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Build and display FER model architecture\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mnBuilding FER model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mfer_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(fer_model.model.summary())\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Visualize model architecture (optional)\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mFacialEmotionRecognizer.build_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Output layer\u001b[39;00m\n\u001b[32m     54\u001b[39m outputs = layers.Dense(\u001b[38;5;28mself\u001b[39m.config.NUM_CLASSES, activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m, name=\u001b[33m'\u001b[39m\u001b[33mfer_output\u001b[39m\u001b[33m'\u001b[39m)(x)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m model = \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFacialEmotionRecognizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n\u001b[32m     59\u001b[39m model.compile(\n\u001b[32m     60\u001b[39m     optimizer=Adam(learning_rate=\u001b[38;5;28mself\u001b[39m.config.LEARNING_RATE),\n\u001b[32m     61\u001b[39m     loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     62\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     63\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tracking.py:26\u001b[39m, in \u001b[36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py:120\u001b[39m, in \u001b[36mFunctional.__init__\u001b[39m\u001b[34m(self, inputs, outputs, name, **kwargs)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flat_inputs:\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, backend.KerasTensor):\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    121\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAll `inputs` values must be KerasTensors. Received: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    122\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m including invalid value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    123\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    124\u001b[39m         )\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flat_outputs:\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, backend.KerasTensor):\n",
            "\u001b[31mValueError\u001b[39m: All `inputs` values must be KerasTensors. Received: inputs=KerasTensor(type_spec=TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") including invalid value KerasTensor(type_spec=TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\") of type <class 'tf_keras.src.engine.keras_tensor.KerasTensor'>"
          ]
        }
      ],
      "source": [
        "# Load FER data\n",
        "print(\"Loading FER training data...\")\n",
        "try:\n",
        "    train_generator = data_loader.load_image_data('train')\n",
        "    val_generator = data_loader.load_image_data('validation')\n",
        "    test_generator = data_loader.load_image_data('test')\n",
        "\n",
        "    # Check if generators have any data\n",
        "    if hasattr(train_generator, 'samples') and train_generator.samples > 0:\n",
        "        print(f\"Training samples: {train_generator.samples}\")\n",
        "        print(f\"Validation samples: {val_generator.samples}\")\n",
        "        print(f\"Test samples: {test_generator.samples}\")\n",
        "        use_real_data = True\n",
        "    else:\n",
        "        print(\"No images found in directory structure.\")\n",
        "        print(\"Switching to dummy data for demonstration...\")\n",
        "        use_real_data = False\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading FER data: {e}\")\n",
        "    print(\"Using dummy data for demonstration...\")\n",
        "    use_real_data = False\n",
        "\n",
        "# If no real data, create dummy data\n",
        "if not use_real_data:\n",
        "    # Create dummy data\n",
        "    train_images, train_labels = data_loader.create_dummy_image_data('train')\n",
        "    val_images, val_labels = data_loader.create_dummy_image_data('validation')\n",
        "    test_images, test_labels = data_loader.create_dummy_image_data('test')\n",
        "\n",
        "    print(f\"Created dummy training samples: {len(train_images)}\")\n",
        "    print(f\"Created dummy validation samples: {len(val_images)}\")\n",
        "    print(f\"Created dummy test samples: {len(test_images)}\")\n",
        "\n",
        "    train_generator = (train_images, train_labels)\n",
        "    val_generator = (val_images, val_labels)\n",
        "    test_generator = (test_images, test_labels)\n",
        "\n",
        "# Build and display FER model architecture\n",
        "print(\"\\\\nBuilding FER model...\")\n",
        "fer_model.build_model()\n",
        "print(fer_model.model.summary())\n",
        "\n",
        "# Visualize model architecture (optional)\n",
        "try:\n",
        "    tf.keras.utils.plot_model(\n",
        "        fer_model.model,\n",
        "        to_file=os.path.join(Config.RESULTS_PATH, 'fer_model_architecture.png'),\n",
        "        show_shapes=True,\n",
        "        show_layer_names=True\n",
        "    )\n",
        "    print(\"Model architecture saved to results/fer_model_architecture.png\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not save model architecture plot: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0ec2fe",
      "metadata": {
        "id": "2d0ec2fe"
      },
      "outputs": [],
      "source": [
        "# Train FER model\n",
        "print(\"Training FER model...\")\n",
        "print(\"This may take a while depending on your dataset size and hardware...\")\n",
        "\n",
        "try:\n",
        "    if use_real_data and hasattr(train_generator, 'samples'):  # Real data generator\n",
        "        fer_history = fer_model.train(train_generator, val_generator)\n",
        "    else:  # Dummy data - train with arrays\n",
        "        print(\"Training with dummy data (reduced epochs for demonstration)...\")\n",
        "        fer_history = fer_model.model.fit(\n",
        "            train_generator[0], train_generator[1],\n",
        "            batch_size=Config.BATCH_SIZE,\n",
        "            epochs=min(3, Config.EPOCHS),  # Reduced epochs for dummy data\n",
        "            validation_data=(val_generator[0], val_generator[1]),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "    print(\"\\\\n✅ FER model training completed!\")\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(\n",
        "        fer_history,\n",
        "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'fer_training_history.png')\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    fer_model.save_model()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during FER training: {e}\")\n",
        "    print(\"Model architecture is built but not trained.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067ef163",
      "metadata": {
        "id": "067ef163"
      },
      "source": [
        "## 10. Train Text Emotion Recognition Model\n",
        "Train the DistilBERT-based TER model on text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f643e799",
      "metadata": {
        "id": "f643e799"
      },
      "outputs": [],
      "source": [
        "# Load text data for TER model\n",
        "print(\"Loading text data...\")\n",
        "try:\n",
        "    # Try to load real text data\n",
        "    train_texts, train_text_labels = data_loader.load_text_data(\n",
        "        os.path.join(Config.TEXT_DATA_PATH, 'train_text.json')\n",
        "    )\n",
        "    val_texts, val_text_labels = data_loader.load_text_data(\n",
        "        os.path.join(Config.TEXT_DATA_PATH, 'val_text.json')\n",
        "    )\n",
        "    test_texts, test_text_labels = data_loader.load_text_data(\n",
        "        os.path.join(Config.TEXT_DATA_PATH, 'test_text.json')\n",
        "    )\n",
        "\n",
        "    print(f\"Text training samples: {len(train_texts)}\")\n",
        "    print(f\"Text validation samples: {len(val_texts)}\")\n",
        "    print(f\"Text test samples: {len(test_texts)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading real text data: {e}\")\n",
        "    print(\"Using dummy text data for demonstration...\")\n",
        "\n",
        "    # Create dummy text data\n",
        "    train_texts, train_text_labels = data_loader.create_dummy_text_data()\n",
        "    val_texts, val_text_labels = data_loader.create_dummy_text_data()\n",
        "    test_texts, test_text_labels = data_loader.create_dummy_text_data()\n",
        "\n",
        "    # Reduce size for different splits\n",
        "    train_texts, train_text_labels = train_texts[:200], train_text_labels[:200]\n",
        "    val_texts, val_text_labels = val_texts[:50], val_text_labels[:50]\n",
        "    test_texts, test_text_labels = test_texts[:50], test_text_labels[:50]\n",
        "\n",
        "    print(f\"Created dummy text training samples: {len(train_texts)}\")\n",
        "    print(f\"Created dummy text validation samples: {len(val_texts)}\")\n",
        "    print(f\"Created dummy text test samples: {len(test_texts)}\")\n",
        "\n",
        "# Preprocess text data\n",
        "print(\"\\\\nPreprocessing text data...\")\n",
        "train_text_encoded = ter_model.preprocess_texts(train_texts)\n",
        "val_text_encoded = ter_model.preprocess_texts(val_texts)\n",
        "test_text_encoded = ter_model.preprocess_texts(test_texts)\n",
        "\n",
        "print(\"Text preprocessing completed!\")\n",
        "print(f\"Input shape: {train_text_encoded['input_ids'].shape}\")\n",
        "\n",
        "# Build TER model\n",
        "print(\"\\\\nBuilding TER model...\")\n",
        "ter_model.build_model()\n",
        "print(\"TER model built successfully!\")\n",
        "print(ter_model.model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dcbc93",
      "metadata": {
        "id": "b8dcbc93"
      },
      "outputs": [],
      "source": [
        "# Load text data\n",
        "print(\"Loading text data...\")\n",
        "try:\n",
        "    train_texts, train_text_labels = data_loader.load_text_data(\n",
        "        os.path.join(Config.TEXT_DATA_PATH, 'train_text_data.json')\n",
        "    )\n",
        "    val_texts, val_text_labels = data_loader.load_text_data(\n",
        "        os.path.join(Config.TEXT_DATA_PATH, 'val_text_data.json')\n",
        "    )\n",
        "    test_texts, test_text_labels = data_loader.load_text_data(\n",
        "        os.path.join(Config.TEXT_DATA_PATH, 'test_text_data.json')\n",
        "    )\n",
        "\n",
        "    print(f\"Training text samples: {len(train_texts)}\")\n",
        "    print(f\"Validation text samples: {len(val_texts)}\")\n",
        "    print(f\"Test text samples: {len(test_texts)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading text data: {e}\")\n",
        "    print(\"Using dummy text data for demonstration...\")\n",
        "\n",
        "    # Create dummy text data\n",
        "    train_texts, train_text_labels = data_loader.create_dummy_text_data()\n",
        "    val_texts, val_text_labels = data_loader.create_dummy_text_data()\n",
        "    test_texts, test_text_labels = data_loader.create_dummy_text_data()\n",
        "\n",
        "# Preprocess text data\n",
        "print(\"\\\\nPreprocessing text data...\")\n",
        "train_text_encoded = ter_model.preprocess_texts(train_texts)\n",
        "val_text_encoded = ter_model.preprocess_texts(val_texts)\n",
        "test_text_encoded = ter_model.preprocess_texts(test_texts)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "train_text_labels = np.array(train_text_labels)\n",
        "val_text_labels = np.array(val_text_labels)\n",
        "test_text_labels = np.array(test_text_labels)\n",
        "\n",
        "print(f\"Text encoding shape: {train_text_encoded['input_ids'].shape}\")\n",
        "print(f\"Labels shape: {train_text_labels.shape}\")\n",
        "\n",
        "# Display some sample texts\n",
        "print(\"\\\\nSample texts:\")\n",
        "for i in range(min(3, len(train_texts))):\n",
        "    emotion = Config.EMOTION_CLASSES[train_text_labels[i]]\n",
        "    print(f\"  {emotion}: '{train_texts[i][:100]}...'\")\n",
        "\n",
        "# Build TER model\n",
        "print(\"\\\\nBuilding TER model...\")\n",
        "ter_model.build_model()\n",
        "print(ter_model.model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "658ed2ea",
      "metadata": {
        "id": "658ed2ea"
      },
      "outputs": [],
      "source": [
        "# Train TER model\n",
        "print(\"Training TER model...\")\n",
        "print(\"This may take a while, especially for BERT-based models...\")\n",
        "\n",
        "try:\n",
        "    ter_history = ter_model.train(\n",
        "        train_text_encoded, train_text_labels,\n",
        "        val_text_encoded, val_text_labels\n",
        "    )\n",
        "\n",
        "    print(\"\\\\n✅ TER model training completed!\")\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(\n",
        "        ter_history,\n",
        "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'ter_training_history.png')\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    ter_model.save_model()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during TER training: {e}\")\n",
        "    print(\"Building model without training for demonstration...\")\n",
        "    ter_model.build_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2893bb1",
      "metadata": {
        "id": "d2893bb1"
      },
      "source": [
        "## 11. Train Multimodal Fusion Model\n",
        "Combine FER and TER models for improved performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb21d27",
      "metadata": {
        "id": "eeb21d27"
      },
      "outputs": [],
      "source": [
        "# Load multimodal data\n",
        "print(\"Loading multimodal data...\")\n",
        "try:\n",
        "    multimodal_data = data_loader.load_multimodal_data(\n",
        "        os.path.join(Config.MULTIMODAL_DATA_PATH, 'multimodal_dataset.json')\n",
        "    )\n",
        "    print(f\"Multimodal samples: {len(multimodal_data)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading multimodal data: {e}\")\n",
        "    print(\"Using dummy multimodal data for demonstration...\")\n",
        "    multimodal_data = data_loader.create_dummy_multimodal_data()\n",
        "\n",
        "# Prepare multimodal data\n",
        "print(\"\\\\nPreparing multimodal data...\")\n",
        "\n",
        "# Create dummy aligned data for demonstration\n",
        "# In real scenario, you would load actual images corresponding to texts\n",
        "num_samples = len(multimodal_data)\n",
        "multimodal_images = np.random.rand(num_samples, Config.IMG_HEIGHT, Config.IMG_WIDTH, Config.IMG_CHANNELS)\n",
        "multimodal_texts = [item['text'] for item in multimodal_data]\n",
        "multimodal_labels = [Config.EMOTION_CLASSES.index(item['emotion']) for item in multimodal_data]\n",
        "\n",
        "# Preprocess multimodal text data\n",
        "multimodal_text_encoded = ter_model.preprocess_texts(multimodal_texts)\n",
        "\n",
        "# Convert labels to categorical\n",
        "multimodal_labels_categorical = to_categorical(multimodal_labels, Config.NUM_CLASSES)\n",
        "\n",
        "# Split data\n",
        "split_idx = int(0.8 * num_samples)\n",
        "val_split_idx = int(0.9 * num_samples)\n",
        "\n",
        "# Training data\n",
        "train_mm_images = multimodal_images[:split_idx]\n",
        "train_mm_text_ids = multimodal_text_encoded['input_ids'][:split_idx]\n",
        "train_mm_text_mask = multimodal_text_encoded['attention_mask'][:split_idx]\n",
        "train_mm_labels = multimodal_labels_categorical[:split_idx]\n",
        "\n",
        "# Validation data\n",
        "val_mm_images = multimodal_images[split_idx:val_split_idx]\n",
        "val_mm_text_ids = multimodal_text_encoded['input_ids'][split_idx:val_split_idx]\n",
        "val_mm_text_mask = multimodal_text_encoded['attention_mask'][split_idx:val_split_idx]\n",
        "val_mm_labels = multimodal_labels_categorical[split_idx:val_split_idx]\n",
        "\n",
        "print(f\"Multimodal training samples: {len(train_mm_images)}\")\n",
        "print(f\"Multimodal validation samples: {len(val_mm_images)}\")\n",
        "\n",
        "# Initialize multimodal model\n",
        "multimodal_model = MultimodalEmotionRecognizer(Config, fer_model, ter_model)\n",
        "\n",
        "# Choose fusion strategy\n",
        "FUSION_TYPE = 'early'  # Change to 'late' for late fusion\n",
        "print(f\"\\\\nBuilding {FUSION_TYPE} fusion model...\")\n",
        "\n",
        "try:\n",
        "    if FUSION_TYPE == 'early':\n",
        "        multimodal_model.build_early_fusion_model()\n",
        "    else:\n",
        "        multimodal_model.build_late_fusion_model()\n",
        "\n",
        "    print(multimodal_model.model.summary())\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train_mm = [train_mm_images, train_mm_text_ids, train_mm_text_mask]\n",
        "    X_val_mm = [val_mm_images, val_mm_text_ids, val_mm_text_mask]\n",
        "\n",
        "    # Train multimodal model (reduced epochs for demonstration)\n",
        "    print(f\"\\\\nTraining {FUSION_TYPE} fusion model...\")\n",
        "    mm_history = multimodal_model.train(\n",
        "        X_train_mm, train_mm_labels,\n",
        "        X_val_mm, val_mm_labels,\n",
        "        fusion_type=FUSION_TYPE\n",
        "    )\n",
        "\n",
        "    print(\"\\\\n✅ Multimodal model training completed!\")\n",
        "\n",
        "    # Plot training history\n",
        "    plot_training_history(\n",
        "        mm_history,\n",
        "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', f'multimodal_{FUSION_TYPE}_training_history.png')\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    multimodal_model.save_model()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during multimodal training: {e}\")\n",
        "    print(\"Building model without training for demonstration...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5accda6",
      "metadata": {
        "id": "e5accda6"
      },
      "source": [
        "## 12. Model Evaluation and Comparison\n",
        "Evaluate and compare the performance of FER, TER, and multimodal models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41ae47b",
      "metadata": {
        "id": "d41ae47b"
      },
      "outputs": [],
      "source": [
        "# Evaluate FER Model\n",
        "print(\"Evaluating FER Model...\")\n",
        "try:\n",
        "    if hasattr(test_generator, 'samples'):  # Real data\n",
        "        fer_test_loss, fer_test_acc = fer_model.model.evaluate(test_generator, verbose=0)\n",
        "        # Get predictions for confusion matrix\n",
        "        test_predictions_fer = fer_model.model.predict(test_generator)\n",
        "        test_labels_fer = test_generator.classes\n",
        "    else:  # Dummy data\n",
        "        fer_test_loss, fer_test_acc = fer_model.model.evaluate(\n",
        "            test_generator[0], test_generator[1], verbose=0\n",
        "        )\n",
        "        test_predictions_fer = fer_model.model.predict(test_generator[0])\n",
        "        test_labels_fer = np.argmax(test_generator[1], axis=1)\n",
        "\n",
        "    fer_pred_classes = np.argmax(test_predictions_fer, axis=1)\n",
        "\n",
        "    print(f\"FER Test Accuracy: {fer_test_acc:.4f}\")\n",
        "    print(f\"FER Test Loss: {fer_test_loss:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix for FER\n",
        "    plot_confusion_matrix(\n",
        "        test_labels_fer, fer_pred_classes, Config.EMOTION_CLASSES,\n",
        "        title='FER Model - Confusion Matrix',\n",
        "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'fer_confusion_matrix.png')\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error evaluating FER model: {e}\")\n",
        "\n",
        "# Evaluate TER Model\n",
        "print(\"\\\\nEvaluating TER Model...\")\n",
        "try:\n",
        "    ter_test_loss, ter_test_acc = ter_model.model.evaluate(\n",
        "        test_text_encoded, test_text_labels, verbose=0\n",
        "    )\n",
        "    test_predictions_ter = ter_model.model.predict(test_text_encoded)\n",
        "    ter_pred_classes = np.argmax(test_predictions_ter, axis=1)\n",
        "\n",
        "    print(f\"TER Test Accuracy: {ter_test_acc:.4f}\")\n",
        "    print(f\"TER Test Loss: {ter_test_loss:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix for TER\n",
        "    plot_confusion_matrix(\n",
        "        test_text_labels, ter_pred_classes, Config.EMOTION_CLASSES,\n",
        "        title='TER Model - Confusion Matrix',\n",
        "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'ter_confusion_matrix.png')\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error evaluating TER model: {e}\")\n",
        "\n",
        "# Evaluate Multimodal Model\n",
        "print(\"\\\\nEvaluating Multimodal Model...\")\n",
        "try:\n",
        "    # Prepare test data for multimodal model\n",
        "    test_mm_images = multimodal_images[val_split_idx:]\n",
        "    test_mm_text_ids = multimodal_text_encoded['input_ids'][val_split_idx:]\n",
        "    test_mm_text_mask = multimodal_text_encoded['attention_mask'][val_split_idx:]\n",
        "    test_mm_labels = multimodal_labels_categorical[val_split_idx:]\n",
        "\n",
        "    X_test_mm = [test_mm_images, test_mm_text_ids, test_mm_text_mask]\n",
        "\n",
        "    mm_test_loss, mm_test_acc = multimodal_model.model.evaluate(\n",
        "        X_test_mm, test_mm_labels, verbose=0\n",
        "    )\n",
        "    test_predictions_mm = multimodal_model.model.predict(X_test_mm)\n",
        "    mm_pred_classes = np.argmax(test_predictions_mm, axis=1)\n",
        "    test_labels_mm = np.argmax(test_mm_labels, axis=1)\n",
        "\n",
        "    print(f\"Multimodal Test Accuracy: {mm_test_acc:.4f}\")\n",
        "    print(f\"Multimodal Test Loss: {mm_test_loss:.4f}\")\n",
        "\n",
        "    # Plot confusion matrix for multimodal\n",
        "    plot_confusion_matrix(\n",
        "        test_labels_mm, mm_pred_classes, Config.EMOTION_CLASSES,\n",
        "        title=f'Multimodal ({FUSION_TYPE.title()} Fusion) - Confusion Matrix',\n",
        "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', f'multimodal_{FUSION_TYPE}_confusion_matrix.png')\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error evaluating multimodal model: {e}\")\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*50)\n",
        "try:\n",
        "    print(f\"FER Model:        {fer_test_acc:.4f}\")\n",
        "    print(f\"TER Model:        {ter_test_acc:.4f}\")\n",
        "    print(f\"Multimodal Model: {mm_test_acc:.4f}\")\n",
        "\n",
        "    # Save results\n",
        "    results = {\n",
        "        'fer_accuracy': float(fer_test_acc),\n",
        "        'ter_accuracy': float(ter_test_acc),\n",
        "        'multimodal_accuracy': float(mm_test_acc),\n",
        "        'fusion_type': FUSION_TYPE,\n",
        "        'emotion_classes': Config.EMOTION_CLASSES\n",
        "    }\n",
        "\n",
        "    save_json(results, os.path.join(Config.RESULTS_PATH, 'metrics', 'model_comparison.json'))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in comparison: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "885690e7",
      "metadata": {
        "id": "885690e7"
      },
      "source": [
        "## 13. Prediction Demonstration\n",
        "Test the models on sample inputs and visualize results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd2d9682",
      "metadata": {
        "id": "cd2d9682"
      },
      "outputs": [],
      "source": [
        "def predict_emotion_from_text(text, model, tokenizer):\n",
        "    \"\"\"Predict emotion from text input.\"\"\"\n",
        "    encoded = tokenizer(\n",
        "        [text],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=Config.MAX_TEXT_LENGTH,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    prediction = model.predict([encoded['input_ids'], encoded['attention_mask']])\n",
        "    predicted_class = np.argmax(prediction[0])\n",
        "    confidence = prediction[0][predicted_class]\n",
        "\n",
        "    return Config.EMOTION_CLASSES[predicted_class], confidence\n",
        "\n",
        "def predict_emotion_from_image(image, model):\n",
        "    \"\"\"Predict emotion from image input.\"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    prediction = model.predict(image)\n",
        "    predicted_class = np.argmax(prediction[0])\n",
        "    confidence = prediction[0][predicted_class]\n",
        "\n",
        "    return Config.EMOTION_CLASSES[predicted_class], confidence\n",
        "\n",
        "def predict_multimodal_emotion(image, text, mm_model, tokenizer):\n",
        "    \"\"\"Predict emotion using multimodal input.\"\"\"\n",
        "    # Preprocess text\n",
        "    encoded = tokenizer(\n",
        "        [text],\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=Config.MAX_TEXT_LENGTH,\n",
        "        return_tensors='tf'\n",
        "    )\n",
        "\n",
        "    # Preprocess image\n",
        "    if len(image.shape) == 3:\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    prediction = mm_model.predict([image, encoded['input_ids'], encoded['attention_mask']])\n",
        "    predicted_class = np.argmax(prediction[0])\n",
        "    confidence = prediction[0][predicted_class]\n",
        "\n",
        "    return Config.EMOTION_CLASSES[predicted_class], confidence\n",
        "\n",
        "# Sample texts for demonstration\n",
        "sample_texts = [\n",
        "    \"I am so happy and excited about this amazing news!\",\n",
        "    \"This makes me really angry and frustrated!\",\n",
        "    \"I'm feeling quite sad and disappointed today.\",\n",
        "    \"That's absolutely disgusting and revolting!\",\n",
        "    \"I'm scared and worried about what might happen.\",\n",
        "    \"What a wonderful surprise that was!\"\n",
        "]\n",
        "\n",
        "print(\"Text Emotion Recognition Demonstration:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, text in enumerate(sample_texts):\n",
        "    try:\n",
        "        emotion, confidence = predict_emotion_from_text(text, ter_model.model, ter_model.tokenizer)\n",
        "        print(f\"Text: '{text[:50]}...'\")\n",
        "        print(f\"Predicted Emotion: {emotion.upper()} (Confidence: {confidence:.3f})\")\n",
        "        print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting for text {i}: {e}\")\n",
        "\n",
        "# Image emotion recognition demonstration with dummy data\n",
        "print(\"\\\\nFacial Emotion Recognition Demonstration:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Create sample images (in practice, you'd load real images)\n",
        "sample_images = [\n",
        "    np.random.rand(Config.IMG_HEIGHT, Config.IMG_WIDTH, Config.IMG_CHANNELS) for _ in range(3)\n",
        "]\n",
        "\n",
        "for i, image in enumerate(sample_images):\n",
        "    try:\n",
        "        emotion, confidence = predict_emotion_from_image(image, fer_model.model)\n",
        "        print(f\"Sample Image {i+1}:\")\n",
        "        print(f\"Predicted Emotion: {emotion.upper()} (Confidence: {confidence:.3f})\")\n",
        "        print(\"-\" * 30)\n",
        "    except Exception as e:\n",
        "        print(f\"Error predicting for image {i}: {e}\")\n",
        "\n",
        "# Multimodal demonstration\n",
        "print(\"\\\\nMultimodal Emotion Recognition Demonstration:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i in range(min(3, len(sample_texts))):\n",
        "    try:\n",
        "        emotion, confidence = predict_multimodal_emotion(\n",
        "            sample_images[i], sample_texts[i],\n",
        "            multimodal_model.model, ter_model.tokenizer\n",
        "        )\n",
        "        print(f\"Text: '{sample_texts[i][:50]}...'\")\n",
        "        print(f\"Image: Sample {i+1}\")\n",
        "        print(f\"Multimodal Prediction: {emotion.upper()} (Confidence: {confidence:.3f})\")\n",
        "        print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in multimodal prediction {i}: {e}\")\n",
        "\n",
        "# Create prediction probability visualization\n",
        "def plot_prediction_probabilities(predictions, title, save_path=None):\n",
        "    \"\"\"Plot prediction probabilities for all emotion classes.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = plt.cm.Set3(np.linspace(0, 1, len(Config.EMOTION_CLASSES)))\n",
        "\n",
        "    bars = plt.bar(Config.EMOTION_CLASSES, predictions[0], color=colors)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Emotion Classes')\n",
        "    plt.ylabel('Probability')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, prob in zip(bars, predictions[0]):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{prob:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example prediction probability visualization\n",
        "try:\n",
        "    sample_text = \"I am extremely happy and joyful today!\"\n",
        "    encoded_sample = ter_model.preprocess_texts([sample_text])\n",
        "    sample_prediction = ter_model.model.predict([encoded_sample['input_ids'], encoded_sample['attention_mask']])\n",
        "\n",
        "    plot_prediction_probabilities(\n",
        "        sample_prediction,\n",
        "        f\"TER Prediction Probabilities\\\\nText: '{sample_text}'\",\n",
        "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'sample_prediction_probabilities.png')\n",
        "    )\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error creating probability visualization: {e}\")\n",
        "\n",
        "print(\"\\\\n✅ Prediction demonstration completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99c2378",
      "metadata": {
        "id": "b99c2378"
      },
      "source": [
        "## 14. Conclusion and Next Steps\n",
        "\n",
        "### Summary of Results\n",
        "This notebook demonstrated a comprehensive multimodal emotion recognition system that combines:\n",
        "\n",
        "1. **Facial Emotion Recognition (FER)**: CNN-based model for analyzing facial expressions\n",
        "2. **Text Emotion Recognition (TER)**: DistilBERT-based model for analyzing text sentiment\n",
        "3. **Multimodal Fusion**: Early and late fusion strategies for improved performance\n",
        "\n",
        "### Key Achievements\n",
        "- ✅ Successfully implemented and trained three different models\n",
        "- ✅ Created a robust data pipeline with proper organization\n",
        "- ✅ Demonstrated both early and late fusion techniques\n",
        "- ✅ Provided comprehensive evaluation and comparison\n",
        "- ✅ Built prediction capabilities for real-world usage\n",
        "\n",
        "### Model Performance\n",
        "The multimodal approach typically shows improved performance over individual modalities by leveraging complementary information from both visual and textual inputs.\n",
        "\n",
        "### Next Steps for Improvement\n",
        "1. **Data Enhancement**:\n",
        "   - Collect larger, more diverse datasets\n",
        "   - Implement data augmentation techniques\n",
        "   - Balance emotion class distributions\n",
        "\n",
        "2. **Model Architecture**:\n",
        "   - Experiment with attention mechanisms\n",
        "   - Try different fusion strategies\n",
        "   - Implement ensemble methods\n",
        "\n",
        "3. **Optimization**:\n",
        "   - Hyperparameter tuning\n",
        "   - Model compression for deployment\n",
        "   - Real-time inference optimization\n",
        "\n",
        "4. **Deployment**:\n",
        "   - Create web/mobile applications\n",
        "   - Implement streaming capabilities\n",
        "   - Add real-time video processing\n",
        "\n",
        "### Usage in Production\n",
        "To use these models in production:\n",
        "1. Save trained models to Google Drive or cloud storage\n",
        "2. Load models in your application\n",
        "3. Preprocess inputs according to the training pipeline\n",
        "4. Apply appropriate post-processing to predictions\n",
        "\n",
        "### Contact and Credits\n",
        "- **Author**: Henry\n",
        "- **Date**: July 30, 2025\n",
        "- **Environment**: Google Colab with GPU support\n",
        "- **Frameworks**: TensorFlow, HuggingFace Transformers\n",
        "\n",
        "---\n",
        "**Happy Emotion Recognition! 🎭✨**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b24891f928e4e75b391b9972864d85f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba88ef04d10d4f83bd51b6fd76917b05",
            "placeholder": "​",
            "style": "IPY_MODEL_bd45ecf53f4b47ff9e594bab6d1aa8e6",
            "value": " 483/483 [00:00&lt;00:00, 14.5kB/s]"
          }
        },
        "0bb7386f9ba548978989a611e89c85cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1297516bfb0548e384a5d139235a60c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdc675c6b7944215bc7216f6d3fed954",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a2dabe975f8408a8faaba53c02e1f2f",
            "value": 483
          }
        },
        "17ecf68029404bfabb7735f9c543d81c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c85aba4232bd476aaf209f6b7e268773",
            "placeholder": "​",
            "style": "IPY_MODEL_b7a53409d5ad437696a8660fad6d76a2",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.33kB/s]"
          }
        },
        "1a45d93c25fb4cee89d981300d2a729e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d471d4711e848ff8b91bc552dd9c33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28261fdf6a2e430da380f85382473492": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f386d5b387504ec287cccdd6821f7e00",
            "placeholder": "​",
            "style": "IPY_MODEL_3220d1c94dc349a495bad77278d58ebc",
            "value": "vocab.txt: 100%"
          }
        },
        "2ac5e67f9cbc4db8afa709be0a583da0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c92b36c57774fccbc79f49e5b5ebd55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3220d1c94dc349a495bad77278d58ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a2dabe975f8408a8faaba53c02e1f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41bf980810444cdebec292503ef66961": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c7d183754da4d2280e297d813162d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "621fa8c5a637479bbdcad5784704b84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64ec69e08a924fed8e82f6e0cdc4120d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f19bec67dee4984babced020c550dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c92b36c57774fccbc79f49e5b5ebd55",
            "placeholder": "​",
            "style": "IPY_MODEL_a109e968c78d4494b07b9e01b2062986",
            "value": "config.json: 100%"
          }
        },
        "7d2f1f4108dc444988bc130936e7f7c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8192196aaf904f7d8c5692547c01760e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a45d93c25fb4cee89d981300d2a729e",
            "placeholder": "​",
            "style": "IPY_MODEL_d7a32b300b4341f9bb65e2fb1f869169",
            "value": " 466k/466k [00:00&lt;00:00, 9.27MB/s]"
          }
        },
        "819d9a7fefd44e69a26848511f3ffbd5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "853e8fe021e9472f9c7ebdf3ca9ab583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffa3dfa79da541dd9a67375c4608a8f7",
            "placeholder": "​",
            "style": "IPY_MODEL_4c7d183754da4d2280e297d813162d58",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8baa543ef29541628dbb8708f1a1d54d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d71d26a146149c895c0715d3e7dd913": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64ec69e08a924fed8e82f6e0cdc4120d",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bb7386f9ba548978989a611e89c85cb",
            "value": 48
          }
        },
        "8f345f79051943648453dc8b177f6511": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9154c437c97c4a46bd3c125c3e7c8624": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5658c94ec6a4d398790c2978c0ece00",
            "placeholder": "​",
            "style": "IPY_MODEL_8baa543ef29541628dbb8708f1a1d54d",
            "value": "tokenizer.json: 100%"
          }
        },
        "91f8819e97164942bc807d5d636285bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_853e8fe021e9472f9c7ebdf3ca9ab583",
              "IPY_MODEL_8d71d26a146149c895c0715d3e7dd913",
              "IPY_MODEL_17ecf68029404bfabb7735f9c543d81c"
            ],
            "layout": "IPY_MODEL_8f345f79051943648453dc8b177f6511"
          }
        },
        "95ac3c1ed40141d6ad087c86e52fc50b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a803a80da51413aa4b31d0b4f64e3dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_819d9a7fefd44e69a26848511f3ffbd5",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d2f1f4108dc444988bc130936e7f7c7",
            "value": 231508
          }
        },
        "9f05f03dda84484d9993214b2470a407": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41bf980810444cdebec292503ef66961",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_621fa8c5a637479bbdcad5784704b84c",
            "value": 466062
          }
        },
        "a109e968c78d4494b07b9e01b2062986": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a34d22578abe4f68869f03d76b77de9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df80782a54a847569098ae2a0b42cbe3",
            "placeholder": "​",
            "style": "IPY_MODEL_1d471d4711e848ff8b91bc552dd9c33f",
            "value": " 232k/232k [00:00&lt;00:00, 3.45MB/s]"
          }
        },
        "a77f7a8b9f574bb5839c3e5e5214a0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f19bec67dee4984babced020c550dbc",
              "IPY_MODEL_1297516bfb0548e384a5d139235a60c1",
              "IPY_MODEL_0b24891f928e4e75b391b9972864d85f"
            ],
            "layout": "IPY_MODEL_2ac5e67f9cbc4db8afa709be0a583da0"
          }
        },
        "b7a53409d5ad437696a8660fad6d76a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba88ef04d10d4f83bd51b6fd76917b05": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd45ecf53f4b47ff9e594bab6d1aa8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c85aba4232bd476aaf209f6b7e268773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc675c6b7944215bc7216f6d3fed954": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5658c94ec6a4d398790c2978c0ece00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a32b300b4341f9bb65e2fb1f869169": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df80782a54a847569098ae2a0b42cbe3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f386d5b387504ec287cccdd6821f7e00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dbff2286964081a88e49303ec0f730": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28261fdf6a2e430da380f85382473492",
              "IPY_MODEL_9a803a80da51413aa4b31d0b4f64e3dc",
              "IPY_MODEL_a34d22578abe4f68869f03d76b77de9c"
            ],
            "layout": "IPY_MODEL_95ac3c1ed40141d6ad087c86e52fc50b"
          }
        },
        "facd7a64822447529bc5934faf518a34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb3dfa3b15d94ea0a74587134c69e2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9154c437c97c4a46bd3c125c3e7c8624",
              "IPY_MODEL_9f05f03dda84484d9993214b2470a407",
              "IPY_MODEL_8192196aaf904f7d8c5692547c01760e"
            ],
            "layout": "IPY_MODEL_facd7a64822447529bc5934faf518a34"
          }
        },
        "ffa3dfa79da541dd9a67375c4608a8f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

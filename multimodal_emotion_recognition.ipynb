{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bf6ac6",
   "metadata": {},
   "source": [
    "# Multimodal Emotion Recognition System\n",
    "## Combining Facial and Text Emotion Analysis with Deep Learning\n",
    "\n",
    "This notebook implements a comprehensive multimodal emotion recognition system that combines:\n",
    "- **Facial Emotion Recognition (FER)** using Convolutional Neural Networks\n",
    "- **Text Emotion Recognition (TER)** using DistilBERT transformer\n",
    "- **Multimodal Fusion** with both early and late fusion strategies\n",
    "\n",
    "### Key Features:\n",
    "- üéØ **6 Emotion Classes**: Joy, Anger, Disgust, Sadness, Fear, Surprise\n",
    "- üß† **Advanced Models**: CNN for images, DistilBERT for text\n",
    "- üîó **Fusion Techniques**: Early and late fusion strategies\n",
    "- üìä **Comprehensive Evaluation**: Detailed performance analysis\n",
    "- üöÄ **Google Colab Optimized**: GPU acceleration and easy deployment\n",
    "\n",
    "### Author: Henry\n",
    "### Date: July 30, 2025\n",
    "### Environment: Google Colab with GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95e6d2",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "Let's start by setting up Google Colab environment and installing required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51be7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're running in Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Mount Google Drive for data access\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Set working directory to a folder in your drive\n",
    "    import os\n",
    "    os.chdir('/content/drive/MyDrive/emotion_recognition')\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available for training!\")\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU (training will be slower).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63961d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow>=2.13.0\n",
    "!pip install transformers>=4.21.0\n",
    "!pip install torch>=1.11.0\n",
    "!pip install scikit-learn>=1.1.0\n",
    "!pip install matplotlib>=3.5.0\n",
    "!pip install seaborn>=0.11.0\n",
    "!pip install numpy>=1.21.0\n",
    "!pip install pandas>=1.4.0\n",
    "!pip install pillow>=9.0.0\n",
    "!pip install tqdm>=4.64.0\n",
    "\n",
    "# Restart runtime after installation (uncomment if needed)\n",
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Transformers\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Using GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230c0a3",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants\n",
    "Define all model configurations, hyperparameters, and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067635d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration and Constants\n",
    "class Config:\n",
    "    # Emotion classes\n",
    "    EMOTION_CLASSES = ['joy', 'anger', 'disgust', 'sadness', 'fear', 'surprise']\n",
    "    NUM_CLASSES = len(EMOTION_CLASSES)\n",
    "    \n",
    "    # Image parameters\n",
    "    IMG_HEIGHT = 48\n",
    "    IMG_WIDTH = 48\n",
    "    IMG_CHANNELS = 1  # Grayscale\n",
    "    \n",
    "    # Text parameters\n",
    "    MAX_TEXT_LENGTH = 128\n",
    "    BERT_MODEL = 'distilbert-base-uncased'\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    PATIENCE = 10\n",
    "    \n",
    "    # Data paths (adjust for your Google Drive structure)\n",
    "    BASE_PATH = '/content/drive/MyDrive/emotion_recognition'\n",
    "    DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "    RAW_DATA_PATH = os.path.join(DATA_PATH, 'raw')\n",
    "    PROCESSED_DATA_PATH = os.path.join(DATA_PATH, 'processed')\n",
    "    MODELS_PATH = os.path.join(BASE_PATH, 'models')\n",
    "    RESULTS_PATH = os.path.join(BASE_PATH, 'results')\n",
    "    \n",
    "    # Specific data paths\n",
    "    FER_IMAGES_PATH = os.path.join(RAW_DATA_PATH, 'fer_images')\n",
    "    TEXT_DATA_PATH = os.path.join(RAW_DATA_PATH, 'text_data')\n",
    "    MULTIMODAL_DATA_PATH = os.path.join(RAW_DATA_PATH, 'multimodal_data')\n",
    "    \n",
    "    # Model save paths\n",
    "    FER_MODEL_PATH = os.path.join(MODELS_PATH, 'fer_model.h5')\n",
    "    TER_MODEL_PATH = os.path.join(MODELS_PATH, 'ter_model.h5')\n",
    "    MULTIMODAL_MODEL_PATH = os.path.join(MODELS_PATH, 'multimodal_model.h5')\n",
    "\n",
    "# Print configuration\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Emotion classes: {Config.EMOTION_CLASSES}\")\n",
    "print(f\"Image size: {Config.IMG_HEIGHT}x{Config.IMG_WIDTH}x{Config.IMG_CHANNELS}\")\n",
    "print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {Config.LEARNING_RATE}\")\n",
    "print(f\"Base path: {Config.BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f923878",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n",
    "Helper functions for data processing, visualization, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154d6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"Create necessary directories for the project.\"\"\"\n",
    "    directories = [\n",
    "        Config.DATA_PATH,\n",
    "        Config.RAW_DATA_PATH,\n",
    "        Config.PROCESSED_DATA_PATH,\n",
    "        Config.MODELS_PATH,\n",
    "        Config.RESULTS_PATH,\n",
    "        Config.FER_IMAGES_PATH,\n",
    "        Config.TEXT_DATA_PATH,\n",
    "        Config.MULTIMODAL_DATA_PATH,\n",
    "        os.path.join(Config.RESULTS_PATH, 'metrics'),\n",
    "        os.path.join(Config.RESULTS_PATH, 'plots'),\n",
    "    ]\n",
    "    \n",
    "    # Create emotion subdirectories for FER images\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        for emotion in Config.EMOTION_CLASSES:\n",
    "            directories.append(os.path.join(Config.FER_IMAGES_PATH, split, emotion))\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    print(\"Directory structure created successfully!\")\n",
    "\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"Plot training history for loss and accuracy.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history.history['loss'], label='Training Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, title='Confusion Matrix', save_path=None):\n",
    "    \"\"\"Plot confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, class_names):\n",
    "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    y_pred = np.argmax(predictions, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1) if y_test.ndim > 1 else y_test\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'classification_report': report,\n",
    "        'predictions': predictions,\n",
    "        'y_pred': y_pred,\n",
    "        'y_true': y_true\n",
    "    }\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    \"\"\"Save data as JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"Data saved to {filepath}\")\n",
    "\n",
    "print(\"Utility functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdce3af",
   "metadata": {},
   "source": [
    "## 4. Data Loading and Preprocessing\n",
    "Functions for loading and preprocessing both image and text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb438c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Class for loading and preprocessing data.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(config.BERT_MODEL)\n",
    "    \n",
    "    def load_image_data(self, subset='train'):\n",
    "        \"\"\"Load image data using ImageDataGenerator.\"\"\"\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.2,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "        \n",
    "        # For validation and test, don't apply augmentation\n",
    "        if subset in ['validation', 'test']:\n",
    "            datagen = ImageDataGenerator(rescale=1./255)\n",
    "        \n",
    "        data_path = os.path.join(self.config.FER_IMAGES_PATH, subset)\n",
    "        \n",
    "        if not os.path.exists(data_path):\n",
    "            print(f\"Warning: Path {data_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_image_data(subset)\n",
    "        \n",
    "        generator = datagen.flow_from_directory(\n",
    "            data_path,\n",
    "            target_size=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH),\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "            color_mode='grayscale',\n",
    "            classes=self.config.EMOTION_CLASSES,\n",
    "            shuffle=(subset == 'train')\n",
    "        )\n",
    "        \n",
    "        return generator\n",
    "    \n",
    "    def create_dummy_image_data(self, subset):\n",
    "        \"\"\"Create dummy image data for testing.\"\"\"\n",
    "        print(f\"Creating dummy {subset} data...\")\n",
    "        \n",
    "        # Create dummy images and labels\n",
    "        num_samples = 100 if subset == 'train' else 50\n",
    "        images = np.random.rand(num_samples, self.config.IMG_HEIGHT, \n",
    "                              self.config.IMG_WIDTH, self.config.IMG_CHANNELS)\n",
    "        labels = np.random.randint(0, self.config.NUM_CLASSES, num_samples)\n",
    "        labels = to_categorical(labels, self.config.NUM_CLASSES)\n",
    "        \n",
    "        return images, labels\n",
    "    \n",
    "    def load_text_data(self, file_path):\n",
    "        \"\"\"Load text data from JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_text_data()\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        texts = [item['text'] for item in data]\n",
    "        emotions = [item['emotion'] for item in data]\n",
    "        \n",
    "        # Convert emotion labels to indices\n",
    "        emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.config.EMOTION_CLASSES)}\n",
    "        labels = [emotion_to_idx[emotion] for emotion in emotions]\n",
    "        \n",
    "        return texts, labels\n",
    "    \n",
    "    def create_dummy_text_data(self):\n",
    "        \"\"\"Create dummy text data for testing.\"\"\"\n",
    "        print(\"Creating dummy text data...\")\n",
    "        \n",
    "        dummy_texts = [\n",
    "            \"I am so happy today!\",\n",
    "            \"This makes me really angry.\",\n",
    "            \"That's completely disgusting.\",\n",
    "            \"I feel so sad about this.\",\n",
    "            \"This is really scary.\",\n",
    "            \"What a surprise that was!\"\n",
    "        ] * 50  # Repeat to get more samples\n",
    "        \n",
    "        dummy_labels = list(range(self.config.NUM_CLASSES)) * 50\n",
    "        \n",
    "        return dummy_texts, dummy_labels\n",
    "    \n",
    "    def preprocess_text(self, texts):\n",
    "        \"\"\"Preprocess text data for BERT.\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.config.MAX_TEXT_LENGTH,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return encodings\n",
    "    \n",
    "    def load_multimodal_data(self, file_path):\n",
    "        \"\"\"Load multimodal data from JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_multimodal_data()\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def create_dummy_multimodal_data(self):\n",
    "        \"\"\"Create dummy multimodal data for testing.\"\"\"\n",
    "        print(\"Creating dummy multimodal data...\")\n",
    "        \n",
    "        dummy_data = []\n",
    "        for i in range(300):  # 300 samples\n",
    "            emotion_idx = i % self.config.NUM_CLASSES\n",
    "            emotion = self.config.EMOTION_CLASSES[emotion_idx]\n",
    "            \n",
    "            dummy_data.append({\n",
    "                'image_path': f'dummy_image_{i}.jpg',\n",
    "                'text': f'This is a {emotion} text sample number {i}.',\n",
    "                'emotion': emotion\n",
    "            })\n",
    "        \n",
    "        return dummy_data\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(Config)\n",
    "print(\"Data loader initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab2d66",
   "metadata": {},
   "source": [
    "## 5. Facial Emotion Recognition (FER) Model\n",
    "CNN-based model for facial emotion recognition from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65f770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialEmotionRecognizer:\n",
    "    \"\"\"CNN-based Facial Emotion Recognition model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build CNN model for facial emotion recognition.\"\"\"\n",
    "        inputs = keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS))\n",
    "        \n",
    "        # First convolutional block\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        \n",
    "        # Fourth convolutional block\n",
    "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "        \n",
    "        # Global average pooling and dense layers\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        \n",
    "        # Output layer\n",
    "        outputs = layers.Dense(self.config.NUM_CLASSES, activation='softmax', name='fer_output')(x)\n",
    "        \n",
    "        model = Model(inputs, outputs, name='FacialEmotionRecognizer')\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train(self, train_generator, val_generator):\n",
    "        \"\"\"Train the FER model.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=self.config.PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                self.config.FER_MODEL_PATH,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            validation_data=val_generator,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on input data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built or loaded. Call build_model() or load_model() first.\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        \"\"\"Load a saved model.\"\"\"\n",
    "        path = model_path or self.config.FER_MODEL_PATH\n",
    "        if os.path.exists(path):\n",
    "            self.model = keras.models.load_model(path)\n",
    "            print(f\"FER model loaded from {path}\")\n",
    "        else:\n",
    "            print(f\"Model file not found at {path}\")\n",
    "    \n",
    "    def save_model(self, model_path=None):\n",
    "        \"\"\"Save the current model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
    "        \n",
    "        path = model_path or self.config.FER_MODEL_PATH\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.model.save(path)\n",
    "        print(f\"FER model saved to {path}\")\n",
    "\n",
    "# Initialize FER model\n",
    "fer_model = FacialEmotionRecognizer(Config)\n",
    "print(\"FER model class initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa46bc",
   "metadata": {},
   "source": [
    "## 6. Text Emotion Recognition (TER) Model\n",
    "DistilBERT-based model for text emotion recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613275b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmotionRecognizer:\n",
    "    \"\"\"DistilBERT-based Text Emotion Recognition model.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(config.BERT_MODEL)\n",
    "        self.model = None\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build DistilBERT model for text emotion recognition.\"\"\"\n",
    "        # Load pre-trained DistilBERT\n",
    "        distilbert = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "            self.config.BERT_MODEL,\n",
    "            num_labels=self.config.NUM_CLASSES,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        \n",
    "        # Input layers\n",
    "        input_ids = keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='attention_mask')\n",
    "        \n",
    "        # Get DistilBERT outputs\n",
    "        distilbert_outputs = distilbert([input_ids, attention_mask])\\n        \n",
    "        # Extract logits\n",
    "        logits = distilbert_outputs.logits\n",
    "        \n",
    "        # Apply softmax for probability distribution\n",
    "        outputs = layers.Softmax(name='ter_output')(logits)\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(inputs=[input_ids, attention_mask], outputs=outputs, name='TextEmotionRecognizer')\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def preprocess_texts(self, texts):\n",
    "        \"\"\"Preprocess texts for model input.\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.config.MAX_TEXT_LENGTH,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask']\n",
    "        }\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train the TER model.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=self.config.PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                self.config.TER_MODEL_PATH,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on input data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built or loaded. Call build_model() or load_model() first.\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def load_model(self, model_path=None):\n",
    "        \"\"\"Load a saved model.\"\"\"\n",
    "        path = model_path or self.config.TER_MODEL_PATH\n",
    "        if os.path.exists(path):\n",
    "            self.model = keras.models.load_model(path)\n",
    "            print(f\"TER model loaded from {path}\")\n",
    "        else:\n",
    "            print(f\"Model file not found at {path}\")\n",
    "    \n",
    "    def save_model(self, model_path=None):\n",
    "        \"\"\"Save the current model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
    "        \n",
    "        path = model_path or self.config.TER_MODEL_PATH\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.model.save(path)\n",
    "        print(f\"TER model saved to {path}\")\n",
    "\n",
    "# Initialize TER model\n",
    "ter_model = TextEmotionRecognizer(Config)\n",
    "print(\"TER model class initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061a798",
   "metadata": {},
   "source": [
    "## 7. Multimodal Fusion Model\n",
    "Combining FER and TER models for improved emotion recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da1077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalEmotionRecognizer:\n",
    "    \"\"\"Multimodal emotion recognition combining FER and TER.\"\"\"\n",
    "    \n",
    "    def __init__(self, config, fer_model, ter_model):\n",
    "        self.config = config\n",
    "        self.fer_model = fer_model\n",
    "        self.ter_model = ter_model\n",
    "        self.model = None\n",
    "    \n",
    "    def build_early_fusion_model(self):\n",
    "        \"\"\"Build early fusion model combining image and text features.\"\"\"\n",
    "        # Image input branch\n",
    "        image_input = keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS), name='image_input')\n",
    "        \n",
    "        # Text input branches\n",
    "        text_input_ids = keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_input_ids')\n",
    "        text_attention_mask = keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_attention_mask')\n",
    "        \n",
    "        # Extract image features using FER model backbone\n",
    "        fer_base = self.fer_model.model.layers[:-1]  # Remove final classification layer\n",
    "        image_features = image_input\n",
    "        for layer in fer_base[1:]:  # Skip input layer\n",
    "            if hasattr(layer, 'name') and 'fer_output' not in layer.name:\n",
    "                image_features = layer(image_features)\n",
    "        \n",
    "        # Extract text features using TER model backbone\n",
    "        ter_base = self.ter_model.model.layers[2]  # DistilBERT layer\n",
    "        text_features = ter_base([text_input_ids, text_attention_mask]).last_hidden_state\n",
    "        text_features = layers.GlobalAveragePooling1D()(text_features)\n",
    "        \n",
    "        # Fusion layer\n",
    "        combined_features = layers.Concatenate(name='feature_fusion')([image_features, text_features])\\n        \n",
    "        # Additional fusion layers\n",
    "        x = layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001))(combined_features)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        \n",
    "        # Final classification layer\n",
    "        outputs = layers.Dense(self.config.NUM_CLASSES, activation='softmax', name='multimodal_output')(x)\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(\n",
    "            inputs=[image_input, text_input_ids, text_attention_mask],\n",
    "            outputs=outputs,\n",
    "            name='MultimodalEmotionRecognizer_EarlyFusion'\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def build_late_fusion_model(self):\n",
    "        \"\"\"Build late fusion model combining FER and TER predictions.\"\"\"\n",
    "        # Image input\n",
    "        image_input = keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS), name='image_input')\n",
    "        \n",
    "        # Text inputs\n",
    "        text_input_ids = keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_input_ids')\n",
    "        text_attention_mask = keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_attention_mask')\n",
    "        \n",
    "        # Get FER predictions\n",
    "        fer_predictions = self.fer_model.model(image_input)\n",
    "        \n",
    "        # Get TER predictions\n",
    "        ter_predictions = self.ter_model.model([text_input_ids, text_attention_mask])\n",
    "        \n",
    "        # Weighted fusion of predictions\n",
    "        fusion_weights = layers.Dense(2, activation='softmax', name='fusion_weights')(\n",
    "            layers.Concatenate()([\n",
    "                layers.GlobalAveragePooling2D()(self.fer_model.model.layers[-3].output),  # FER features\n",
    "                layers.Dense(256, activation='relu')(ter_predictions)  # TER features\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        # Apply weights\n",
    "        weighted_fer = layers.Multiply()([fer_predictions, layers.Lambda(lambda x: x[:, 0:1])(fusion_weights)])\n",
    "        weighted_ter = layers.Multiply()([ter_predictions, layers.Lambda(lambda x: x[:, 1:2])(fusion_weights)])\n",
    "        \n",
    "        # Final prediction\n",
    "        outputs = layers.Add(name='multimodal_output')([weighted_fer, weighted_ter])\n",
    "        \n",
    "        # Create model\n",
    "        model = Model(\n",
    "            inputs=[image_input, text_input_ids, text_attention_mask],\n",
    "            outputs=outputs,\n",
    "            name='MultimodalEmotionRecognizer_LateFusion'\n",
    "        )\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def simple_average_fusion(self, fer_predictions, ter_predictions):\n",
    "        \"\"\"Simple average fusion of FER and TER predictions.\"\"\"\n",
    "        return (fer_predictions + ter_predictions) / 2\n",
    "    \n",
    "    def weighted_average_fusion(self, fer_predictions, ter_predictions, fer_weight=0.6):\n",
    "        \"\"\"Weighted average fusion of FER and TER predictions.\"\"\"\n",
    "        ter_weight = 1 - fer_weight\n",
    "        return fer_weight * fer_predictions + ter_weight * ter_predictions\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, fusion_type='early'):\n",
    "        \"\"\"Train the multimodal model.\"\"\"\n",
    "        if fusion_type == 'early':\n",
    "            self.build_early_fusion_model()\n",
    "        elif fusion_type == 'late':\n",
    "            self.build_late_fusion_model()\n",
    "        else:\n",
    "            raise ValueError(\"fusion_type must be 'early' or 'late'\")\n",
    "        \n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=self.config.PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                self.config.MULTIMODAL_MODEL_PATH,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on input data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built or loaded. Call build_*_fusion_model() first.\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def save_model(self, model_path=None):\n",
    "        \"\"\"Save the current model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
    "        \n",
    "        path = model_path or self.config.MULTIMODAL_MODEL_PATH\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.model.save(path)\n",
    "        print(f\"Multimodal model saved to {path}\")\n",
    "\n",
    "print(\"Multimodal fusion model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dccfc8",
   "metadata": {},
   "source": [
    "## 8. Setup Directory Structure\n",
    "Create the necessary directory structure for data organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d8bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure\n",
    "create_directories()\n",
    "\n",
    "# Display the created directory structure\n",
    "def display_directory_structure(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Display directory structure in a tree format.\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print(f\"{prefix}üìÅ {os.path.basename(path)} (will be created)\")\n",
    "        return\n",
    "    \n",
    "    items = sorted(os.listdir(path))\n",
    "    for i, item in enumerate(items):\n",
    "        item_path = os.path.join(path, item)\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"‚îî‚îÄ‚îÄ \" if is_last else \"‚îú‚îÄ‚îÄ \"\n",
    "        \n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{prefix}{current_prefix}üìÅ {item}/\")\n",
    "            extension = \"    \" if is_last else \"‚îÇ   \"\n",
    "            display_directory_structure(item_path, prefix + extension, max_depth, current_depth + 1)\n",
    "        else:\n",
    "            print(f\"{prefix}{current_prefix}üìÑ {item}\")\n",
    "\n",
    "print(\"\\\\nüìÅ Project Directory Structure:\")\n",
    "print(\"emotion_recognition/\")\n",
    "display_directory_structure(Config.BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c633195",
   "metadata": {},
   "source": [
    "## 9. Train Facial Emotion Recognition Model\n",
    "Train the CNN-based FER model on facial expression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b451a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FER data\n",
    "print(\"Loading FER training data...\")\n",
    "try:\n",
    "    train_generator = data_loader.load_image_data('train')\n",
    "    val_generator = data_loader.load_image_data('validation')\n",
    "    test_generator = data_loader.load_image_data('test')\n",
    "    \n",
    "    print(f\"Training samples: {train_generator.samples if hasattr(train_generator, 'samples') else 'Using dummy data'}\")\n",
    "    print(f\"Validation samples: {val_generator.samples if hasattr(val_generator, 'samples') else 'Using dummy data'}\")\n",
    "    print(f\"Test samples: {test_generator.samples if hasattr(test_generator, 'samples') else 'Using dummy data'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading FER data: {e}\")\n",
    "    print(\"Using dummy data for demonstration...\")\n",
    "    \n",
    "    # Create dummy data\n",
    "    train_images, train_labels = data_loader.create_dummy_image_data('train')\n",
    "    val_images, val_labels = data_loader.create_dummy_image_data('validation')\n",
    "    test_images, test_labels = data_loader.create_dummy_image_data('test')\n",
    "    \n",
    "    train_generator = (train_images, train_labels)\n",
    "    val_generator = (val_images, val_labels)\n",
    "    test_generator = (test_images, test_labels)\n",
    "\n",
    "# Build and display FER model architecture\n",
    "print(\"\\\\nBuilding FER model...\")\n",
    "fer_model.build_model()\n",
    "print(fer_model.model.summary())\n",
    "\n",
    "# Visualize model architecture (optional)\n",
    "tf.keras.utils.plot_model(\n",
    "    fer_model.model, \n",
    "    to_file=os.path.join(Config.RESULTS_PATH, 'fer_model_architecture.png'),\n",
    "    show_shapes=True, \n",
    "    show_layer_names=True\n",
    ")\n",
    "print(\"Model architecture saved to results/fer_model_architecture.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ec2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train FER model\n",
    "print(\"Training FER model...\")\n",
    "print(\"This may take a while depending on your dataset size and hardware...\")\n",
    "\n",
    "try:\n",
    "    if hasattr(train_generator, 'samples'):  # Real data generator\n",
    "        fer_history = fer_model.train(train_generator, val_generator)\n",
    "    else:  # Dummy data\n",
    "        fer_history = fer_model.model.fit(\n",
    "            train_generator[0], train_generator[1],\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            epochs=min(5, Config.EPOCHS),  # Reduced epochs for dummy data\n",
    "            validation_data=(val_generator[0], val_generator[1]),\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    print(\"\\\\n‚úÖ FER model training completed!\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        fer_history, \n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'fer_training_history.png')\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    fer_model.save_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during FER training: {e}\")\n",
    "    print(\"Building model without training for demonstration...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ef163",
   "metadata": {},
   "source": [
    "## 10. Train Text Emotion Recognition Model\n",
    "Train the DistilBERT-based TER model on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dcbc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text data\n",
    "print(\"Loading text data...\")\n",
    "try:\n",
    "    train_texts, train_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'train_text_data.json')\n",
    "    )\n",
    "    val_texts, val_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'val_text_data.json')\n",
    "    )\n",
    "    test_texts, test_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'test_text_data.json')\n",
    "    )\n",
    "    \n",
    "    print(f\"Training text samples: {len(train_texts)}\")\n",
    "    print(f\"Validation text samples: {len(val_texts)}\")\n",
    "    print(f\"Test text samples: {len(test_texts)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading text data: {e}\")\n",
    "    print(\"Using dummy text data for demonstration...\")\n",
    "    \n",
    "    # Create dummy text data\n",
    "    train_texts, train_text_labels = data_loader.create_dummy_text_data()\n",
    "    val_texts, val_text_labels = data_loader.create_dummy_text_data()\n",
    "    test_texts, test_text_labels = data_loader.create_dummy_text_data()\n",
    "\n",
    "# Preprocess text data\n",
    "print(\"\\\\nPreprocessing text data...\")\n",
    "train_text_encoded = ter_model.preprocess_texts(train_texts)\n",
    "val_text_encoded = ter_model.preprocess_texts(val_texts)\n",
    "test_text_encoded = ter_model.preprocess_texts(test_texts)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "train_text_labels = np.array(train_text_labels)\n",
    "val_text_labels = np.array(val_text_labels)\n",
    "test_text_labels = np.array(test_text_labels)\n",
    "\n",
    "print(f\"Text encoding shape: {train_text_encoded['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {train_text_labels.shape}\")\n",
    "\n",
    "# Display some sample texts\n",
    "print(\"\\\\nSample texts:\")\n",
    "for i in range(min(3, len(train_texts))):\n",
    "    emotion = Config.EMOTION_CLASSES[train_text_labels[i]]\n",
    "    print(f\"  {emotion}: '{train_texts[i][:100]}...'\")\n",
    "\n",
    "# Build TER model\n",
    "print(\"\\\\nBuilding TER model...\")\n",
    "ter_model.build_model()\n",
    "print(ter_model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ed2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TER model\n",
    "print(\"Training TER model...\")\n",
    "print(\"This may take a while, especially for BERT-based models...\")\n",
    "\n",
    "try:\n",
    "    ter_history = ter_model.train(\n",
    "        train_text_encoded, train_text_labels,\n",
    "        val_text_encoded, val_text_labels\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n‚úÖ TER model training completed!\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        ter_history,\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'ter_training_history.png')\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    ter_model.save_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during TER training: {e}\")\n",
    "    print(\"Building model without training for demonstration...\")\n",
    "    ter_model.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2893bb1",
   "metadata": {},
   "source": [
    "## 11. Train Multimodal Fusion Model\n",
    "Combine FER and TER models for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb21d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multimodal data\n",
    "print(\"Loading multimodal data...\")\n",
    "try:\n",
    "    multimodal_data = data_loader.load_multimodal_data(\n",
    "        os.path.join(Config.MULTIMODAL_DATA_PATH, 'multimodal_dataset.json')\n",
    "    )\n",
    "    print(f\"Multimodal samples: {len(multimodal_data)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading multimodal data: {e}\")\n",
    "    print(\"Using dummy multimodal data for demonstration...\")\n",
    "    multimodal_data = data_loader.create_dummy_multimodal_data()\n",
    "\n",
    "# Prepare multimodal data\n",
    "print(\"\\\\nPreparing multimodal data...\")\n",
    "\n",
    "# Create dummy aligned data for demonstration\n",
    "# In real scenario, you would load actual images corresponding to texts\n",
    "num_samples = len(multimodal_data)\n",
    "multimodal_images = np.random.rand(num_samples, Config.IMG_HEIGHT, Config.IMG_WIDTH, Config.IMG_CHANNELS)\n",
    "multimodal_texts = [item['text'] for item in multimodal_data]\n",
    "multimodal_labels = [Config.EMOTION_CLASSES.index(item['emotion']) for item in multimodal_data]\n",
    "\n",
    "# Preprocess multimodal text data\n",
    "multimodal_text_encoded = ter_model.preprocess_texts(multimodal_texts)\n",
    "\n",
    "# Convert labels to categorical\n",
    "multimodal_labels_categorical = to_categorical(multimodal_labels, Config.NUM_CLASSES)\n",
    "\n",
    "# Split data\n",
    "split_idx = int(0.8 * num_samples)\n",
    "val_split_idx = int(0.9 * num_samples)\n",
    "\n",
    "# Training data\n",
    "train_mm_images = multimodal_images[:split_idx]\n",
    "train_mm_text_ids = multimodal_text_encoded['input_ids'][:split_idx]\n",
    "train_mm_text_mask = multimodal_text_encoded['attention_mask'][:split_idx]\n",
    "train_mm_labels = multimodal_labels_categorical[:split_idx]\n",
    "\n",
    "# Validation data\n",
    "val_mm_images = multimodal_images[split_idx:val_split_idx]\n",
    "val_mm_text_ids = multimodal_text_encoded['input_ids'][split_idx:val_split_idx]\n",
    "val_mm_text_mask = multimodal_text_encoded['attention_mask'][split_idx:val_split_idx]\n",
    "val_mm_labels = multimodal_labels_categorical[split_idx:val_split_idx]\n",
    "\n",
    "print(f\"Multimodal training samples: {len(train_mm_images)}\")\n",
    "print(f\"Multimodal validation samples: {len(val_mm_images)}\")\n",
    "\n",
    "# Initialize multimodal model\n",
    "multimodal_model = MultimodalEmotionRecognizer(Config, fer_model, ter_model)\n",
    "\n",
    "# Choose fusion strategy\n",
    "FUSION_TYPE = 'early'  # Change to 'late' for late fusion\n",
    "print(f\"\\\\nBuilding {FUSION_TYPE} fusion model...\")\n",
    "\n",
    "try:\n",
    "    if FUSION_TYPE == 'early':\n",
    "        multimodal_model.build_early_fusion_model()\n",
    "    else:\n",
    "        multimodal_model.build_late_fusion_model()\n",
    "    \n",
    "    print(multimodal_model.model.summary())\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train_mm = [train_mm_images, train_mm_text_ids, train_mm_text_mask]\n",
    "    X_val_mm = [val_mm_images, val_mm_text_ids, val_mm_text_mask]\n",
    "    \n",
    "    # Train multimodal model (reduced epochs for demonstration)\n",
    "    print(f\"\\\\nTraining {FUSION_TYPE} fusion model...\")\n",
    "    mm_history = multimodal_model.train(\n",
    "        X_train_mm, train_mm_labels,\n",
    "        X_val_mm, val_mm_labels,\n",
    "        fusion_type=FUSION_TYPE\n",
    "    )\n",
    "    \n",
    "    print(\"\\\\n‚úÖ Multimodal model training completed!\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        mm_history,\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', f'multimodal_{FUSION_TYPE}_training_history.png')\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    multimodal_model.save_model()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during multimodal training: {e}\")\n",
    "    print(\"Building model without training for demonstration...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5accda6",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation and Comparison\n",
    "Evaluate and compare the performance of FER, TER, and multimodal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ae47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FER Model\n",
    "print(\"Evaluating FER Model...\")\n",
    "try:\n",
    "    if hasattr(test_generator, 'samples'):  # Real data\n",
    "        fer_test_loss, fer_test_acc = fer_model.model.evaluate(test_generator, verbose=0)\n",
    "        # Get predictions for confusion matrix\n",
    "        test_predictions_fer = fer_model.model.predict(test_generator)\n",
    "        test_labels_fer = test_generator.classes\n",
    "    else:  # Dummy data\n",
    "        fer_test_loss, fer_test_acc = fer_model.model.evaluate(\n",
    "            test_generator[0], test_generator[1], verbose=0\n",
    "        )\n",
    "        test_predictions_fer = fer_model.model.predict(test_generator[0])\n",
    "        test_labels_fer = np.argmax(test_generator[1], axis=1)\n",
    "    \n",
    "    fer_pred_classes = np.argmax(test_predictions_fer, axis=1)\n",
    "    \n",
    "    print(f\"FER Test Accuracy: {fer_test_acc:.4f}\")\n",
    "    print(f\"FER Test Loss: {fer_test_loss:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix for FER\n",
    "    plot_confusion_matrix(\n",
    "        test_labels_fer, fer_pred_classes, Config.EMOTION_CLASSES,\n",
    "        title='FER Model - Confusion Matrix',\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'fer_confusion_matrix.png')\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating FER model: {e}\")\n",
    "\n",
    "# Evaluate TER Model\n",
    "print(\"\\\\nEvaluating TER Model...\")\n",
    "try:\n",
    "    ter_test_loss, ter_test_acc = ter_model.model.evaluate(\n",
    "        test_text_encoded, test_text_labels, verbose=0\n",
    "    )\n",
    "    test_predictions_ter = ter_model.model.predict(test_text_encoded)\n",
    "    ter_pred_classes = np.argmax(test_predictions_ter, axis=1)\n",
    "    \n",
    "    print(f\"TER Test Accuracy: {ter_test_acc:.4f}\")\n",
    "    print(f\"TER Test Loss: {ter_test_loss:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix for TER\n",
    "    plot_confusion_matrix(\n",
    "        test_text_labels, ter_pred_classes, Config.EMOTION_CLASSES,\n",
    "        title='TER Model - Confusion Matrix',\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'ter_confusion_matrix.png')\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating TER model: {e}\")\n",
    "\n",
    "# Evaluate Multimodal Model\n",
    "print(\"\\\\nEvaluating Multimodal Model...\")\n",
    "try:\n",
    "    # Prepare test data for multimodal model\n",
    "    test_mm_images = multimodal_images[val_split_idx:]\n",
    "    test_mm_text_ids = multimodal_text_encoded['input_ids'][val_split_idx:]\n",
    "    test_mm_text_mask = multimodal_text_encoded['attention_mask'][val_split_idx:]\n",
    "    test_mm_labels = multimodal_labels_categorical[val_split_idx:]\n",
    "    \n",
    "    X_test_mm = [test_mm_images, test_mm_text_ids, test_mm_text_mask]\n",
    "    \n",
    "    mm_test_loss, mm_test_acc = multimodal_model.model.evaluate(\n",
    "        X_test_mm, test_mm_labels, verbose=0\n",
    "    )\n",
    "    test_predictions_mm = multimodal_model.model.predict(X_test_mm)\n",
    "    mm_pred_classes = np.argmax(test_predictions_mm, axis=1)\n",
    "    test_labels_mm = np.argmax(test_mm_labels, axis=1)\n",
    "    \n",
    "    print(f\"Multimodal Test Accuracy: {mm_test_acc:.4f}\")\n",
    "    print(f\"Multimodal Test Loss: {mm_test_loss:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix for multimodal\n",
    "    plot_confusion_matrix(\n",
    "        test_labels_mm, mm_pred_classes, Config.EMOTION_CLASSES,\n",
    "        title=f'Multimodal ({FUSION_TYPE.title()} Fusion) - Confusion Matrix',\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', f'multimodal_{FUSION_TYPE}_confusion_matrix.png')\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating multimodal model: {e}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "try:\n",
    "    print(f\"FER Model:        {fer_test_acc:.4f}\")\n",
    "    print(f\"TER Model:        {ter_test_acc:.4f}\")\n",
    "    print(f\"Multimodal Model: {mm_test_acc:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'fer_accuracy': float(fer_test_acc),\n",
    "        'ter_accuracy': float(ter_test_acc),\n",
    "        'multimodal_accuracy': float(mm_test_acc),\n",
    "        'fusion_type': FUSION_TYPE,\n",
    "        'emotion_classes': Config.EMOTION_CLASSES\n",
    "    }\n",
    "    \n",
    "    save_json(results, os.path.join(Config.RESULTS_PATH, 'metrics', 'model_comparison.json'))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in comparison: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885690e7",
   "metadata": {},
   "source": [
    "## 13. Prediction Demonstration\n",
    "Test the models on sample inputs and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2d9682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion_from_text(text, model, tokenizer):\n",
    "    \"\"\"Predict emotion from text input.\"\"\"\n",
    "    encoded = tokenizer(\n",
    "        [text],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=Config.MAX_TEXT_LENGTH,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    prediction = model.predict([encoded['input_ids'], encoded['attention_mask']])\n",
    "    predicted_class = np.argmax(prediction[0])\n",
    "    confidence = prediction[0][predicted_class]\n",
    "    \n",
    "    return Config.EMOTION_CLASSES[predicted_class], confidence\n",
    "\n",
    "def predict_emotion_from_image(image, model):\n",
    "    \"\"\"Predict emotion from image input.\"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    prediction = model.predict(image)\n",
    "    predicted_class = np.argmax(prediction[0])\n",
    "    confidence = prediction[0][predicted_class]\n",
    "    \n",
    "    return Config.EMOTION_CLASSES[predicted_class], confidence\n",
    "\n",
    "def predict_multimodal_emotion(image, text, mm_model, tokenizer):\n",
    "    \"\"\"Predict emotion using multimodal input.\"\"\"\n",
    "    # Preprocess text\n",
    "    encoded = tokenizer(\n",
    "        [text],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=Config.MAX_TEXT_LENGTH,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    \n",
    "    # Preprocess image\n",
    "    if len(image.shape) == 3:\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = mm_model.predict([image, encoded['input_ids'], encoded['attention_mask']])\n",
    "    predicted_class = np.argmax(prediction[0])\n",
    "    confidence = prediction[0][predicted_class]\n",
    "    \n",
    "    return Config.EMOTION_CLASSES[predicted_class], confidence\n",
    "\n",
    "# Sample texts for demonstration\n",
    "sample_texts = [\n",
    "    \"I am so happy and excited about this amazing news!\",\n",
    "    \"This makes me really angry and frustrated!\",\n",
    "    \"I'm feeling quite sad and disappointed today.\",\n",
    "    \"That's absolutely disgusting and revolting!\",\n",
    "    \"I'm scared and worried about what might happen.\",\n",
    "    \"What a wonderful surprise that was!\"\n",
    "]\n",
    "\n",
    "print(\"Text Emotion Recognition Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(sample_texts):\n",
    "    try:\n",
    "        emotion, confidence = predict_emotion_from_text(text, ter_model.model, ter_model.tokenizer)\n",
    "        print(f\"Text: '{text[:50]}...'\")\n",
    "        print(f\"Predicted Emotion: {emotion.upper()} (Confidence: {confidence:.3f})\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for text {i}: {e}\")\n",
    "\n",
    "# Image emotion recognition demonstration with dummy data\n",
    "print(\"\\\\nFacial Emotion Recognition Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create sample images (in practice, you'd load real images)\n",
    "sample_images = [\n",
    "    np.random.rand(Config.IMG_HEIGHT, Config.IMG_WIDTH, Config.IMG_CHANNELS) for _ in range(3)\n",
    "]\n",
    "\n",
    "for i, image in enumerate(sample_images):\n",
    "    try:\n",
    "        emotion, confidence = predict_emotion_from_image(image, fer_model.model)\n",
    "        print(f\"Sample Image {i+1}:\")\n",
    "        print(f\"Predicted Emotion: {emotion.upper()} (Confidence: {confidence:.3f})\")\n",
    "        print(\"-\" * 30)\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting for image {i}: {e}\")\n",
    "\n",
    "# Multimodal demonstration\n",
    "print(\"\\\\nMultimodal Emotion Recognition Demonstration:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(min(3, len(sample_texts))):\n",
    "    try:\n",
    "        emotion, confidence = predict_multimodal_emotion(\n",
    "            sample_images[i], sample_texts[i], \n",
    "            multimodal_model.model, ter_model.tokenizer\n",
    "        )\n",
    "        print(f\"Text: '{sample_texts[i][:50]}...'\")\n",
    "        print(f\"Image: Sample {i+1}\")\n",
    "        print(f\"Multimodal Prediction: {emotion.upper()} (Confidence: {confidence:.3f})\")\n",
    "        print(\"-\" * 50)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in multimodal prediction {i}: {e}\")\n",
    "\n",
    "# Create prediction probability visualization\n",
    "def plot_prediction_probabilities(predictions, title, save_path=None):\n",
    "    \"\"\"Plot prediction probabilities for all emotion classes.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(Config.EMOTION_CLASSES)))\n",
    "    \n",
    "    bars = plt.bar(Config.EMOTION_CLASSES, predictions[0], color=colors)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Emotion Classes')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, prob in zip(bars, predictions[0]):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{prob:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example prediction probability visualization\n",
    "try:\n",
    "    sample_text = \"I am extremely happy and joyful today!\"\n",
    "    encoded_sample = ter_model.preprocess_texts([sample_text])\n",
    "    sample_prediction = ter_model.model.predict([encoded_sample['input_ids'], encoded_sample['attention_mask']])\n",
    "    \n",
    "    plot_prediction_probabilities(\n",
    "        sample_prediction,\n",
    "        f\"TER Prediction Probabilities\\\\nText: '{sample_text}'\",\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'sample_prediction_probabilities.png')\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating probability visualization: {e}\")\n",
    "\n",
    "print(\"\\\\n‚úÖ Prediction demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c2378",
   "metadata": {},
   "source": [
    "## 14. Conclusion and Next Steps\n",
    "\n",
    "### Summary of Results\n",
    "This notebook demonstrated a comprehensive multimodal emotion recognition system that combines:\n",
    "\n",
    "1. **Facial Emotion Recognition (FER)**: CNN-based model for analyzing facial expressions\n",
    "2. **Text Emotion Recognition (TER)**: DistilBERT-based model for analyzing text sentiment\n",
    "3. **Multimodal Fusion**: Early and late fusion strategies for improved performance\n",
    "\n",
    "### Key Achievements\n",
    "- ‚úÖ Successfully implemented and trained three different models\n",
    "- ‚úÖ Created a robust data pipeline with proper organization\n",
    "- ‚úÖ Demonstrated both early and late fusion techniques\n",
    "- ‚úÖ Provided comprehensive evaluation and comparison\n",
    "- ‚úÖ Built prediction capabilities for real-world usage\n",
    "\n",
    "### Model Performance\n",
    "The multimodal approach typically shows improved performance over individual modalities by leveraging complementary information from both visual and textual inputs.\n",
    "\n",
    "### Next Steps for Improvement\n",
    "1. **Data Enhancement**:\n",
    "   - Collect larger, more diverse datasets\n",
    "   - Implement data augmentation techniques\n",
    "   - Balance emotion class distributions\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Experiment with attention mechanisms\n",
    "   - Try different fusion strategies\n",
    "   - Implement ensemble methods\n",
    "\n",
    "3. **Optimization**:\n",
    "   - Hyperparameter tuning\n",
    "   - Model compression for deployment\n",
    "   - Real-time inference optimization\n",
    "\n",
    "4. **Deployment**:\n",
    "   - Create web/mobile applications\n",
    "   - Implement streaming capabilities\n",
    "   - Add real-time video processing\n",
    "\n",
    "### Usage in Production\n",
    "To use these models in production:\n",
    "1. Save trained models to Google Drive or cloud storage\n",
    "2. Load models in your application\n",
    "3. Preprocess inputs according to the training pipeline\n",
    "4. Apply appropriate post-processing to predictions\n",
    "\n",
    "### Contact and Credits\n",
    "- **Author**: Henry\n",
    "- **Date**: July 30, 2025\n",
    "- **Environment**: Google Colab with GPU support\n",
    "- **Frameworks**: TensorFlow, HuggingFace Transformers\n",
    "\n",
    "---\n",
    "**Happy Emotion Recognition! üé≠‚ú®**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

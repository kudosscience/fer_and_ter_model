{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89bf6ac6",
   "metadata": {
    "id": "89bf6ac6"
   },
   "source": [
    "# Multimodal Emotion Recognition System\n",
    "## Combining Facial and Text Emotion Analysis with Deep Learning\n",
    "\n",
    "This notebook implements a comprehensive multimodal emotion recognition system that combines:\n",
    "- **Facial Emotion Recognition (FER)** using Convolutional Neural Networks\n",
    "- **Text Emotion Recognition (TER)** using DistilBERT transformer\n",
    "- **Multimodal Fusion** with both early and late fusion strategies\n",
    "\n",
    "### Key Features:\n",
    "- 🎯 **6 Emotion Classes**: Joy, Anger, Disgust, Sadness, Fear, Surprise\n",
    "- 🧠 **Advanced Models**: CNN for images, DistilBERT for text\n",
    "- 🔗 **Fusion Techniques**: Early and late fusion strategies\n",
    "- 📊 **Comprehensive Evaluation**: Detailed performance analysis\n",
    "- 🚀 **Google Colab Optimized**: GPU acceleration and easy deployment\n",
    "\n",
    "### Author: Henry Ward\n",
    "### Date: July 30, 2025\n",
    "### Environment: Google Colab with GPU support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c95e6d2",
   "metadata": {
    "id": "2c95e6d2"
   },
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "Let's start by setting up Google Colab environment and installing required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51be7d35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51be7d35",
    "outputId": "b6a008e0-cc52-4076-8c44-fa294cd937ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in local environment\n",
      "TensorFlow version: 2.19.0\n",
      "GPU available: []\n",
      "No GPU found. Using CPU (training will be slower).\n",
      "TensorFlow version: 2.19.0\n",
      "GPU available: []\n",
      "No GPU found. Using CPU (training will be slower).\n"
     ]
    }
   ],
   "source": [
    "# Check if we're running in Google Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running in Google Colab\")\n",
    "    # Mount Google Drive for data access\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Set working directory to a folder in your drive\n",
    "    import os\n",
    "    os.chdir('/content/drive/MyDrive/emotion_recognition')\n",
    "    print(f\"Working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"GPU is available for training!\")\n",
    "else:\n",
    "    print(\"No GPU found. Using CPU (training will be slower).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63961d2",
   "metadata": {
    "id": "b63961d2"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow>=2.13.0\n",
    "!pip install transformers>=4.21.0\n",
    "!pip install torch>=1.11.0\n",
    "!pip install scikit-learn>=1.1.0\n",
    "!pip install matplotlib>=3.5.0\n",
    "!pip install seaborn>=0.11.0\n",
    "!pip install numpy>=1.21.0\n",
    "!pip install pandas>=1.4.0\n",
    "!pip install pillow>=9.0.0\n",
    "!pip install tqdm>=4.64.0\n",
    "\n",
    "# Restart runtime after installation (uncomment if needed)\n",
    "# import os\n",
    "# os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c26538e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3c26538e",
    "outputId": "ff9fde9c-83e0-4b7b-98f5-54ffefafa9e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "All libraries imported successfully!\n",
      "TensorFlow version: 2.19.0\n",
      "Using GPU: False\n",
      "All libraries imported successfully!\n",
      "TensorFlow version: 2.19.0\n",
      "Using GPU: False\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow and Keras - Fix for compatibility\n",
    "import tensorflow as tf\n",
    "\n",
    "# Use tf.keras explicitly to avoid version conflicts\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Transformers\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Using GPU: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230c0a3",
   "metadata": {
    "id": "8230c0a3"
   },
   "source": [
    "## 2. Configuration and Constants\n",
    "Define all model configurations, hyperparameters, and constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "067635d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "067635d1",
    "outputId": "f7ddf1a6-c555-4525-ee51-b0efd8a0b13a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "Emotion classes: ['joy', 'anger', 'disgust', 'sadness', 'fear', 'surprise']\n",
      "Image size: 48x48x1\n",
      "Batch size: 32\n",
      "Learning rate: 0.001\n",
      "Base path: /content/drive/MyDrive/emotion_recognition\n"
     ]
    }
   ],
   "source": [
    "# Configuration and Constants\n",
    "class Config:\n",
    "    # Emotion classes\n",
    "    EMOTION_CLASSES = ['joy', 'anger', 'disgust', 'sadness', 'fear', 'surprise']\n",
    "    NUM_CLASSES = len(EMOTION_CLASSES)\n",
    "\n",
    "    # Image parameters\n",
    "    IMG_HEIGHT = 48\n",
    "    IMG_WIDTH = 48\n",
    "    IMG_CHANNELS = 1  # Grayscale\n",
    "\n",
    "    # Text parameters\n",
    "    MAX_TEXT_LENGTH = 128\n",
    "    BERT_MODEL = 'distilbert-base-uncased'\n",
    "\n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    PATIENCE = 10\n",
    "\n",
    "    # Data paths (adjust for your Google Drive structure)\n",
    "    BASE_PATH = '/content/drive/MyDrive/emotion_recognition'\n",
    "    DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "    RAW_DATA_PATH = os.path.join(DATA_PATH, 'raw')\n",
    "    PROCESSED_DATA_PATH = os.path.join(DATA_PATH, 'processed')\n",
    "    MODELS_PATH = os.path.join(BASE_PATH, 'models')\n",
    "    RESULTS_PATH = os.path.join(BASE_PATH, 'results')\n",
    "\n",
    "    # Specific data paths\n",
    "    FER_IMAGES_PATH = os.path.join(RAW_DATA_PATH, 'fer_images')\n",
    "    TEXT_DATA_PATH = os.path.join(RAW_DATA_PATH, 'text_data')\n",
    "    MULTIMODAL_DATA_PATH = os.path.join(RAW_DATA_PATH, 'multimodal_data')\n",
    "\n",
    "    # Model save paths\n",
    "    FER_MODEL_PATH = os.path.join(MODELS_PATH, 'fer_model.h5')\n",
    "    TER_MODEL_PATH = os.path.join(MODELS_PATH, 'ter_model.h5')\n",
    "    MULTIMODAL_MODEL_PATH = os.path.join(MODELS_PATH, 'multimodal_model.h5')\n",
    "\n",
    "# Print configuration\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Emotion classes: {Config.EMOTION_CLASSES}\")\n",
    "print(f\"Image size: {Config.IMG_HEIGHT}x{Config.IMG_WIDTH}x{Config.IMG_CHANNELS}\")\n",
    "print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {Config.LEARNING_RATE}\")\n",
    "print(f\"Base path: {Config.BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f923878",
   "metadata": {
    "id": "0f923878"
   },
   "source": [
    "## 3. Utility Functions\n",
    "Helper functions for data processing, visualization, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154d6dba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "154d6dba",
    "outputId": "8af1bf68-feac-49e0-d6b9-dddd3d16c746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Data loading and preprocessing utilities.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # Initialize tokenizer for text preprocessing\n",
    "        try:\n",
    "            from transformers import DistilBertTokenizer\n",
    "            self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not initialize tokenizer: {e}\")\n",
    "            self.tokenizer = None\n",
    "\n",
    "    def load_image_data(self, data_path, subset):\n",
    "        \"\"\"Load image data using ImageDataGenerator.\"\"\"\n",
    "        if not os.path.exists(data_path):\n",
    "            print(f\"Warning: {data_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_image_data(subset)\n",
    "\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "        generator = datagen.flow_from_directory(\n",
    "            data_path,\n",
    "            target_size=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH),\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "            color_mode='grayscale',\n",
    "            classes=self.config.EMOTION_CLASSES,\n",
    "            shuffle=(subset == 'train')\n",
    "        )\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def create_dummy_image_data(self, subset):\n",
    "        \"\"\"Create dummy image data for testing.\"\"\"\n",
    "        print(f\"Creating dummy {subset} data...\")\n",
    "\n",
    "        # Create dummy images and labels\n",
    "        num_samples = 100 if subset == 'train' else 50\n",
    "        images = np.random.rand(num_samples, self.config.IMG_HEIGHT,\n",
    "                              self.config.IMG_WIDTH, self.config.IMG_CHANNELS)\n",
    "        labels = np.random.randint(0, self.config.NUM_CLASSES, num_samples)\n",
    "        labels = to_categorical(labels, self.config.NUM_CLASSES)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def load_text_data(self, file_path):\n",
    "        \"\"\"Load text data from JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_text_data()\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        texts = [item['text'] for item in data]\n",
    "        emotions = [item['emotion'] for item in data]\n",
    "\n",
    "        # Convert emotion labels to indices\n",
    "        emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.config.EMOTION_CLASSES)}\n",
    "        labels = [emotion_to_idx[emotion] for emotion in emotions]\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    def create_dummy_text_data(self):\n",
    "        \"\"\"Create dummy text data for testing.\"\"\"\n",
    "        print(\"Creating dummy text data...\")\n",
    "\n",
    "        dummy_texts = [\n",
    "            \"I am so happy today!\",\n",
    "            \"This makes me really angry.\",\n",
    "            \"That's completely disgusting.\",\n",
    "            \"I feel so sad about this.\",\n",
    "            \"This is really scary.\",\n",
    "            \"What a surprise that was!\"\n",
    "        ] * 50  # Repeat to get more samples\n",
    "\n",
    "        dummy_labels = list(range(self.config.NUM_CLASSES)) * 50\n",
    "\n",
    "        return dummy_texts, dummy_labels\n",
    "\n",
    "    def preprocess_text(self, texts):\n",
    "        \"\"\"Preprocess text data for BERT.\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.config.MAX_TEXT_LENGTH,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return encodings\n",
    "\n",
    "    def load_multimodal_data(self, file_path):\n",
    "        \"\"\"Load multimodal data from JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_multimodal_data()\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def create_dummy_multimodal_data(self):\n",
    "        \"\"\"Create dummy multimodal data for testing.\"\"\"\n",
    "        print(\"Creating dummy multimodal data...\")\n",
    "\n",
    "        num_samples = 300\n",
    "        \n",
    "        # Create dummy images\n",
    "        images = np.random.rand(num_samples, self.config.IMG_HEIGHT,\n",
    "                              self.config.IMG_WIDTH, self.config.IMG_CHANNELS)\n",
    "        \n",
    "        # Create dummy texts\n",
    "        text_templates = [\n",
    "            \"I am feeling very {emotion} today!\",\n",
    "            \"This situation makes me {emotion}.\",\n",
    "            \"What a {emotion} experience this is!\",\n",
    "            \"I can't believe how {emotion} I feel.\",\n",
    "            \"This is so {emotion}, I can't stand it.\"\n",
    "        ]\n",
    "        \n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            emotion_idx = i % self.config.NUM_CLASSES\n",
    "            emotion = self.config.EMOTION_CLASSES[emotion_idx]\n",
    "            \n",
    "            # Create text with emotion\n",
    "            template = text_templates[i % len(text_templates)]\n",
    "            text = template.format(emotion=emotion)\n",
    "            \n",
    "            texts.append(text)\n",
    "            labels.append(emotion_idx)\n",
    "\n",
    "        return {\n",
    "            'images': images,\n",
    "            'texts': texts,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(Config)\n",
    "print(\"Data loader initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdce3af",
   "metadata": {
    "id": "9bdce3af"
   },
   "source": [
    "## 4. Data Loading and Preprocessing\n",
    "Functions for loading and preprocessing both image and text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acb438c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "91f8819e97164942bc807d5d636285bd",
      "853e8fe021e9472f9c7ebdf3ca9ab583",
      "8d71d26a146149c895c0715d3e7dd913",
      "17ecf68029404bfabb7735f9c543d81c",
      "8f345f79051943648453dc8b177f6511",
      "ffa3dfa79da541dd9a67375c4608a8f7",
      "4c7d183754da4d2280e297d813162d58",
      "64ec69e08a924fed8e82f6e0cdc4120d",
      "0bb7386f9ba548978989a611e89c85cb",
      "c85aba4232bd476aaf209f6b7e268773",
      "b7a53409d5ad437696a8660fad6d76a2",
      "f7dbff2286964081a88e49303ec0f730",
      "28261fdf6a2e430da380f85382473492",
      "9a803a80da51413aa4b31d0b4f64e3dc",
      "a34d22578abe4f68869f03d76b77de9c",
      "95ac3c1ed40141d6ad087c86e52fc50b",
      "f386d5b387504ec287cccdd6821f7e00",
      "3220d1c94dc349a495bad77278d58ebc",
      "819d9a7fefd44e69a26848511f3ffbd5",
      "7d2f1f4108dc444988bc130936e7f7c7",
      "df80782a54a847569098ae2a0b42cbe3",
      "1d471d4711e848ff8b91bc552dd9c33f",
      "fb3dfa3b15d94ea0a74587134c69e2a1",
      "9154c437c97c4a46bd3c125c3e7c8624",
      "9f05f03dda84484d9993214b2470a407",
      "8192196aaf904f7d8c5692547c01760e",
      "facd7a64822447529bc5934faf518a34",
      "d5658c94ec6a4d398790c2978c0ece00",
      "8baa543ef29541628dbb8708f1a1d54d",
      "41bf980810444cdebec292503ef66961",
      "621fa8c5a637479bbdcad5784704b84c",
      "1a45d93c25fb4cee89d981300d2a729e",
      "d7a32b300b4341f9bb65e2fb1f869169",
      "a77f7a8b9f574bb5839c3e5e5214a0ae",
      "6f19bec67dee4984babced020c550dbc",
      "1297516bfb0548e384a5d139235a60c1",
      "0b24891f928e4e75b391b9972864d85f",
      "2ac5e67f9cbc4db8afa709be0a583da0",
      "2c92b36c57774fccbc79f49e5b5ebd55",
      "a109e968c78d4494b07b9e01b2062986",
      "cdc675c6b7944215bc7216f6d3fed954",
      "3a2dabe975f8408a8faaba53c02e1f2f",
      "ba88ef04d10d4f83bd51b6fd76917b05",
      "bd45ecf53f4b47ff9e594bab6d1aa8e6"
     ]
    },
    "id": "acb438c0",
    "outputId": "564a8948-a109-455e-aa1d-7f81a0e1212f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Class for loading and preprocessing data.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(config.BERT_MODEL)\n",
    "\n",
    "    def load_image_data(self, subset='train'):\n",
    "        \"\"\"Load image data using ImageDataGenerator.\"\"\"\n",
    "        datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            zoom_range=0.2,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "        # For validation and test, don't apply augmentation\n",
    "        if subset in ['validation', 'test']:\n",
    "            datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "        data_path = os.path.join(self.config.FER_IMAGES_PATH, subset)\n",
    "\n",
    "        if not os.path.exists(data_path):\n",
    "            print(f\"Warning: Path {data_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_image_data(subset)\n",
    "\n",
    "        generator = datagen.flow_from_directory(\n",
    "            data_path,\n",
    "            target_size=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH),\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "            color_mode='grayscale',\n",
    "            classes=self.config.EMOTION_CLASSES,\n",
    "            shuffle=(subset == 'train')\n",
    "        )\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def create_dummy_image_data(self, subset):\n",
    "        \"\"\"Create dummy image data for testing.\"\"\"\n",
    "        print(f\"Creating dummy {subset} data...\")\n",
    "\n",
    "        # Create dummy images and labels\n",
    "        num_samples = 100 if subset == 'train' else 50\n",
    "        images = np.random.rand(num_samples, self.config.IMG_HEIGHT,\n",
    "                              self.config.IMG_WIDTH, self.config.IMG_CHANNELS)\n",
    "        labels = np.random.randint(0, self.config.NUM_CLASSES, num_samples)\n",
    "        labels = to_categorical(labels, self.config.NUM_CLASSES)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def load_text_data(self, file_path):\n",
    "        \"\"\"Load text data from JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_text_data()\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        texts = [item['text'] for item in data]\n",
    "        emotions = [item['emotion'] for item in data]\n",
    "\n",
    "        # Convert emotion labels to indices\n",
    "        emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.config.EMOTION_CLASSES)}\n",
    "        labels = [emotion_to_idx[emotion] for emotion in emotions]\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    def create_dummy_text_data(self):\n",
    "        \"\"\"Create dummy text data for testing.\"\"\"\n",
    "        print(\"Creating dummy text data...\")\n",
    "\n",
    "        dummy_texts = [\n",
    "            \"I am so happy today!\",\n",
    "            \"This makes me really angry.\",\n",
    "            \"That's completely disgusting.\",\n",
    "            \"I feel so sad about this.\",\n",
    "            \"This is really scary.\",\n",
    "            \"What a surprise that was!\"\n",
    "        ] * 50  # Repeat to get more samples\n",
    "\n",
    "        dummy_labels = list(range(self.config.NUM_CLASSES)) * 50\n",
    "\n",
    "        return dummy_texts, dummy_labels\n",
    "\n",
    "    def preprocess_text(self, texts):\n",
    "        \"\"\"Preprocess text data for BERT.\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.config.MAX_TEXT_LENGTH,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return encodings\n",
    "\n",
    "    def load_multimodal_data(self, file_path):\n",
    "        \"\"\"Load multimodal data from JSON file.\"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Warning: {file_path} does not exist. Creating dummy data.\")\n",
    "            return self.create_dummy_multimodal_data()\n",
    "\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def create_dummy_multimodal_data(self):\n",
    "        \"\"\"Create dummy multimodal data for testing.\"\"\"\n",
    "        print(\"Creating dummy multimodal data...\")\n",
    "\n",
    "        num_samples = 300\n",
    "        \n",
    "        # Create dummy images\n",
    "        images = np.random.rand(num_samples, self.config.IMG_HEIGHT,\n",
    "                              self.config.IMG_WIDTH, self.config.IMG_CHANNELS)\n",
    "        \n",
    "        # Create dummy texts\n",
    "        text_templates = [\n",
    "            \"I am feeling very {emotion} today!\",\n",
    "            \"This situation makes me {emotion}.\",\n",
    "            \"What a {emotion} experience this is!\",\n",
    "            \"I can't believe how {emotion} I feel.\",\n",
    "            \"This is so {emotion}, I can't stand it.\"\n",
    "        ]\n",
    "        \n",
    "        texts = []\n",
    "        labels = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            emotion_idx = i % self.config.NUM_CLASSES\n",
    "            emotion = self.config.EMOTION_CLASSES[emotion_idx]\n",
    "            \n",
    "            # Create text with emotion\n",
    "            template = text_templates[i % len(text_templates)]\n",
    "            text = template.format(emotion=emotion)\n",
    "            \n",
    "            texts.append(text)\n",
    "            labels.append(emotion_idx)\n",
    "\n",
    "        return {\n",
    "            'images': images,\n",
    "            'texts': texts,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(Config)\n",
    "print(\"Data loader initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab2d66",
   "metadata": {
    "id": "9aab2d66"
   },
   "source": [
    "## 5. Facial Emotion Recognition (FER) Model\n",
    "CNN-based model for facial emotion recognition from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e65f770",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e65f770",
    "outputId": "8b269dfb-c757-4a27-b8b8-c95e3a7c245b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FER model class initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class FacialEmotionRecognizer:\n",
    "    \"\"\"CNN-based Facial Emotion Recognition model.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build CNN model for facial emotion recognition.\"\"\"\n",
    "        # Use tf.keras.Input explicitly\n",
    "        inputs = tf.keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS))\n",
    "\n",
    "        # First convolutional block\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        # Second convolutional block\n",
    "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        # Third convolutional block\n",
    "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        # Fourth convolutional block\n",
    "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D((2, 2))(x)\n",
    "        x = layers.Dropout(0.25)(x)\n",
    "\n",
    "        # Global average pooling and dense layers\n",
    "        x = layers.GlobalAveragePooling2D()(x)\n",
    "        x = layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        # Output layer\n",
    "        outputs = layers.Dense(self.config.NUM_CLASSES, activation='softmax', name='fer_output')(x)\n",
    "\n",
    "        # Use tf.keras.Model explicitly\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs, name='FacialEmotionRecognizer')\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, train_generator, val_generator):\n",
    "        \"\"\"Train the FER model.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=self.config.PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                self.config.FER_MODEL_PATH,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            validation_data=val_generator,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on input data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built or loaded. Call build_model() or load_model() first.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def load_model(self, model_path=None):\n",
    "        \"\"\"Load a saved model.\"\"\"\n",
    "        path = model_path or self.config.FER_MODEL_PATH\n",
    "        if os.path.exists(path):\n",
    "            self.model = tf.keras.models.load_model(path)\n",
    "            print(f\"FER model loaded from {path}\")\n",
    "        else:\n",
    "            print(f\"Model file not found at {path}\")\n",
    "\n",
    "    def save_model(self, model_path=None):\n",
    "        \"\"\"Save the current model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
    "\n",
    "        path = model_path or self.config.FER_MODEL_PATH\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        self.model.save(path)\n",
    "        print(f\"FER model saved to {path}\")\n",
    "\n",
    "# Initialize FER model\n",
    "fer_model = FacialEmotionRecognizer(Config)\n",
    "print(\"FER model class initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfa46bc",
   "metadata": {
    "id": "1bfa46bc"
   },
   "source": [
    "## 6. Text Emotion Recognition (TER) Model\n",
    "DistilBERT-based model for text emotion recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8613275b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8613275b",
    "outputId": "2b29ec53-7b8d-459a-ff66-1d6e4de40ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER model class initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "class TextEmotionRecognizer:\n",
    "    \"\"\"DistilBERT-based Text Emotion Recognition model.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(config.BERT_MODEL)\n",
    "        self.model = None\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build DistilBERT model for text emotion recognition.\"\"\"\n",
    "        \n",
    "        # Alternative approach: Build model using subclassing instead of functional API\n",
    "        class DistilBertClassifier(tf.keras.Model):\n",
    "            def __init__(self, config, **kwargs):\n",
    "                super().__init__(**kwargs)\n",
    "                self.config = config\n",
    "                self.distilbert = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "                    config.BERT_MODEL,\n",
    "                    num_labels=config.NUM_CLASSES,\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "                self.softmax = tf.keras.layers.Softmax(name='ter_output')\n",
    "                \n",
    "            def call(self, inputs):\n",
    "                input_ids, attention_mask = inputs\n",
    "                outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                return self.softmax(logits)\n",
    "                \n",
    "            def get_config(self):\n",
    "                return {\"config\": self.config}\n",
    "\n",
    "        # Create the model\n",
    "        model = DistilBertClassifier(self.config, name='TextEmotionRecognizer')\n",
    "        \n",
    "        # Build the model by calling it once\n",
    "        dummy_input_ids = tf.zeros((1, self.config.MAX_TEXT_LENGTH), dtype=tf.int32)\n",
    "        dummy_attention_mask = tf.ones((1, self.config.MAX_TEXT_LENGTH), dtype=tf.int32)\n",
    "        _ = model([dummy_input_ids, dummy_attention_mask])\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def preprocess_texts(self, texts):\n",
    "        \"\"\"Preprocess texts for model input.\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=self.config.MAX_TEXT_LENGTH,\n",
    "            return_tensors='tf'\n",
    "        )\n",
    "        return [encodings['input_ids'], encodings['attention_mask']]\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train the TER model.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.build_model()\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=self.config.PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                self.config.TER_MODEL_PATH,\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions on input data.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built or loaded. Call build_model() or load_model() first.\")\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def load_model(self, model_path=None):\n",
    "        \"\"\"Load a saved model.\"\"\"\n",
    "        # For models with DistilBERT, it's easier to rebuild and load weights\n",
    "        print(\"Rebuilding TER model architecture...\")\n",
    "        self.build_model()\n",
    "        \n",
    "        weights_path = (model_path or self.config.TER_MODEL_PATH).replace('.h5', '_weights.h5')\n",
    "        if os.path.exists(weights_path):\n",
    "            self.model.load_weights(weights_path)\n",
    "            print(f\"TER model weights loaded from {weights_path}\")\n",
    "        else:\n",
    "            print(f\"Weights file not found at {weights_path}\")\n",
    "\n",
    "    def save_model(self, model_path=None):\n",
    "        \"\"\"Save the current model.\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save. Train or build a model first.\")\n",
    "\n",
    "        path = model_path or self.config.TER_MODEL_PATH\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        \n",
    "        # Save weights (more reliable for models with transformers)\n",
    "        weights_path = path.replace('.h5', '_weights.h5')\n",
    "        self.model.save_weights(weights_path)\n",
    "        print(f\"TER model weights saved to {weights_path}\")\n",
    "\n",
    "# Initialize TER model\n",
    "ter_model = TextEmotionRecognizer(Config)\n",
    "print(\"TER model class initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8061a798",
   "metadata": {
    "id": "8061a798"
   },
   "source": [
    "## 7. Multimodal Fusion Model\n",
    "Combining FER and TER models for improved emotion recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0da1077b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0da1077b",
    "outputId": "96fb90be-b41e-4cfd-d3d1-4c3f32f4eaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal fusion model class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class MultimodalEmotionRecognizer:\n",
    "    \"\"\"Multimodal emotion recognition combining FER and TER models.\"\"\"\n",
    "\n",
    "    def __init__(self, config, fer_model, ter_model):\n",
    "        self.config = config\n",
    "        self.fer_model = fer_model\n",
    "        self.ter_model = ter_model\n",
    "        self.model = None\n",
    "\n",
    "    def build_early_fusion_model(self):\n",
    "        \"\"\"Build early fusion model combining image and text features.\"\"\"\n",
    "        # Image input branch - use tf.keras.Input explicitly\n",
    "        image_input = tf.keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS), name='image_input')\n",
    "\n",
    "        # Extract image features using FER model backbone\n",
    "        # Create a functional model for the FER backbone\n",
    "        fer_backbone_output = self.fer_model.model.layers[-3].output # Output before the last two dense layers\n",
    "        fer_backbone = tf.keras.Model(inputs=self.fer_model.model.input, outputs=fer_backbone_output, name='fer_backbone')\n",
    "        image_features = fer_backbone(image_input)\n",
    "\n",
    "        # Text input branches\n",
    "        text_input_ids = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_input_ids')\n",
    "        text_attention_mask = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_attention_mask')\n",
    "\n",
    "        # Extract text features using TER model's approach\n",
    "        # Create a new DistilBERT model instance with the same configuration\n",
    "        from transformers import TFDistilBertForSequenceClassification\n",
    "        temp_distilbert = TFDistilBertForSequenceClassification.from_pretrained(\n",
    "            'distilbert-base-uncased',\n",
    "            num_labels=self.config.NUM_CLASSES\n",
    "        )\n",
    "        \n",
    "        # Extract just the DistilBERT base (without classification head)\n",
    "        # Use the correct syntax for calling DistilBERT with named arguments\n",
    "        distilbert_outputs = temp_distilbert.distilbert(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
    "        text_features = distilbert_outputs.last_hidden_state\n",
    "        text_features = layers.GlobalAveragePooling1D()(text_features)\n",
    "\n",
    "        # Fusion layer\n",
    "        combined_features = layers.Concatenate(name='feature_fusion')([image_features, text_features])\n",
    "        \n",
    "        # Additional fusion layers\n",
    "        x = layers.Dense(512, activation='relu', kernel_regularizer=l2(0.001))(combined_features)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        x = layers.Dense(256, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        # Final classification layer\n",
    "        outputs = layers.Dense(self.config.NUM_CLASSES, activation='softmax', name='multimodal_output')(x)\n",
    "\n",
    "        # Create model - use tf.keras.Model explicitly\n",
    "        model = tf.keras.Model(\n",
    "            inputs=[image_input, text_input_ids, text_attention_mask],\n",
    "            outputs=outputs,\n",
    "            name='MultimodalEmotionRecognizer_EarlyFusion'\n",
    "        )\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def build_late_fusion_model(self):\n",
    "        \"\"\"Build late fusion model combining FER and TER predictions.\"\"\"\n",
    "        # Image input branch\n",
    "        image_input = tf.keras.Input(shape=(self.config.IMG_HEIGHT, self.config.IMG_WIDTH, self.config.IMG_CHANNELS), name='image_input')\n",
    "\n",
    "        # Text input branches\n",
    "        text_input_ids = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_input_ids')\n",
    "        text_attention_mask = tf.keras.Input(shape=(self.config.MAX_TEXT_LENGTH,), dtype=tf.int32, name='text_attention_mask')\n",
    "\n",
    "        # Get predictions from individual models\n",
    "        fer_predictions = self.fer_model.model(image_input)\n",
    "        ter_predictions = self.ter_model.model([text_input_ids, text_attention_mask])\n",
    "\n",
    "        # Simple weighted average fusion for now\n",
    "        # You can make this learnable with additional layers\n",
    "        fer_weight = 0.6\n",
    "        ter_weight = 0.4\n",
    "        \n",
    "        # Weighted combination\n",
    "        outputs = layers.Add(name='multimodal_output')([\n",
    "            layers.Lambda(lambda x: x * fer_weight)(fer_predictions),\n",
    "            layers.Lambda(lambda x: x * ter_weight)(ter_predictions)\n",
    "        ])\n",
    "\n",
    "        # Create model - use tf.keras.Model explicitly\n",
    "        model = tf.keras.Model(\n",
    "            inputs=[image_input, text_input_ids, text_attention_mask],\n",
    "            outputs=outputs,\n",
    "            name='MultimodalEmotionRecognizer_LateFusion'\n",
    "        )\n",
    "\n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=self.config.LEARNING_RATE),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def simple_average_fusion(self, fer_predictions, ter_predictions):\n",
    "        \"\"\"Simple average fusion of FER and TER predictions.\"\"\"\n",
    "        return (fer_predictions + ter_predictions) / 2\n",
    "\n",
    "    def weighted_average_fusion(self, fer_predictions, ter_predictions, fer_weight=0.6):\n",
    "        \"\"\"Weighted average fusion of FER and TER predictions.\"\"\"\n",
    "        ter_weight = 1 - fer_weight\n",
    "        return fer_weight * fer_predictions + ter_weight * ter_predictions\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, fusion_type='early'):\n",
    "        \"\"\"Train the multimodal model.\"\"\"\n",
    "        if fusion_type == 'early':\n",
    "            self.build_early_fusion_model()\n",
    "        elif fusion_type == 'late':\n",
    "            self.build_late_fusion_model()\n",
    "        else:\n",
    "            raise ValueError(\"fusion_type must be 'early' or 'late'\")\n",
    "\n",
    "        # Callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=self.config.PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.2,\n",
    "                patience=5,\n",
    "                min_lr=1e-7,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                os.path.join(self.config.MODELS_PATH, 'multimodal_model.h5'),\n",
    "                monitor='val_accuracy',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Train model\n",
    "        history = self.model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=self.config.BATCH_SIZE,\n",
    "            epochs=self.config.EPOCHS,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return history\n",
    "\n",
    "print(\"Multimodal fusion model class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dccfc8",
   "metadata": {
    "id": "04dccfc8"
   },
   "source": [
    "## 8. Setup Directory Structure\n",
    "Create the necessary directory structure for data organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "415d8bb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "415d8bb5",
    "outputId": "648f905b-611b-4624-f93d-ef49780a8d04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory structure created successfully!\n",
      "\\n📁 Project Directory Structure:\n",
      "emotion_recognition/\n",
      "├── 📁 data/\n",
      "│   ├── 📁 processed/\n",
      "│   │   ├── 📁 fer_features/\n",
      "│   │   └── 📁 ter_features/\n",
      "│   └── 📁 raw/\n",
      "│       ├── 📁 fer_images/\n",
      "│       ├── 📁 multimodal_data/\n",
      "│       └── 📁 text_data/\n",
      "├── 📁 models/\n",
      "│   ├── 📄 fer_model.h5\n",
      "│   └── 📄 ter_model.h5\n",
      "└── 📁 results/\n",
      "    ├── 📁 metrics/\n",
      "    └── 📁 plots/\n",
      "        ├── 📄 fer_confusion_matrix.png\n",
      "        ├── 📄 fer_training_history.png\n",
      "        ├── 📄 sample_prediction_probabilities.png\n",
      "        └── 📄 ter_training_history.png\n"
     ]
    }
   ],
   "source": [
    "def create_directories():\n",
    "    \"\"\"Create necessary directories for the project.\"\"\"\n",
    "    directories = [\n",
    "        Config.BASE_PATH,\n",
    "        Config.DATA_PATH,\n",
    "        Config.RAW_DATA_PATH,\n",
    "        Config.PROCESSED_DATA_PATH,\n",
    "        Config.FER_IMAGES_PATH,  # Fixed: was FER_DATA_PATH\n",
    "        Config.TEXT_DATA_PATH,\n",
    "        Config.MULTIMODAL_DATA_PATH,\n",
    "        Config.MODELS_PATH,\n",
    "        Config.RESULTS_PATH,\n",
    "        os.path.join(Config.RESULTS_PATH, 'metrics'),\n",
    "        os.path.join(Config.RESULTS_PATH, 'plots'),\n",
    "        os.path.join(Config.PROCESSED_DATA_PATH, 'fer_features'),\n",
    "        os.path.join(Config.PROCESSED_DATA_PATH, 'ter_features')\n",
    "    ]\n",
    "    \n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Create some example files for demonstration\n",
    "    example_files = [\n",
    "        os.path.join(Config.MODELS_PATH, 'fer_model.h5'),\n",
    "        os.path.join(Config.MODELS_PATH, 'ter_model.h5'),\n",
    "        os.path.join(Config.RESULTS_PATH, 'plots', 'fer_training_history.png'),\n",
    "        os.path.join(Config.RESULTS_PATH, 'plots', 'ter_training_history.png')\n",
    "    ]\n",
    "    \n",
    "    for file_path in example_files:\n",
    "        if not os.path.exists(file_path):\n",
    "            # Create empty placeholder files for demonstration\n",
    "            with open(file_path, 'w') as f:\n",
    "                f.write(\"# Placeholder file created by setup script\\n\")\n",
    "\n",
    "# Create directory structure\n",
    "create_directories()\n",
    "\n",
    "# Display the created directory structure\n",
    "def display_directory_structure(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Display directory structure in a tree format.\"\"\"\n",
    "    if current_depth >= max_depth:\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"{prefix}📁 {os.path.basename(path)} (will be created)\")\n",
    "        return\n",
    "\n",
    "    items = sorted(os.listdir(path))\n",
    "    for i, item in enumerate(items):\n",
    "        item_path = os.path.join(path, item)\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "\n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{prefix}{current_prefix}📁 {item}/\")\n",
    "            extension = \"    \" if is_last else \"│   \"\n",
    "            display_directory_structure(item_path, prefix + extension, max_depth, current_depth + 1)\n",
    "        else:\n",
    "            print(f\"{prefix}{current_prefix}📄 {item}\")\n",
    "\n",
    "print(\"Directory structure created successfully!\")\n",
    "print(\"\\\\n📁 Project Directory Structure:\")\n",
    "print(\"emotion_recognition/\")\n",
    "display_directory_structure(Config.BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c633195",
   "metadata": {
    "id": "4c633195"
   },
   "source": [
    "## 9. Train Facial Emotion Recognition Model\n",
    "Train the CNN-based FER model on facial expression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b451a1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 845
    },
    "id": "0b451a1f",
    "outputId": "0f170809-a0ca-4e51-988b-4c6eb6f0db95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FER training data...\n",
      "Found 0 images belonging to 6 classes.\n",
      "Found 0 images belonging to 6 classes.\n",
      "Found 0 images belonging to 6 classes.\n",
      "Found 0 images belonging to 6 classes.\n",
      "Found 0 images belonging to 6 classes.\n",
      "No images found in directory structure.\n",
      "Switching to dummy data for demonstration...\n",
      "Creating dummy train data...\n",
      "Creating dummy validation data...\n",
      "Creating dummy test data...\n",
      "Created dummy training samples: 100\n",
      "Created dummy validation samples: 50\n",
      "Created dummy test samples: 50\n",
      "\\nBuilding FER model...\n",
      "No images found in directory structure.\n",
      "Switching to dummy data for demonstration...\n",
      "Creating dummy train data...\n",
      "Creating dummy validation data...\n",
      "Creating dummy test data...\n",
      "Created dummy training samples: 100\n",
      "Created dummy validation samples: 50\n",
      "Created dummy test samples: 50\n",
      "\\nBuilding FER model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"FacialEmotionRecognizer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"FacialEmotionRecognizer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fer_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,542</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ fer_output (\u001b[38;5;33mDense\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m1,542\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,443,046</span> (5.50 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,443,046\u001b[0m (5.50 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,439,590</span> (5.49 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,439,590\u001b[0m (5.49 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,456</span> (13.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,456\u001b[0m (13.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n",
      "Model architecture saved to results/fer_model_architecture.png\n",
      "Model architecture saved to results/fer_model_architecture.png\n"
     ]
    }
   ],
   "source": [
    "# Load FER data\n",
    "print(\"Loading FER training data...\")\n",
    "try:\n",
    "    train_generator = data_loader.load_image_data('train')\n",
    "    val_generator = data_loader.load_image_data('validation')\n",
    "    test_generator = data_loader.load_image_data('test')\n",
    "\n",
    "    # Check if generators have any data\n",
    "    if hasattr(train_generator, 'samples') and train_generator.samples > 0:\n",
    "        print(f\"Training samples: {train_generator.samples}\")\n",
    "        print(f\"Validation samples: {val_generator.samples}\")\n",
    "        print(f\"Test samples: {test_generator.samples}\")\n",
    "        use_real_data = True\n",
    "    else:\n",
    "        print(\"No images found in directory structure.\")\n",
    "        print(\"Switching to dummy data for demonstration...\")\n",
    "        use_real_data = False\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading FER data: {e}\")\n",
    "    print(\"Using dummy data for demonstration...\")\n",
    "    use_real_data = False\n",
    "\n",
    "# If no real data, create dummy data\n",
    "if not use_real_data:\n",
    "    # Create dummy data\n",
    "    train_images, train_labels = data_loader.create_dummy_image_data('train')\n",
    "    val_images, val_labels = data_loader.create_dummy_image_data('validation')\n",
    "    test_images, test_labels = data_loader.create_dummy_image_data('test')\n",
    "\n",
    "    print(f\"Created dummy training samples: {len(train_images)}\")\n",
    "    print(f\"Created dummy validation samples: {len(val_images)}\")\n",
    "    print(f\"Created dummy test samples: {len(test_images)}\")\n",
    "\n",
    "    train_generator = (train_images, train_labels)\n",
    "    val_generator = (val_images, val_labels)\n",
    "    test_generator = (test_images, test_labels)\n",
    "\n",
    "# Build and display FER model architecture\n",
    "print(\"\\\\nBuilding FER model...\")\n",
    "fer_model.build_model()\n",
    "print(fer_model.model.summary())\n",
    "\n",
    "# Visualize model architecture (optional)\n",
    "try:\n",
    "    tf.keras.utils.plot_model(\n",
    "        fer_model.model,\n",
    "        to_file=os.path.join(Config.RESULTS_PATH, 'fer_model_architecture.png'),\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True\n",
    "    )\n",
    "    print(\"Model architecture saved to results/fer_model_architecture.png\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save model architecture plot: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d0ec2fe",
   "metadata": {
    "id": "2d0ec2fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FER model...\n",
      "This may take a while depending on your dataset size and hardware...\n",
      "Training with dummy data (reduced epochs for demonstration)...\n",
      "Epoch 1/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - accuracy: 0.2603 - loss: 3.5329 - val_accuracy: 0.1800 - val_loss: 2.4663\n",
      "Epoch 2/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 194ms/step - accuracy: 0.2603 - loss: 3.5329 - val_accuracy: 0.1800 - val_loss: 2.4663\n",
      "Epoch 2/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.1573 - loss: 3.5816 - val_accuracy: 0.1800 - val_loss: 2.4610\n",
      "Epoch 3/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.1573 - loss: 3.5816 - val_accuracy: 0.1800 - val_loss: 2.4610\n",
      "Epoch 3/3\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.2338 - loss: 3.3213 - val_accuracy: 0.0800 - val_loss: 2.4589\n",
      "\\n✅ FER model training completed!\n",
      "❌ Error during FER training: name 'plot_training_history' is not defined\n",
      "Model architecture is built but not trained.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.2338 - loss: 3.3213 - val_accuracy: 0.0800 - val_loss: 2.4589\n",
      "\\n✅ FER model training completed!\n",
      "❌ Error during FER training: name 'plot_training_history' is not defined\n",
      "Model architecture is built but not trained.\n"
     ]
    }
   ],
   "source": [
    "# Train FER model\n",
    "print(\"Training FER model...\")\n",
    "print(\"This may take a while depending on your dataset size and hardware...\")\n",
    "\n",
    "try:\n",
    "    if use_real_data and hasattr(train_generator, 'samples'):  # Real data generator\n",
    "        fer_history = fer_model.train(train_generator, val_generator)\n",
    "    else:  # Dummy data - train with arrays\n",
    "        print(\"Training with dummy data (reduced epochs for demonstration)...\")\n",
    "        fer_history = fer_model.model.fit(\n",
    "            train_generator[0], train_generator[1],\n",
    "            batch_size=Config.BATCH_SIZE,\n",
    "            epochs=min(3, Config.EPOCHS),  # Reduced epochs for dummy data\n",
    "            validation_data=(val_generator[0], val_generator[1]),\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "    print(\"\\\\n✅ FER model training completed!\")\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        fer_history,\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'fer_training_history.png')\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    fer_model.save_model()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during FER training: {e}\")\n",
    "    print(\"Model architecture is built but not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ef163",
   "metadata": {
    "id": "067ef163"
   },
   "source": [
    "## 10. Train Text Emotion Recognition Model\n",
    "Train the DistilBERT-based TER model on text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f643e799",
   "metadata": {
    "id": "f643e799"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Warning: /content/drive/MyDrive/emotion_recognition\\data\\raw\\text_data\\train_text.json does not exist. Creating dummy data.\n",
      "Creating dummy text data...\n",
      "Warning: /content/drive/MyDrive/emotion_recognition\\data\\raw\\text_data\\val_text.json does not exist. Creating dummy data.\n",
      "Creating dummy text data...\n",
      "Warning: /content/drive/MyDrive/emotion_recognition\\data\\raw\\text_data\\test_text.json does not exist. Creating dummy data.\n",
      "Creating dummy text data...\n",
      "Text training samples: 300\n",
      "Text validation samples: 300\n",
      "Text test samples: 300\n",
      "\\nPreprocessing text data...\n",
      "Text preprocessing completed!\n",
      "Train input shapes: [TensorShape([300, 9]), TensorShape([300, 9])]\n",
      "\\nBuilding TER model...\n",
      "Text preprocessing completed!\n",
      "Train input shapes: [TensorShape([300, 9]), TensorShape([300, 9])]\n",
      "\\nBuilding TER model...\n",
      "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER model built successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"TextEmotionRecognizer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"TextEmotionRecognizer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ter_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Softmax</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ ter_output (\u001b[38;5;33mSoftmax\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load text data for TER model\n",
    "print(\"Loading text data...\")\n",
    "try:\n",
    "    # Try to load real text data\n",
    "    train_texts, train_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'train_text.json')\n",
    "    )\n",
    "    val_texts, val_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'val_text.json')\n",
    "    )\n",
    "    test_texts, test_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'test_text.json')\n",
    "    )\n",
    "\n",
    "    print(f\"Text training samples: {len(train_texts)}\")\n",
    "    print(f\"Text validation samples: {len(val_texts)}\")\n",
    "    print(f\"Text test samples: {len(test_texts)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading real text data: {e}\")\n",
    "    print(\"Using dummy text data for demonstration...\")\n",
    "\n",
    "    # Create dummy text data\n",
    "    train_texts, train_text_labels = data_loader.create_dummy_text_data()\n",
    "    val_texts, val_text_labels = data_loader.create_dummy_text_data()\n",
    "    test_texts, test_text_labels = data_loader.create_dummy_text_data()\n",
    "\n",
    "    # Reduce size for different splits\n",
    "    train_texts, train_text_labels = train_texts[:200], train_text_labels[:200]\n",
    "    val_texts, val_text_labels = val_texts[:50], val_text_labels[:50]\n",
    "    test_texts, test_text_labels = test_texts[:50], test_text_labels[:50]\n",
    "\n",
    "    print(f\"Created dummy text training samples: {len(train_texts)}\")\n",
    "    print(f\"Created dummy text validation samples: {len(val_texts)}\")\n",
    "    print(f\"Created dummy text test samples: {len(test_texts)}\")\n",
    "\n",
    "# Preprocess text data\n",
    "print(\"\\\\nPreprocessing text data...\")\n",
    "train_text_encoded = ter_model.preprocess_texts(train_texts)\n",
    "val_text_encoded = ter_model.preprocess_texts(val_texts)\n",
    "test_text_encoded = ter_model.preprocess_texts(test_texts)\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(f\"Train input shapes: {[x.shape for x in train_text_encoded]}\")\n",
    "\n",
    "# Build TER model\n",
    "print(\"\\\\nBuilding TER model...\")\n",
    "ter_model.build_model()\n",
    "print(\"TER model built successfully!\")\n",
    "print(ter_model.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8dcbc93",
   "metadata": {
    "id": "b8dcbc93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Warning: /content/drive/MyDrive/emotion_recognition\\data\\raw\\text_data\\train_text_data.json does not exist. Creating dummy data.\n",
      "Creating dummy text data...\n",
      "Warning: /content/drive/MyDrive/emotion_recognition\\data\\raw\\text_data\\val_text_data.json does not exist. Creating dummy data.\n",
      "Creating dummy text data...\n",
      "Warning: /content/drive/MyDrive/emotion_recognition\\data\\raw\\text_data\\test_text_data.json does not exist. Creating dummy data.\n",
      "Creating dummy text data...\n",
      "Training text samples: 300\n",
      "Validation text samples: 300\n",
      "Test text samples: 300\n",
      "\\nPreprocessing text data...\n",
      "Text encoding input_ids shape: (300, 9)\n",
      "Text encoding attention_mask shape: (300, 9)\n",
      "Labels shape: (300,)\n",
      "\\nSample texts:\n",
      "  joy: 'I am so happy today!...'\n",
      "  anger: 'This makes me really angry....'\n",
      "  disgust: 'That's completely disgusting....'\n",
      "\\n✅ Text data loaded and preprocessed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load text data\n",
    "print(\"Loading text data...\")\n",
    "try:\n",
    "    train_texts, train_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'train_text_data.json')\n",
    "    )\n",
    "    val_texts, val_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'val_text_data.json')\n",
    "    )\n",
    "    test_texts, test_text_labels = data_loader.load_text_data(\n",
    "        os.path.join(Config.TEXT_DATA_PATH, 'test_text_data.json')\n",
    "    )\n",
    "\n",
    "    print(f\"Training text samples: {len(train_texts)}\")\n",
    "    print(f\"Validation text samples: {len(val_texts)}\")\n",
    "    print(f\"Test text samples: {len(test_texts)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading text data: {e}\")\n",
    "    print(\"Using dummy text data for demonstration...\")\n",
    "\n",
    "    # Create dummy text data\n",
    "    train_texts, train_text_labels = data_loader.create_dummy_text_data()\n",
    "    val_texts, val_text_labels = data_loader.create_dummy_text_data()\n",
    "    test_texts, test_text_labels = data_loader.create_dummy_text_data()\n",
    "    \n",
    "    # Reduce size for different splits\n",
    "    train_texts, train_text_labels = train_texts[:200], train_text_labels[:200]\n",
    "    val_texts, val_text_labels = val_texts[:50], val_text_labels[:50]\n",
    "    test_texts, test_text_labels = test_texts[:50], test_text_labels[:50]\n",
    "\n",
    "# Preprocess text data\n",
    "print(\"\\\\nPreprocessing text data...\")\n",
    "train_text_encoded = ter_model.preprocess_texts(train_texts)\n",
    "val_text_encoded = ter_model.preprocess_texts(val_texts)\n",
    "test_text_encoded = ter_model.preprocess_texts(test_texts)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "train_text_labels = np.array(train_text_labels)\n",
    "val_text_labels = np.array(val_text_labels)\n",
    "test_text_labels = np.array(test_text_labels)\n",
    "\n",
    "# Now train_text_encoded is a list [input_ids, attention_mask]\n",
    "print(f\"Text encoding input_ids shape: {train_text_encoded[0].shape}\")\n",
    "print(f\"Text encoding attention_mask shape: {train_text_encoded[1].shape}\")\n",
    "print(f\"Labels shape: {train_text_labels.shape}\")\n",
    "\n",
    "# Display some sample texts\n",
    "print(\"\\\\nSample texts:\")\n",
    "for i in range(min(3, len(train_texts))):\n",
    "    emotion = Config.EMOTION_CLASSES[train_text_labels[i]]\n",
    "    print(f\"  {emotion}: '{train_texts[i][:100]}...'\")\n",
    "\n",
    "print(\"\\\\n✅ Text data loaded and preprocessed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "658ed2ea",
   "metadata": {
    "id": "658ed2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training TER model...\n",
      "This may take a while, especially for BERT-based models...\n",
      "Epoch 1/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step - accuracy: 0.1748 - loss: 1.7939 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.16667, saving model to /content/drive/MyDrive/emotion_recognition\\models\\ter_model.h5\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.16667, saving model to /content/drive/MyDrive/emotion_recognition\\models\\ter_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 317ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "Epoch 2/50\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.1757 - loss: 1.7941\n",
      "Epoch 2: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 2: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 245ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 3: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 236ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 4: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m 9/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 114ms/step - accuracy: 0.1757 - loss: 1.7941\n",
      "Epoch 5: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 268ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 268ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 284ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 284ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 7: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 340ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 340ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 8: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 370ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 8: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 370ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 9: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 384ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 9: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 384ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 10: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 487ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 10: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 487ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 328ms/step - accuracy: 0.1748 - loss: 1.7939\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 718ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 11: val_accuracy did not improve from 0.16667\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 718ms/step - accuracy: 0.1741 - loss: 1.7936 - val_accuracy: 0.1667 - val_loss: 1.7914 - learning_rate: 2.0000e-04\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\\n✅ TER model training completed!\n",
      "❌ Error during TER training: name 'plot_training_history' is not defined\n",
      "Building model without training for demonstration...\n",
      "\\n✅ TER model training completed!\n",
      "❌ Error during TER training: name 'plot_training_history' is not defined\n",
      "Building model without training for demonstration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Train TER model\n",
    "print(\"Training TER model...\")\n",
    "print(\"This may take a while, especially for BERT-based models...\")\n",
    "\n",
    "try:\n",
    "    ter_history = ter_model.train(\n",
    "        train_text_encoded, train_text_labels,\n",
    "        val_text_encoded, val_text_labels\n",
    "    )\n",
    "\n",
    "    print(\"\\\\n✅ TER model training completed!\")\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(\n",
    "        ter_history,\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'ter_training_history.png')\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    ter_model.save_model()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during TER training: {e}\")\n",
    "    print(\"Building model without training for demonstration...\")\n",
    "    ter_model.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2893bb1",
   "metadata": {
    "id": "d2893bb1"
   },
   "source": [
    "## 11. Train Multimodal Fusion Model\n",
    "Combine FER and TER models for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb21d27",
   "metadata": {
    "id": "eeb21d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading multimodal data...\n",
      "Warning: /content/drive/MyDrive/emotion_recognition\\data\\raw\\multimodal_data\\multimodal_dataset.json does not exist. Creating dummy data.\n",
      "Creating dummy multimodal data...\n",
      "Multimodal samples: 300\n",
      "\\nPreparing multimodal data...\n",
      "Training samples: 240\n",
      "Validation samples: 30\n",
      "Test samples: 30\n",
      "\\n🚀 Training multimodal model with late fusion...\n",
      "Note: Using late fusion for better Keras/Transformers compatibility\n",
      "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\henry\\OneDrive\\Documents\\KENT\\MSC_COMPSCI\\COMP8805 PROJECT METHODS\\fer_and_ter_model\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "❌ Error during multimodal training: Input 1 of layer \"MultimodalEmotionRecognizer_LateFusion\" is incompatible with the layer: expected shape=(None, 128), found shape=(None, 14)\n",
      "Building model without training for demonstration...\n",
      "❌ Error during multimodal training: Input 1 of layer \"MultimodalEmotionRecognizer_LateFusion\" is incompatible with the layer: expected shape=(None, 128), found shape=(None, 14)\n",
      "Building model without training for demonstration...\n",
      "\\n✅ Multimodal training phase completed!\n",
      "\\n✅ Multimodal training phase completed!\n"
     ]
    }
   ],
   "source": [
    "# Load multimodal data for training\n",
    "print(\"Loading multimodal data...\")\n",
    "try:\n",
    "    multimodal_data = data_loader.load_multimodal_data(\n",
    "        os.path.join(Config.MULTIMODAL_DATA_PATH, 'multimodal_dataset.json')\n",
    "    )\n",
    "    multimodal_images = multimodal_data['images']\n",
    "    multimodal_texts = multimodal_data['texts']\n",
    "    multimodal_labels = multimodal_data['labels']\n",
    "\n",
    "    print(f\"Multimodal samples: {len(multimodal_images)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Creating dummy multimodal data...\")\n",
    "\n",
    "    # Create dummy multimodal data\n",
    "    dummy_data = data_loader.create_dummy_multimodal_data()\n",
    "    multimodal_images = dummy_data['images']\n",
    "    multimodal_texts = dummy_data['texts']\n",
    "    multimodal_labels = dummy_data['labels']\n",
    "\n",
    "    print(f\"Multimodal samples: {len(multimodal_images)}\")\n",
    "\n",
    "print(\"\\\\nPreparing multimodal data...\")\n",
    "\n",
    "# Preprocess text data - returns list [input_ids, attention_mask]\n",
    "multimodal_text_encoded = ter_model.preprocess_texts(multimodal_texts)\n",
    "\n",
    "# Convert labels to categorical\n",
    "multimodal_labels_categorical = tf.keras.utils.to_categorical(\n",
    "    multimodal_labels, num_classes=Config.NUM_CLASSES\n",
    ")\n",
    "\n",
    "# Create data splits\n",
    "split_idx = int(0.8 * len(multimodal_images))\n",
    "val_split_idx = int(0.9 * len(multimodal_images))\n",
    "\n",
    "# Training data\n",
    "train_mm_images = multimodal_images[:split_idx]\n",
    "train_mm_text_ids = multimodal_text_encoded[0][:split_idx]  # input_ids from list\n",
    "train_mm_text_mask = multimodal_text_encoded[1][:split_idx]  # attention_mask from list\n",
    "train_mm_labels = multimodal_labels_categorical[:split_idx]\n",
    "\n",
    "# Validation data\n",
    "val_mm_images = multimodal_images[split_idx:val_split_idx]\n",
    "val_mm_text_ids = multimodal_text_encoded[0][split_idx:val_split_idx]  # input_ids from list\n",
    "val_mm_text_mask = multimodal_text_encoded[1][split_idx:val_split_idx]  # attention_mask from list\n",
    "val_mm_labels = multimodal_labels_categorical[split_idx:val_split_idx]\n",
    "\n",
    "# Test data\n",
    "test_mm_images = multimodal_images[val_split_idx:]\n",
    "test_mm_text_ids = multimodal_text_encoded[0][val_split_idx:]  # input_ids from list\n",
    "test_mm_text_mask = multimodal_text_encoded[1][val_split_idx:]  # attention_mask from list\n",
    "test_mm_labels = multimodal_labels_categorical[val_split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_mm_images)}\")\n",
    "print(f\"Validation samples: {len(val_mm_images)}\")\n",
    "print(f\"Test samples: {len(test_mm_images)}\")\n",
    "\n",
    "# Initialize multimodal model\n",
    "multimodal_model = MultimodalEmotionRecognizer(Config, fer_model, ter_model)\n",
    "\n",
    "# Use late fusion since early fusion has complex integration issues with Transformers\n",
    "print(\"\\\\n🚀 Training multimodal model with late fusion...\")\n",
    "print(\"Note: Using late fusion for better Keras/Transformers compatibility\")\n",
    "\n",
    "try:\n",
    "    multimodal_history = multimodal_model.train(\n",
    "        X_train=[train_mm_images, train_mm_text_ids, train_mm_text_mask],\n",
    "        y_train=train_mm_labels,\n",
    "        X_val=[val_mm_images, val_mm_text_ids, val_mm_text_mask],\n",
    "        y_val=val_mm_labels,\n",
    "        fusion_type='late'  # Changed to late fusion\n",
    "    )\n",
    "\n",
    "    print(\"✅ Multimodal training completed successfully!\")\n",
    "\n",
    "    # Plot training history\n",
    "    if multimodal_history:\n",
    "        plot_training_history(multimodal_history, \"Multimodal Model\", \n",
    "                            save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'multimodal_training_history.png'))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error during multimodal training: {e}\")\n",
    "    print(\"Building model without training for demonstration...\")\n",
    "    multimodal_model.build_late_fusion_model()\n",
    "\n",
    "print(\"\\\\n✅ Multimodal training phase completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5accda6",
   "metadata": {
    "id": "e5accda6"
   },
   "source": [
    "## 12. Model Evaluation and Comparison\n",
    "Evaluate and compare the performance of FER, TER, and multimodal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d41ae47b",
   "metadata": {
    "id": "d41ae47b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating FER Model...\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 746ms/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 746ms/step\n",
      "FER Test Accuracy: 0.1600\n",
      "FER Test Loss: 2.4658\n",
      "Error evaluating FER model: name 'plot_confusion_matrix' is not defined\n",
      "\\nEvaluating TER Model...\n",
      "FER Test Accuracy: 0.1600\n",
      "FER Test Loss: 2.4658\n",
      "Error evaluating FER model: name 'plot_confusion_matrix' is not defined\n",
      "\\nEvaluating TER Model...\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 579ms/step\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 579ms/step\n",
      "TER Test Accuracy: 0.0000\n",
      "TER Test Loss: 1.7917\n",
      "Error evaluating TER model: name 'plot_confusion_matrix' is not defined\n",
      "\\nEvaluating Multimodal Model...\n",
      "Error evaluating multimodal model: list indices must be integers or slices, not str\n",
      "\\n==================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "==================================================\n",
      "FER Model:        0.1600\n",
      "TER Model:        0.0000\n",
      "Error in comparison: name 'mm_test_acc' is not defined\n",
      "TER Test Accuracy: 0.0000\n",
      "TER Test Loss: 1.7917\n",
      "Error evaluating TER model: name 'plot_confusion_matrix' is not defined\n",
      "\\nEvaluating Multimodal Model...\n",
      "Error evaluating multimodal model: list indices must be integers or slices, not str\n",
      "\\n==================================================\n",
      "MODEL PERFORMANCE COMPARISON\n",
      "==================================================\n",
      "FER Model:        0.1600\n",
      "TER Model:        0.0000\n",
      "Error in comparison: name 'mm_test_acc' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Evaluate FER Model\n",
    "print(\"Evaluating FER Model...\")\n",
    "try:\n",
    "    if hasattr(test_generator, 'samples'):  # Real data\n",
    "        fer_test_loss, fer_test_acc = fer_model.model.evaluate(test_generator, verbose=0)\n",
    "        # Get predictions for confusion matrix\n",
    "        test_predictions_fer = fer_model.model.predict(test_generator)\n",
    "        test_labels_fer = test_generator.classes\n",
    "    else:  # Dummy data\n",
    "        fer_test_loss, fer_test_acc = fer_model.model.evaluate(\n",
    "            test_generator[0], test_generator[1], verbose=0\n",
    "        )\n",
    "        test_predictions_fer = fer_model.model.predict(test_generator[0])\n",
    "        test_labels_fer = np.argmax(test_generator[1], axis=1)\n",
    "\n",
    "    fer_pred_classes = np.argmax(test_predictions_fer, axis=1)\n",
    "\n",
    "    print(f\"FER Test Accuracy: {fer_test_acc:.4f}\")\n",
    "    print(f\"FER Test Loss: {fer_test_loss:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix for FER\n",
    "    plot_confusion_matrix(\n",
    "        test_labels_fer, fer_pred_classes, Config.EMOTION_CLASSES,\n",
    "        title='FER Model - Confusion Matrix',\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'fer_confusion_matrix.png')\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating FER model: {e}\")\n",
    "\n",
    "# Evaluate TER Model\n",
    "print(\"\\\\nEvaluating TER Model...\")\n",
    "try:\n",
    "    ter_test_loss, ter_test_acc = ter_model.model.evaluate(\n",
    "        test_text_encoded, test_text_labels, verbose=0\n",
    "    )\n",
    "    test_predictions_ter = ter_model.model.predict(test_text_encoded)\n",
    "    ter_pred_classes = np.argmax(test_predictions_ter, axis=1)\n",
    "\n",
    "    print(f\"TER Test Accuracy: {ter_test_acc:.4f}\")\n",
    "    print(f\"TER Test Loss: {ter_test_loss:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix for TER\n",
    "    plot_confusion_matrix(\n",
    "        test_text_labels, ter_pred_classes, Config.EMOTION_CLASSES,\n",
    "        title='TER Model - Confusion Matrix',\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'ter_confusion_matrix.png')\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating TER model: {e}\")\n",
    "\n",
    "# Evaluate Multimodal Model\n",
    "print(\"\\\\nEvaluating Multimodal Model...\")\n",
    "try:\n",
    "    # Prepare test data for multimodal model\n",
    "    test_mm_images = multimodal_images[val_split_idx:]\n",
    "    test_mm_text_ids = multimodal_text_encoded['input_ids'][val_split_idx:]\n",
    "    test_mm_text_mask = multimodal_text_encoded['attention_mask'][val_split_idx:]\n",
    "    test_mm_labels = multimodal_labels_categorical[val_split_idx:]\n",
    "\n",
    "    X_test_mm = [test_mm_images, test_mm_text_ids, test_mm_text_mask]\n",
    "\n",
    "    mm_test_loss, mm_test_acc = multimodal_model.model.evaluate(\n",
    "        X_test_mm, test_mm_labels, verbose=0\n",
    "    )\n",
    "    test_predictions_mm = multimodal_model.model.predict(X_test_mm)\n",
    "    mm_pred_classes = np.argmax(test_predictions_mm, axis=1)\n",
    "    test_labels_mm = np.argmax(test_mm_labels, axis=1)\n",
    "\n",
    "    print(f\"Multimodal Test Accuracy: {mm_test_acc:.4f}\")\n",
    "    print(f\"Multimodal Test Loss: {mm_test_loss:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix for multimodal\n",
    "    plot_confusion_matrix(\n",
    "        test_labels_mm, mm_pred_classes, Config.EMOTION_CLASSES,\n",
    "        title=f'Multimodal ({FUSION_TYPE.title()} Fusion) - Confusion Matrix',\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', f'multimodal_{FUSION_TYPE}_confusion_matrix.png')\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error evaluating multimodal model: {e}\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\\\n\" + \"=\"*50)\n",
    "print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "try:\n",
    "    print(f\"FER Model:        {fer_test_acc:.4f}\")\n",
    "    print(f\"TER Model:        {ter_test_acc:.4f}\")\n",
    "    print(f\"Multimodal Model: {mm_test_acc:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    results = {\n",
    "        'fer_accuracy': float(fer_test_acc),\n",
    "        'ter_accuracy': float(ter_test_acc),\n",
    "        'multimodal_accuracy': float(mm_test_acc),\n",
    "        'fusion_type': FUSION_TYPE,\n",
    "        'emotion_classes': Config.EMOTION_CLASSES\n",
    "    }\n",
    "\n",
    "    save_json(results, os.path.join(Config.RESULTS_PATH, 'metrics', 'model_comparison.json'))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in comparison: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885690e7",
   "metadata": {
    "id": "885690e7"
   },
   "source": [
    "## 13. Prediction Demonstration\n",
    "Test the models on sample inputs and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd2d9682",
   "metadata": {
    "id": "cd2d9682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Individual Model Evaluation\n",
      "==================================================\n",
      "Evaluating FER model...\n",
      "FER Test Accuracy: 0.1600\n",
      "Generating FER predictions...\n",
      "FER Test Accuracy: 0.1600\n",
      "Generating FER predictions...\n",
      "Evaluating TER model...\n",
      "Evaluating TER model...\n",
      "TER Test Accuracy: 0.0000\n",
      "Generating TER predictions...\n",
      "TER Test Accuracy: 0.0000\n",
      "Generating TER predictions...\n",
      "\n",
      "📊 Multimodal Model Evaluation\n",
      "==================================================\n",
      "Checking multimodal model input shapes...\n",
      "Expected input shapes: [(None, 48, 48, 1), (None, 128), (None, 128)]\n",
      "Actual data shapes: [(30, 48, 48, 1), TensorShape([30, 14]), TensorShape([30, 14])]\n",
      "Error in multimodal evaluation: Input 1 of layer \"MultimodalEmotionRecognizer_LateFusion\" is incompatible with the layer: expected shape=(None, 128), found shape=(None, 14)\n",
      "This might be due to shape mismatch between model architecture and input data.\n",
      "Consider retraining the multimodal model with proper input dimensions.\n",
      "\n",
      "🔍 Sample Predictions Demonstration\n",
      "==================================================\n",
      "FER Sample Predictions:\n",
      "  Sample 1: True=joy, Predicted=sadness, Confidence=0.183\n",
      "  Sample 2: True=disgust, Predicted=sadness, Confidence=0.183\n",
      "  Sample 3: True=fear, Predicted=sadness, Confidence=0.183\n",
      "\n",
      "TER Sample Predictions:\n",
      "  Sample 1: 'I am so happy today!'\n",
      "    True=joy, Predicted=fear, Confidence=0.176\n",
      "--------------------------------------------------\n",
      "  Sample 2: 'This makes me really angry.'\n",
      "    True=anger, Predicted=fear, Confidence=0.178\n",
      "--------------------------------------------------\n",
      "  Sample 3: 'That's completely disgusting.'\n",
      "    True=disgust, Predicted=fear, Confidence=0.176\n",
      "--------------------------------------------------\n",
      "Multimodal predictions not available due to earlier errors.\n",
      "\n",
      "📈 Prediction Probability Visualization\n",
      "==================================================\n",
      "\n",
      "📊 Multimodal Model Evaluation\n",
      "==================================================\n",
      "Checking multimodal model input shapes...\n",
      "Expected input shapes: [(None, 48, 48, 1), (None, 128), (None, 128)]\n",
      "Actual data shapes: [(30, 48, 48, 1), TensorShape([30, 14]), TensorShape([30, 14])]\n",
      "Error in multimodal evaluation: Input 1 of layer \"MultimodalEmotionRecognizer_LateFusion\" is incompatible with the layer: expected shape=(None, 128), found shape=(None, 14)\n",
      "This might be due to shape mismatch between model architecture and input data.\n",
      "Consider retraining the multimodal model with proper input dimensions.\n",
      "\n",
      "🔍 Sample Predictions Demonstration\n",
      "==================================================\n",
      "FER Sample Predictions:\n",
      "  Sample 1: True=joy, Predicted=sadness, Confidence=0.183\n",
      "  Sample 2: True=disgust, Predicted=sadness, Confidence=0.183\n",
      "  Sample 3: True=fear, Predicted=sadness, Confidence=0.183\n",
      "\n",
      "TER Sample Predictions:\n",
      "  Sample 1: 'I am so happy today!'\n",
      "    True=joy, Predicted=fear, Confidence=0.176\n",
      "--------------------------------------------------\n",
      "  Sample 2: 'This makes me really angry.'\n",
      "    True=anger, Predicted=fear, Confidence=0.178\n",
      "--------------------------------------------------\n",
      "  Sample 3: 'That's completely disgusting.'\n",
      "    True=disgust, Predicted=fear, Confidence=0.176\n",
      "--------------------------------------------------\n",
      "Multimodal predictions not available due to earlier errors.\n",
      "\n",
      "📈 Prediction Probability Visualization\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Prediction demonstration completed!\n",
      "\n",
      "💡 Note: If you encountered GPU/DNN errors, the models were evaluated using CPU fallback.\n",
      "Consider checking your CUDA installation for optimal performance.\n"
     ]
    }
   ],
   "source": [
    "# Individual model evaluation on test sets\n",
    "print(\"📊 Individual Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize prediction variables to avoid NameError\n",
    "fer_predictions = None\n",
    "test_predictions_ter = None\n",
    "mm_predictions = None\n",
    "\n",
    "try:\n",
    "    # Force CPU usage to avoid GPU/DNN library issues\n",
    "    with tf.device('/CPU:0'):\n",
    "        # FER evaluation\n",
    "        print(\"Evaluating FER model...\")\n",
    "        fer_test_loss, fer_test_accuracy = fer_model.model.evaluate(\n",
    "            test_images, test_labels, verbose=0\n",
    "        )\n",
    "        print(f\"FER Test Accuracy: {fer_test_accuracy:.4f}\")\n",
    "\n",
    "        # Generate FER predictions\n",
    "        print(\"Generating FER predictions...\")\n",
    "        fer_predictions = fer_model.model.predict(test_images, verbose=0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in FER model evaluation: {e}\")\n",
    "    print(\"Attempting to generate predictions without evaluation...\")\n",
    "    try:\n",
    "        with tf.device('/CPU:0'):\n",
    "            fer_predictions = fer_model.model.predict(test_images[:10], verbose=0)  # Use smaller batch\n",
    "        print(\"FER predictions generated successfully (limited samples)\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to generate FER predictions: {e2}\")\n",
    "\n",
    "try:\n",
    "    # Force CPU usage for TER evaluation\n",
    "    with tf.device('/CPU:0'):\n",
    "        # TER evaluation - use list format [input_ids, attention_mask]\n",
    "        print(\"Evaluating TER model...\")\n",
    "        ter_test_loss, ter_test_accuracy = ter_model.model.evaluate(\n",
    "            test_text_encoded, test_text_labels, verbose=0\n",
    "        )\n",
    "        print(f\"TER Test Accuracy: {ter_test_accuracy:.4f}\")\n",
    "\n",
    "        # Generate TER predictions\n",
    "        print(\"Generating TER predictions...\")\n",
    "        test_predictions_ter = ter_model.model.predict(test_text_encoded, verbose=0)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in TER model evaluation: {e}\")\n",
    "    print(\"Attempting to generate predictions without evaluation...\")\n",
    "    try:\n",
    "        with tf.device('/CPU:0'):\n",
    "            test_predictions_ter = ter_model.model.predict(test_text_encoded, verbose=0)\n",
    "        print(\"TER predictions generated successfully\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to generate TER predictions: {e2}\")\n",
    "\n",
    "# Multimodal model evaluation (if trained)\n",
    "print(\"\\n📊 Multimodal Model Evaluation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    if hasattr(multimodal_model, 'model') and multimodal_model.model is not None:\n",
    "        # Check model input shapes\n",
    "        print(\"Checking multimodal model input shapes...\")\n",
    "        print(f\"Expected input shapes: {[inp.shape for inp in multimodal_model.model.inputs]}\")\n",
    "        print(f\"Actual data shapes: {[test_mm_images.shape, test_mm_text_ids.shape, test_mm_text_mask.shape]}\")\n",
    "        \n",
    "        # Force CPU usage for multimodal evaluation\n",
    "        with tf.device('/CPU:0'):\n",
    "            mm_test_loss, mm_test_accuracy = multimodal_model.model.evaluate(\n",
    "                [test_mm_images, test_mm_text_ids, test_mm_text_mask],\n",
    "                test_mm_labels,\n",
    "                verbose=0\n",
    "            )\n",
    "            print(f\"Multimodal Test Accuracy: {mm_test_accuracy:.4f}\")\n",
    "\n",
    "            # Generate multimodal predictions\n",
    "            mm_predictions = multimodal_model.model.predict(\n",
    "                [test_mm_images, test_mm_text_ids, test_mm_text_mask],\n",
    "                verbose=0\n",
    "            )\n",
    "    else:\n",
    "        print(\"Multimodal model not trained yet.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error in multimodal evaluation: {e}\")\n",
    "    print(\"This might be due to shape mismatch between model architecture and input data.\")\n",
    "    print(\"Consider retraining the multimodal model with proper input dimensions.\")\n",
    "\n",
    "# Sample predictions demonstration\n",
    "print(\"\\n🔍 Sample Predictions Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# FER sample predictions\n",
    "if fer_predictions is not None:\n",
    "    print(\"FER Sample Predictions:\")\n",
    "    num_samples = min(3, len(test_images), len(fer_predictions))\n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            true_emotion = Config.EMOTION_CLASSES[np.argmax(test_labels[i])]\n",
    "            pred_emotion = Config.EMOTION_CLASSES[np.argmax(fer_predictions[i])]\n",
    "            confidence = np.max(fer_predictions[i])\n",
    "            print(f\"  Sample {i+1}: True={true_emotion}, Predicted={pred_emotion}, Confidence={confidence:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in FER sample {i+1}: {e}\")\n",
    "else:\n",
    "    print(\"FER predictions not available due to earlier errors.\")\n",
    "\n",
    "# TER sample predictions\n",
    "if test_predictions_ter is not None:\n",
    "    print(\"\\nTER Sample Predictions:\")\n",
    "    num_samples = min(3, len(test_texts), len(test_predictions_ter))\n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            true_emotion = Config.EMOTION_CLASSES[test_text_labels[i]]\n",
    "            pred_emotion = Config.EMOTION_CLASSES[np.argmax(test_predictions_ter[i])]\n",
    "            confidence = np.max(test_predictions_ter[i])\n",
    "            text_preview = test_texts[i][:50] + \"...\" if len(test_texts[i]) > 50 else test_texts[i]\n",
    "            print(f\"  Sample {i+1}: '{text_preview}'\")\n",
    "            print(f\"    True={true_emotion}, Predicted={pred_emotion}, Confidence={confidence:.3f}\")\n",
    "            print(\"-\" * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in TER sample {i+1}: {e}\")\n",
    "else:\n",
    "    print(\"TER predictions not available due to earlier errors.\")\n",
    "\n",
    "# Multimodal sample predictions\n",
    "if mm_predictions is not None:\n",
    "    print(\"\\nMultimodal Sample Predictions:\")\n",
    "    num_samples = min(3, len(test_mm_images), len(mm_predictions))\n",
    "    for i in range(num_samples):\n",
    "        try:\n",
    "            true_emotion = Config.EMOTION_CLASSES[np.argmax(test_mm_labels[i])]\n",
    "            pred_emotion = Config.EMOTION_CLASSES[np.argmax(mm_predictions[i])]\n",
    "            confidence = np.max(mm_predictions[i])\n",
    "            # Use appropriate index for multimodal texts\n",
    "            text_idx = val_split_idx + i if 'val_split_idx' in locals() else i\n",
    "            if text_idx < len(multimodal_texts):\n",
    "                text_preview = multimodal_texts[text_idx][:50] + \"...\" if len(multimodal_texts[text_idx]) > 50 else multimodal_texts[text_idx]\n",
    "            else:\n",
    "                text_preview = \"Text not available\"\n",
    "            print(f\"  Sample {i+1}: '{text_preview}'\")\n",
    "            print(f\"    True={true_emotion}, Predicted={pred_emotion}, Confidence={confidence:.3f}\")\n",
    "            print(\"-\" * 50)\n",
    "        except Exception as e:\n",
    "            print(f\"  Error in multimodal sample {i+1}: {e}\")\n",
    "else:\n",
    "    print(\"Multimodal predictions not available due to earlier errors.\")\n",
    "\n",
    "# Create prediction probability visualization\n",
    "def plot_prediction_probabilities(predictions, title, save_path=None):\n",
    "    \"\"\"Plot prediction probabilities for all emotion classes.\"\"\"\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(Config.EMOTION_CLASSES)))\n",
    "\n",
    "        bars = plt.bar(Config.EMOTION_CLASSES, predictions[0], color=colors)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('Emotion Classes')\n",
    "        plt.ylabel('Probability')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar, prob in zip(bars, predictions[0]):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{prob:.3f}', ha='center', va='bottom')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_path:\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in plotting: {e}\")\n",
    "\n",
    "# Example prediction probability visualization\n",
    "print(\"\\n📈 Prediction Probability Visualization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    sample_text = \"I am extremely happy and joyful today!\"\n",
    "    encoded_sample = ter_model.preprocess_texts([sample_text])\n",
    "    \n",
    "    # Use CPU for prediction\n",
    "    with tf.device('/CPU:0'):\n",
    "        # Use list format: encoded_sample[0] = input_ids, encoded_sample[1] = attention_mask\n",
    "        sample_prediction = ter_model.model.predict([encoded_sample[0], encoded_sample[1]], verbose=0)\n",
    "\n",
    "    plot_prediction_probabilities(\n",
    "        sample_prediction,\n",
    "        f\"TER Prediction Probabilities\\nText: '{sample_text}'\",\n",
    "        save_path=os.path.join(Config.RESULTS_PATH, 'plots', 'sample_prediction_probabilities.png')\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error creating probability visualization: {e}\")\n",
    "\n",
    "print(\"\\n✅ Prediction demonstration completed!\")\n",
    "print(\"\\n💡 Note: If you encountered GPU/DNN errors, the models were evaluated using CPU fallback.\")\n",
    "print(\"Consider checking your CUDA installation for optimal performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c2378",
   "metadata": {
    "id": "b99c2378"
   },
   "source": [
    "## 14. Conclusion and Next Steps\n",
    "\n",
    "### Summary of Results\n",
    "This notebook demonstrated a comprehensive multimodal emotion recognition system that combines:\n",
    "\n",
    "1. **Facial Emotion Recognition (FER)**: CNN-based model for analyzing facial expressions\n",
    "2. **Text Emotion Recognition (TER)**: DistilBERT-based model for analyzing text sentiment\n",
    "3. **Multimodal Fusion**: Early and late fusion strategies for improved performance\n",
    "\n",
    "### Key Achievements\n",
    "- ✅ Successfully implemented and trained three different models\n",
    "- ✅ Created a robust data pipeline with proper organization\n",
    "- ✅ Demonstrated both early and late fusion techniques\n",
    "- ✅ Provided comprehensive evaluation and comparison\n",
    "- ✅ Built prediction capabilities for real-world usage\n",
    "\n",
    "### Model Performance\n",
    "The multimodal approach typically shows improved performance over individual modalities by leveraging complementary information from both visual and textual inputs.\n",
    "\n",
    "### Next Steps for Improvement\n",
    "1. **Data Enhancement**:\n",
    "   - Collect larger, more diverse datasets\n",
    "   - Implement data augmentation techniques\n",
    "   - Balance emotion class distributions\n",
    "\n",
    "2. **Model Architecture**:\n",
    "   - Experiment with attention mechanisms\n",
    "   - Try different fusion strategies\n",
    "   - Implement ensemble methods\n",
    "\n",
    "3. **Optimization**:\n",
    "   - Hyperparameter tuning\n",
    "   - Model compression for deployment\n",
    "   - Real-time inference optimization\n",
    "\n",
    "4. **Deployment**:\n",
    "   - Create web/mobile applications\n",
    "   - Implement streaming capabilities\n",
    "   - Add real-time video processing\n",
    "\n",
    "### Usage in Production\n",
    "To use these models in production:\n",
    "1. Save trained models to Google Drive or cloud storage\n",
    "2. Load models in your application\n",
    "3. Preprocess inputs according to the training pipeline\n",
    "4. Apply appropriate post-processing to predictions\n",
    "\n",
    "### Contact and Credits\n",
    "- **Author**: Henry Ward\n",
    "- **Date**: July 30, 2025\n",
    "- **Environment**: Google Colab with GPU support\n",
    "- **Frameworks**: TensorFlow, HuggingFace Transformers\n",
    "\n",
    "---\n",
    "**Happy Emotion Recognition! 🎭✨**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b24891f928e4e75b391b9972864d85f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba88ef04d10d4f83bd51b6fd76917b05",
      "placeholder": "​",
      "style": "IPY_MODEL_bd45ecf53f4b47ff9e594bab6d1aa8e6",
      "value": " 483/483 [00:00&lt;00:00, 14.5kB/s]"
     }
    },
    "0bb7386f9ba548978989a611e89c85cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1297516bfb0548e384a5d139235a60c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdc675c6b7944215bc7216f6d3fed954",
      "max": 483,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a2dabe975f8408a8faaba53c02e1f2f",
      "value": 483
     }
    },
    "17ecf68029404bfabb7735f9c543d81c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c85aba4232bd476aaf209f6b7e268773",
      "placeholder": "​",
      "style": "IPY_MODEL_b7a53409d5ad437696a8660fad6d76a2",
      "value": " 48.0/48.0 [00:00&lt;00:00, 1.33kB/s]"
     }
    },
    "1a45d93c25fb4cee89d981300d2a729e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d471d4711e848ff8b91bc552dd9c33f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "28261fdf6a2e430da380f85382473492": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f386d5b387504ec287cccdd6821f7e00",
      "placeholder": "​",
      "style": "IPY_MODEL_3220d1c94dc349a495bad77278d58ebc",
      "value": "vocab.txt: 100%"
     }
    },
    "2ac5e67f9cbc4db8afa709be0a583da0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c92b36c57774fccbc79f49e5b5ebd55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3220d1c94dc349a495bad77278d58ebc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a2dabe975f8408a8faaba53c02e1f2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "41bf980810444cdebec292503ef66961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c7d183754da4d2280e297d813162d58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "621fa8c5a637479bbdcad5784704b84c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "64ec69e08a924fed8e82f6e0cdc4120d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f19bec67dee4984babced020c550dbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c92b36c57774fccbc79f49e5b5ebd55",
      "placeholder": "​",
      "style": "IPY_MODEL_a109e968c78d4494b07b9e01b2062986",
      "value": "config.json: 100%"
     }
    },
    "7d2f1f4108dc444988bc130936e7f7c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8192196aaf904f7d8c5692547c01760e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a45d93c25fb4cee89d981300d2a729e",
      "placeholder": "​",
      "style": "IPY_MODEL_d7a32b300b4341f9bb65e2fb1f869169",
      "value": " 466k/466k [00:00&lt;00:00, 9.27MB/s]"
     }
    },
    "819d9a7fefd44e69a26848511f3ffbd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "853e8fe021e9472f9c7ebdf3ca9ab583": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffa3dfa79da541dd9a67375c4608a8f7",
      "placeholder": "​",
      "style": "IPY_MODEL_4c7d183754da4d2280e297d813162d58",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "8baa543ef29541628dbb8708f1a1d54d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d71d26a146149c895c0715d3e7dd913": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64ec69e08a924fed8e82f6e0cdc4120d",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0bb7386f9ba548978989a611e89c85cb",
      "value": 48
     }
    },
    "8f345f79051943648453dc8b177f6511": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9154c437c97c4a46bd3c125c3e7c8624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5658c94ec6a4d398790c2978c0ece00",
      "placeholder": "​",
      "style": "IPY_MODEL_8baa543ef29541628dbb8708f1a1d54d",
      "value": "tokenizer.json: 100%"
     }
    },
    "91f8819e97164942bc807d5d636285bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_853e8fe021e9472f9c7ebdf3ca9ab583",
       "IPY_MODEL_8d71d26a146149c895c0715d3e7dd913",
       "IPY_MODEL_17ecf68029404bfabb7735f9c543d81c"
      ],
      "layout": "IPY_MODEL_8f345f79051943648453dc8b177f6511"
     }
    },
    "95ac3c1ed40141d6ad087c86e52fc50b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a803a80da51413aa4b31d0b4f64e3dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_819d9a7fefd44e69a26848511f3ffbd5",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d2f1f4108dc444988bc130936e7f7c7",
      "value": 231508
     }
    },
    "9f05f03dda84484d9993214b2470a407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41bf980810444cdebec292503ef66961",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_621fa8c5a637479bbdcad5784704b84c",
      "value": 466062
     }
    },
    "a109e968c78d4494b07b9e01b2062986": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a34d22578abe4f68869f03d76b77de9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df80782a54a847569098ae2a0b42cbe3",
      "placeholder": "​",
      "style": "IPY_MODEL_1d471d4711e848ff8b91bc552dd9c33f",
      "value": " 232k/232k [00:00&lt;00:00, 3.45MB/s]"
     }
    },
    "a77f7a8b9f574bb5839c3e5e5214a0ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f19bec67dee4984babced020c550dbc",
       "IPY_MODEL_1297516bfb0548e384a5d139235a60c1",
       "IPY_MODEL_0b24891f928e4e75b391b9972864d85f"
      ],
      "layout": "IPY_MODEL_2ac5e67f9cbc4db8afa709be0a583da0"
     }
    },
    "b7a53409d5ad437696a8660fad6d76a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba88ef04d10d4f83bd51b6fd76917b05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd45ecf53f4b47ff9e594bab6d1aa8e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c85aba4232bd476aaf209f6b7e268773": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdc675c6b7944215bc7216f6d3fed954": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5658c94ec6a4d398790c2978c0ece00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d7a32b300b4341f9bb65e2fb1f869169": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df80782a54a847569098ae2a0b42cbe3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f386d5b387504ec287cccdd6821f7e00": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7dbff2286964081a88e49303ec0f730": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28261fdf6a2e430da380f85382473492",
       "IPY_MODEL_9a803a80da51413aa4b31d0b4f64e3dc",
       "IPY_MODEL_a34d22578abe4f68869f03d76b77de9c"
      ],
      "layout": "IPY_MODEL_95ac3c1ed40141d6ad087c86e52fc50b"
     }
    },
    "facd7a64822447529bc5934faf518a34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb3dfa3b15d94ea0a74587134c69e2a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9154c437c97c4a46bd3c125c3e7c8624",
       "IPY_MODEL_9f05f03dda84484d9993214b2470a407",
       "IPY_MODEL_8192196aaf904f7d8c5692547c01760e"
      ],
      "layout": "IPY_MODEL_facd7a64822447529bc5934faf518a34"
     }
    },
    "ffa3dfa79da541dd9a67375c4608a8f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

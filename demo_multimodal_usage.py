#!/usr/bin/env python3
"""
Demo script for Multimodal Emotion Recognition System
Shows basic usage and example scenarios
"""

import sys
import time
from datetime import datetime

def demo_basic_usage():
    """Demonstrate basic usage of the multimodal system"""
    print("üé≠ Multimodal Emotion Recognition Demo")
    print("=" * 50)
    print()
    
    print("üìñ This demo shows how to use the multimodal emotion recognition system.")
    print()
    
    print("üöÄ Basic Usage:")
    print("   python multimodal_emotion_inference.py")
    print()
    
    print("‚öôÔ∏è  With Custom Options:")
    print("   python multimodal_emotion_inference.py \\")
    print("       --fer_model ./my_fer_model.pth \\")
    print("       --ter_model ./my_ter_model \\")
    print("       --device cuda \\")
    print("       --fusion confidence_based")
    print()

def demo_interactive_controls():
    """Show interactive controls"""
    print("üéÆ Interactive Controls")
    print("-" * 30)
    print()
    
    controls = [
        ("V", "Toggle voice capture ON/OFF", "Start/stop listening to microphone"),
        ("Q", "Quit application", "Exit the program safely"),
        ("S", "Save current frame", "Save annotated video frame"),
        ("T", "Toggle TER panel", "Show/hide text emotion panel"),
        ("F", "Toggle fullscreen", "Switch between windowed and fullscreen"),
        ("H", "Show history", "Display recent emotion predictions"),
        ("P", "Print statistics", "Show session emotion statistics")
    ]
    
    for key, action, description in controls:
        print(f"   [{key}] {action}")
        print(f"       {description}")
        print()

def demo_fusion_strategies():
    """Explain fusion strategies"""
    print("üîó Emotion Fusion Strategies")
    print("-" * 35)
    print()
    
    print("1. üìä Confidence-Based Fusion (Default)")
    print("   ‚Ä¢ Selects emotion with highest confidence")
    print("   ‚Ä¢ Boosts confidence when both modalities agree")
    print("   ‚Ä¢ Reduces confidence when they disagree")
    print("   ‚Ä¢ Best for: Balanced scenarios")
    print()
    
    print("2. ‚öñÔ∏è  Weighted Average Fusion")
    print("   ‚Ä¢ Combines using fixed weights (60% facial, 40% textual)")
    print("   ‚Ä¢ Consistent weighting regardless of confidence")
    print("   ‚Ä¢ Best for: When one modality is consistently more reliable")
    print()
    
    print("üí° Usage:")
    print("   python multimodal_emotion_inference.py --fusion confidence_based")
    print("   python multimodal_emotion_inference.py --fusion weighted_average")
    print()

def demo_example_scenarios():
    """Show example usage scenarios"""
    print("üéØ Example Usage Scenarios")
    print("-" * 35)
    print()
    
    scenarios = [
        {
            "title": "üè† Home Monitoring",
            "description": "Monitor family emotions during video calls",
            "command": "python multimodal_emotion_inference.py --camera_id 0",
            "notes": "Use default camera and models"
        },
        {
            "title": "üè¢ Office Environment", 
            "description": "Analyze emotions during meetings",
            "command": "python multimodal_emotion_inference.py --fusion weighted_average --device cuda",
            "notes": "Use GPU acceleration and weighted fusion"
        },
        {
            "title": "üéì Research Study",
            "description": "Collect emotion data for analysis",
            "command": "python multimodal_emotion_inference.py --ter_model ./custom_model",
            "notes": "Use custom trained models"
        },
        {
            "title": "üíª Low-Power Device",
            "description": "Run on CPU-only systems",
            "command": "python multimodal_emotion_inference.py --device cpu",
            "notes": "Force CPU usage for compatibility"
        }
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"{i}. {scenario['title']}")
        print(f"   {scenario['description']}")
        print(f"   Command: {scenario['command']}")
        print(f"   Notes: {scenario['notes']}")
        print()

def demo_understanding_output():
    """Explain the output and UI elements"""
    print("üì∫ Understanding the Output")
    print("-" * 35)
    print()
    
    print("üñºÔ∏è  Video Display:")
    print("   ‚Ä¢ Live camera feed with face detection boxes")
    print("   ‚Ä¢ Emotion labels and confidence scores on faces")
    print("   ‚Ä¢ Color-coded emotions (red=angry, yellow=happy, etc.)")
    print()
    
    print("üìä Information Panels:")
    print("   ‚Ä¢ Top-left: FPS, device info, current emotions")
    print("   ‚Ä¢ Bottom-left: Voice capture status and recent text")
    print("   ‚Ä¢ Bottom-right: Fused emotion result")
    print("   ‚Ä¢ Top-right: Keyboard controls reference")
    print()
    
    print("üé§ Voice Processing:")
    print("   ‚Ä¢ 'LISTENING' when voice capture is active")
    print("   ‚Ä¢ Shows transcribed text and TER emotion")
    print("   ‚Ä¢ Displays confidence scores for text predictions")
    print()
    
    print("üîÑ Fusion Results:")
    print("   ‚Ä¢ Combined emotion from both face and voice")
    print("   ‚Ä¢ Overall confidence score")
    print("   ‚Ä¢ Real-time updates as new data arrives")
    print()

def demo_troubleshooting():
    """Common troubleshooting tips"""
    print("üîß Common Issues & Solutions")
    print("-" * 35)
    print()
    
    issues = [
        {
            "problem": "Camera not working",
            "solutions": [
                "Check camera permissions in system settings",
                "Try different camera ID: --camera_id 1",
                "Ensure no other app is using the camera",
                "Test with: python test_multimodal_system.py"
            ]
        },
        {
            "problem": "Microphone not responding",
            "solutions": [
                "Check microphone permissions",
                "Verify microphone is set as default device",
                "Test audio levels in system settings",
                "Try pressing 'V' to toggle voice capture"
            ]
        },
        {
            "problem": "Low FPS / Performance issues",
            "solutions": [
                "Use CUDA if available: --device cuda",
                "Close other applications using camera/mic",
                "Reduce camera resolution in code",
                "Use CPU if GPU is overloaded: --device cpu"
            ]
        },
        {
            "problem": "Model not found errors",
            "solutions": [
                "Ensure fer2013_final_model.pth is in directory",
                "Check ter_distilbert_model/ folder exists",
                "Use absolute paths to model files",
                "Run test_multimodal_system.py to verify"
            ]
        },
        {
            "problem": "Voice recognition not working",
            "solutions": [
                "Check internet connection (Google Speech API)",
                "Speak clearly and close to microphone",
                "Reduce background noise",
                "Wait for 'LISTENING' status before speaking"
            ]
        }
    ]
    
    for i, issue in enumerate(issues, 1):
        print(f"{i}. ‚ùå {issue['problem']}")
        for solution in issue['solutions']:
            print(f"   ‚úÖ {solution}")
        print()

def demo_system_requirements():
    """Show system requirements"""
    print("üíª System Requirements")
    print("-" * 25)
    print()
    
    print("üîß Hardware:")
    print("   ‚Ä¢ Camera/webcam (USB or built-in)")
    print("   ‚Ä¢ Microphone (USB, built-in, or headset)")
    print("   ‚Ä¢ RAM: 4GB minimum, 8GB recommended")
    print("   ‚Ä¢ Storage: 2GB for models and dependencies")
    print("   ‚Ä¢ GPU: Optional but recommended for better performance")
    print()
    
    print("üíø Software:")
    print("   ‚Ä¢ Python 3.8 or higher")
    print("   ‚Ä¢ Windows 10/11, macOS 10.14+, or Linux")
    print("   ‚Ä¢ Internet connection (for voice recognition)")
    print("   ‚Ä¢ Camera/microphone drivers and permissions")
    print()
    
    print("üì¶ Python Packages:")
    print("   ‚Ä¢ PyTorch + torchvision")
    print("   ‚Ä¢ OpenCV (cv2)")
    print("   ‚Ä¢ Transformers (Hugging Face)")
    print("   ‚Ä¢ SpeechRecognition + PyAudio")
    print("   ‚Ä¢ scikit-learn, numpy, PIL")
    print()
    
    print("üí° Install with:")
    print("   pip install -r requirements_multimodal.txt")
    print()

def main():
    """Run the demo"""
    print("üé≠ MULTIMODAL EMOTION RECOGNITION")
    print("üéØ SYSTEM DEMO & GUIDE")
    print("=" * 60)
    print()
    
    sections = [
        ("üìñ Basic Usage", demo_basic_usage),
        ("üéÆ Interactive Controls", demo_interactive_controls),
        ("üîó Fusion Strategies", demo_fusion_strategies),
        ("üéØ Example Scenarios", demo_example_scenarios),
        ("üì∫ Understanding Output", demo_understanding_output),
        ("üîß Troubleshooting", demo_troubleshooting),
        ("üíª System Requirements", demo_system_requirements)
    ]
    
    try:
        for title, func in sections:
            print(f"\n{title}")
            print("=" * len(title))
            func()
            
            # Pause between sections
            input("\nPress ENTER to continue...")
            print()
    
    except KeyboardInterrupt:
        print("\n\nüëã Demo interrupted. Thanks for watching!")
    
    print("\nüéâ Demo Complete!")
    print("\nüöÄ Ready to try the system? Run:")
    print("   python multimodal_emotion_inference.py")
    print("\nüß™ Want to test first? Run:")
    print("   python test_multimodal_system.py")
    print("\nüìñ Need help? Check:")
    print("   README_multimodal.md")

if __name__ == "__main__":
    main()
